<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2021%2F02%2F14%2F%E8%A7%86%E9%A2%91%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[2021年春节VLOG 春节VLOG]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL高可用中间件方案选型]]></title>
    <url>%2F2021%2F01%2F15%2FMySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[MySQL高可用中间件方案选型，稍微总结一下。 常用MySQL高可用中间件方案 MySQL Router MySQL Router is part of InnoDB Cluster and is lightweight middleware that provides transparent routing between your application and back-end MySQL Servers. It is used for a wide variety of use cases, such as providing high availability and scalability by routing database traffic to appropriate back-end MySQL servers. The pluggable architecture also enables developers to extend MySQL Router for custom use cases. Mysql InnoDB Cluster 主要由三个模块构成： 支持Group Replication 功能的Mysql Server(version &gt;= 5.7.17)，模块主要功能在于实现了组内通信，故障转移（英语：failover, 即当活动的服务或应用意外终止时，快速启用冗余或备用的服务器、系统、硬件或者网络接替它们工作）、故障恢复（英语：failback，将系统，组件，服务恢复到故障之前的组态） Mysql-shell：实现快速部署，主要提供了一套AdminAPI，可以自动化配置Group Replication，让我们无须再手动配置cluster中group replication相关参数。 Mysql-router：内置读写分离，负载均衡。自动根据Mysql InnoDB Cluster中的metadata自动调整。 优点：其实也就是InnoDB Cluster的优点缺点：Mysql-router没有SQL审计，限流，分库分表的功能，但是可以通过C来写插件进行扩展；非InnoDB Cluster架构下，主库和从库的拓扑变更需要修改配置并重启。 Maxscale优点： 支持读写分离，高可用，故障转移 提供了监控能力 官网介绍缺点：BSL协议，在生产环境，如果后端超过3个MariaDB实例提供服务，就必须购买商业授权。Failover以及Switchover和Rejoin仅支持基于GTID(全局事务ID)的复制一起使用，并且仅适用于简单的一主多从拓扑架构：即1个master后面跟着多个slave。需要把loss-less无损半同步复制(semi replication)开启，参数rpl_semi_sync_master_wait_point=AFTER_SYNC，确保slave已经接收到了master的binlog，因为master宕机，MaxScale无法远程拷贝scp那一缺失的binlog，那么数据就出现不一致了。 ProxySQLC++开发，轻量级的开源软件，配置数据基于SQLite存储，Proxysql读写分离的中间件，支持高可用 主从\ MGR \ PXC等环境，并提供连接池、缓存、日志记录等功能。优点： 支持多路复用连接池 自动下线后端DB； 延迟超过阀值、 ping 延迟超过阀值、网络不通或宕机 可缓存查询结果 强大的规则路由引擎；实现读写分离、 查询重写、sql流量镜像 提供了监控能力 缺点：例子少，只有官方文档。用户不如MaxScale多。 ArkProxy 优点： 透明读写分离和支持 Hint 分发 100%兼容MySQL语法，用户友好 自动负载均衡、权重分发，灵活控制数据库流量 内部实现消息压缩，同时实现用户连接数限制和统计 Trace智能统计分析及审计，支持将访问请求对接到Kafka，供大数据系统统计分析 内置高效连接池，在高并发时大大提升数据库集群的处理能力 提供自定义一致性读和自路由一致性读来满足数据的强一致性读需求 自定义SQL 拦截，可以拦截危险SQL 配置文件可动态加载，避免重启 Percona 分支数据库支持无任何权限侵入直接上线的功能 缺点：貌似上述功能都要依赖他们的体系。 MyCat优点： 支持 SQL 92标准 支持Mysql集群，可以作为Proxy使用 支持JDBC连接ORACLE、DB2、SQL Server，将其模拟为MySQL Server使用 支持galera for mysql集群，percona-cluster或者mariadb cluster，提供高可用性数据分片集群 自动故障切换，高可用性 支持读写分离，支持Mysql双主多从，以及一主多从的模式 支持全局表，数据自动分片到多个节点，用于高效表关联查询 支持独有的基于E-R 关系的分片策略，实现了高效的表关联查询 多平台支持，部署和实施简单 缺点：MyCat2已经出现了，社区活跃度不太稳定，感觉企业里使用的不多。同时，对数据查询语句作拆分本身就是一个比较头疼的事情，可能会有很多bug。增加节点需要手动修改schema.xml配置文件，然后做一次reload操作等等。schema.xml配置简直不能忍。 Apache shardingsphere-JDBCShardingSphere-JDBC 是 Apache ShardingSphere 的第一个产品，也是 Apache ShardingSphere 的前身。 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。 自动节点克隆：在新增节点，或者停机维护时，增量数据或者基础数据不需要人工手动备份提供，Galera Cluster会自动拉取在线节点数据，最终集群会变为一致。 对应用透明：集群的维护，对应用程序是透明的，几乎感觉不到。以上几点，足以说明Galera Cluster是一个既稳健，又在数据一致性、完整性及高性能方面有出色表现的高可用解决方案。 Galera Cluster架构解决方案：不做读写分离的话 HA + Galera Cluster集成了Galera插件的MySQL集群，是一种新型的，数据不共享的，高度冗余的高可用方案，目前Galera Cluster有两个版本，分别是Percona Xtradb Cluster及MariaDB Cluster，都是基于Galera的，Galera Cluster架构就是multi-master的集群架构。 优点： 多主架构：真正的多点读写的集群，在任何时候读写数据，都是最新的。 同步复制：集群不同节点之间数据同步，没有延迟，在数据库挂掉之后，数据不会丢失。异步复制中，主库将数据更新传播给从库后立即提交事务，而不论从库是否成功读取或重放数据变化。这种情况下，在主库事务提交后的短时间内，主从库数据并不一致。同步复制时，主库的单个更新事务需要在所有从库上同步更新。换句话说，当主库提交事务时，集群中所有节点的数据保持一致。 并发复制：从节点在APPLY数据时，支持并行执行，有更好的性能表现。 故障切换：在出现数据库故障时，因为支持多点写入，切的非常容易。 热插拔：在服务期间，如果数据库挂了，只要监控程序发现的够快，不可服务时间就会非常少。在节点故障期间，节点本身对集群的影响非常小。个人感觉适合交易类的场景，是一个可以保证数据强一致性的架构。 缺点：比较新型的架构使用者较少，还在发展。 MySQL Group Replication架构MGR是以Plugin的形式嵌入在MySQL实例中，插件内部实现了冲突检测、Paxos协议通信等。 MySQL异步复制： master事务的提交不需要经过slave的确认，slave是否接收到master的binlog，master并不care。slave接收到master binlog后先写relay log，最后异步地去执行relay log中的sql应用到自身。由于master的提交不需要确保slave relay log是否被正确接受，当slave接受master binlog失败或者relay log应用失败，master无法感知。假设master发生宕机并且binlog还没来得及被slave接收，而切换程序将slave提升为新的master，就会出现数据不一致的情况！另外，在高并发的情况下，传统的主从复制，从节点可能会与主产生较大的延迟。 MySQL半同步复制： 半同步复制是传统异步复制的改进，在master事务的commit之前，必须确保一个slave收到relay log并且响应给master以后，才能进行事务的commit。但是slave对于relay log的应用仍然是异步进行的 MGR： 由若干个节点共同组成一个复制组，一个事务的提交，必须经过组内大多数节点（N / 2 + 1）决议并通过，才能得以提交。缺点：只支持InnoDB存储引擎；必须有主键；必须打开GTID特性，二进制日志格式必须设置为ROW，用于选主与write set; MySQL 主从同步配置MySQL5.7安装前期准备 123456789101112131415161718# 安装MySQLsudo apt-get updatesudo apt-get install mysql-server# 配置MySQLsudo mysql_secure_installation# 查看密码策略 SHOW VARIABLES LIKE 'validate_password%';# SET global validate_password_policy=LOW;SET global validate_password_length=4;# 查看密码规则 5.7后默认使用auth_socket认证插件SELECT user, authentication_string, plugin, host FROM mysql.user;# 更改策略ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY "root";# 允许所有host的root用户远程连接GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root'; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%';# 允许远程连接vim /etc/mysql/mysql.conf.d/mysqld.cnf # 注释bind-address = 127.0.0.1 主从同步配置 主节点 123456789101112# Masterlog-bin=mysql-bin # 开启二进制日志server-id=1 #设置server-id，需要唯一# 重启MySQLservice mysql restart# 新建同步用户CREATE USER 'MySlave'@'192.168.56.112' IDENTIFIED BY 'root';GRANT REPLICATION SLAVE ON *.* TO 'MySlave'@'192.168.56.112';FLUSH PRIVILEGES; 从节点 123456789101112131415161718# Slavelog-bin=mysql-bin # 开启二进制日志server-id=2 #设置server-id，需要唯一# 重启MySQLservice mysql restart# 主库ip / 主库用户 / 主库密码 / 主库bin log 文件名 / 主库文件偏移量CHANGE MASTER TO MASTER_HOST='192.168.56.111', MASTER_USER='MySlave', MASTER_PASSWORD='root', MASTER_LOG_FILE='mysql-bin.000001', MASTER_LOG_POS=771;START SLAVE;SHOW SLAVE STATUS\G;]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM拆解]]></title>
    <url>%2F2020%2F12%2F18%2FJVM%E6%8B%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[JVM学习笔记 JVM 加载类加载 -&gt; 链接 -&gt; 初始化 加载是指查找字节流，并且据此创建类的过程。加载需要借助类加载器，在 Java 虚拟机中，类加载器使用了双亲委派模型，即接收到加载请求时，会先将请求转发给父类加载器。数组类是JVM虚拟机直接生成的，其他类需要借助类加载器完成查找字节流的过程的。在 Java 虚拟机中，类的唯一性是由类加载器实例以及类的全名一同确定的。即便是同一串字节流，经由不同的类加载器加载，也会得到两个不同的类。 链接链接: 是指将创建成的类合并至 Java 虚拟机中，使之能够执行的过程。它可分为验证、准备以及解析三个阶段。验证: 在于确保被加载类能够满足 Java 虚拟机的约束条件准备: 为被加载类的静态字段分配内存 初始化如果直接赋值的静态字段被 final 所修饰，并且它的类型是基本类型或字符串时，那么该字段便会被 Java 编译器标记成常量值（ConstantValue），其初始化直接由 Java 虚拟机完成。除此之外的直接赋值操作，以及所有静态代码块中的代码，则会被 Java 编译器置于同一方法中，并把它命名为 &lt; clinit &gt;。 JAVA中获取类的方式： 12345678// 四种Calss c1 = Class.forName("com.leezy.top.Coder");Class c2 = Coder.getClass();Class c3 = Coder.class;// 基本内置类型的包装类都有一个Type属性Class&lt;Integer&gt; type = Integer.TYPE;// 获取父类Class superClass = c1.getSuperClass(); JVM处理异常12345678910111213public static void main(String[] args) &#123; try &#123; // 异常监控的代码 &#125; catch() &#123; // catch 代码块所捕获的异常类型不能覆盖后边的，否则编译器会报错 &#125; catch() &#123; &#125; catch() &#123; &#125; finally &#123; // 必定会运行的代码 &#125;&#125; 异常分为检查异常(checked exception)和非检查异常(unchecked exception), RuntimeException 和 Error是非检查异常，其他继承Throwable的Exception都是检查异常，需要程序显式的捕获或者在方法头用throws关键字声明;需要注意的是异常的捕获是比较耗费性能的一件事，这是由于在构造异常实例时，Java 虚拟机便需要生成该异常的栈轨迹(stack trace);在编译生成的字节码中，每个方法都附带一个异常表。异常表中的每一个条目代表一个异常处理器，并且由 from 指针、to 指针、target 指针以及所捕获的异常类型构成。这些指针的值是字节码索引，用以定位字节码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test &#123; private static int tryBlock; private static int catchBlock; private static int finallyBlock; private static int methodExit; public static void main(String[] args) &#123; try &#123; tryBlock = 0; &#125; catch (Exception e) &#123; catchBlock = 1; &#125; finally &#123; finallyBlock = 2; &#125; methodExit = 3; &#125;&#125;// 字节码public class Test &#123; public Test(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public static void main(java.lang.String[]); Code: 0: iconst_0 1: putstatic #2 // Field tryBlock:I 4: iconst_2 5: putstatic #3 // Field finallyBlock:I - 1 8: goto 30 11: astore_1 12: iconst_1 13: putstatic #5 // Field catchBlock:I 16: iconst_2 17: putstatic #3 // Field finallyBlock:I - 2 20: goto 30 23: astore_2 24: iconst_2 25: putstatic #3 // Field finallyBlock:I - 3 28: aload_2 29: athrow 30: iconst_3 31: putstatic #6 // Field methodExit:I 34: return Exception table: from to target type 0 4 11 Class java/lang/Exception 0 4 23 any // 指向复制的finally代码块 11 16 23 any // 指向复制的finally代码块&#125; finally方法一定会执行，因为JVM虚拟机会复制finally代码块的内容，分别放在try-catch代码块所有正常执行路径以及异常执行路径的出口中。 JVM反射机制反射是 Java 语言中一个相当重要的特性，它允许正在运行的 Java 程序观测，甚至是修改程序的动态行为。在默认情况下，方法的反射调用为委派实现，委派给本地实现来进行方法调用。在调用超过 15 次之后，委派实现便会将委派对象切换至动态实现。这个动态实现的字节码是自动生成的，它将直接使用 invoke 指令来调用目标方法。拿到类后如何使用反射 使用 newInstance() 来生成一个该类的实例。它要求该类中拥有一个无参数的构造器。P.S. 可以提高软件的可伸缩性、可扩展性。 使用 isInstance(Object) 来判断一个对象是否该类的实例，语法上等同于 instanceof 关键字。 使用 Array.newInstance(Class,int) 来构造该类型的数组。 使用 getFields()/getConstructors()/getMethods() 来访问该类的成员。方法名中带 Declared 的不会返回父类的成员，但是会返回私有成员；而不带 Declared 的则相反。拿到类后可以做： 使用 Constructor/Field/Method.setAccessible(true) 来绕开 Java 语言的访问限制。 使用 Constructor.newInstance(Object[]) 来生成该类的实例。 使用 Field.get/set(Object) 来访问字段的值。 使用 Method.invoke(Object, Object[]) 来调用方法。 JVM执行方法调用 重载与重写，重载指的是方法名相同而参数类型不相同的方法之间的关系，重写指的是方法名相同并且参数类型也相同的方法之间的关系。如果子类定义了与父类中非私有方法同名的方法，而且这两个方法的参数类型相同，如果这两个方法都是静态的，那么子类中的方法隐藏了父类中的方法。如果这两个方法都不是静态的，且都不是私有的，那么子类的方法重写了父类中的方法。 JAVA编译器对重载方法的选取规则： 在不考虑对基本类型自动装拆箱，以及可变长参数的情况下选取重载方法； 如果在第 1 个阶段中没有找到适配的方法，那么在允许自动装拆箱，但不允许可变长参数的情况下选取重载方法； 如果在第 2 个阶段中没有找到适配的方法，那么在允许自动装拆箱以及可变长参数的情况下选取重载方法。Java识别方法只看方法名和参数类型，而JVM识别方法类名、方法名以及方法描述符。 在Java中，返回类型不一致而其他全部一致不算重载。Java 虚拟机中的静态绑定指的是在解析时便能够直接识别目标方法的情况，而动态绑定则指的是需要在运行过程中根据调用者的动态类型来识别目标方法的情况。 静态绑定和动态绑定具体来说，Java 字节码中与调用相关的指令共有五种。 invokestatic：用于调用静态方法。 invokespecial：用于调用私有实例方法、构造器，以及使用 super 关键字调用父类的实例方法或构造器，和所实现接口的默认方法。 invokevirtual：用于调用非私有实例方法。 invokeinterface：用于调用接口方法。 invokedynamic：用于调用动态方法。 JAVA对象的内存分布JAVA程序中新建对象的方式有： new方法1234567User user = new User();// Foo foo = new Foo(); 编译而成的字节码 0 new Foo 3 dup 4 invokespecial Foo() 7 astore_1 当我们调用一个构造器时，它将优先调用父类的构造器，直至 Object 类。这些构造器的调用者皆为同一对象，也就是通过 new 指令新建而来的对象。通过 new 指令新建出来的对象，它的内存其实涵盖了所有父类中的实例字段。也就是说，虽然子类无法访问父类的私有实例字段，或者子类的实例字段隐藏了父类的同名实例字段，但是子类的实例还是会为这些父类实例字段分配内存的。 反射机制12User user = (User) Class.forName("com.leezy.top.User").newInstance(); // 当需要调用类的带参数的构造函数时，应该采用 Constructor.newInstance()， newInstance创建对象实例的时候仅能调用无参的构造函数，所以必需确保类中有无参数的构造函数，否则将会抛出java.lang.InstantiationException异常，无法进行实例化。 Object.clone需要被克隆的对象需要实现Cloneable接口12User u1 = new User();User u2 = (User)u1.clone(); 需要注意的是，基于克隆(原型模式)创建对象的方式是浅拷贝，如果对象的属性为引用类型，则仅复制地址。 反序列化序列化需要实现Serializable接口，反序列化获取对象的方式如下： 12ObjectInputStream o1 = new ObjectInputStream(new FileInputStream("User.txt"));User u1 = (User)o1.readObject(); Unsafe.allocateInstance 12// 会绕过对象初始化阶段并绕过构造器的安全检查，慎用User instance = (User) UNSAFE.allocateInstance(User.class); 在 Java 虚拟机中，每个 Java 对象都有一个对象头（object header），这个由标记字段和类型指针所构成。其中，标记字段用以存储 Java 虚拟机有关该对象的运行数据，如哈希码、GC 信息以及锁信息，而类型指针则指向该对象的类。在 64 位的 Java 虚拟机中，对象头的标记字段占 64 位，而类型指针又占了 64 位。也就是说，每一个 Java 对象在内存中的额外开销就是 16 个字节。为了节约空间，减少对象内存的使用量，64 位 Java 虚拟机引入了压缩指针的概念（对应虚拟机选项 -XX:+UseCompressedOops，默认开启），对象头中的类型指针也会被压缩成 32 位，使得对象头的大小从 16 字节降至 12 字节。同事默认情况下，Java 虚拟机堆中对象的起始地址需要对齐至 8 的倍数，（对应虚拟机选项 -XX:ObjectAlignmentInBytes，默认值为 8），同时压缩指针会让虚拟机在分配字段的顺序时进行字段重排列。 123456789101112131415161718192021222324252627282930313233343536class A &#123; long l; int i；&#125;class B extends A &#123; long l; int i;&#125;// 启用压缩指针时，B类的字段分布B object internals: OFFSET SIZE TYPE DESCRIPTION 0 4 (object header) 4 4 (object header) 8 4 (object header) 12 4 int A.i 0 16 8 long A.l 0 24 8 long B.l 0 32 4 int B.i 0 36 4 (loss due to the next object alignment)// 关闭压缩指针时，B类的字段分布B object internals: OFFSET SIZE TYPE DESCRIPTION 0 4 (object header) 4 4 (object header) 8 4 (object header) 12 4 (object header) 16 8 long A.l 24 4 int A.i 28 4 (alignment/padding gap) 32 8 long B.l 40 4 int B.i 44 4 (loss due to the next object alignment) JVM垃圾回收机制在JVM中，垃圾就是无引用对象所占用的堆内存空间, 比如对象a和对象b相互引用, 但是再没有其他引用指向a或者b, 此时a和b对象占用的内存空间就是垃圾。目前 Java 虚拟机的主流垃圾回收器采取的是可达性分析算法。这个算法的实质在于将一系列 GC Roots 作为初始的存活对象合集（live set），然后从该合集出发，探索所有能够被该集合引用到的对象，并将其加入到该集合中，这个过程我们也称之为标记（mark）。最终，未被探索到的对象便是死亡的，是可以回收的。传统的垃圾回收算法采用的是一种简单粗暴的方式，那便是 Stop-the-world，停止其他非垃圾回收线程的工作，直到完成垃圾回收。这也就造成了垃圾回收所谓的暂停时间（GC pause）。Java 虚拟机中的 Stop-the-world 是通过安全点（safepoint）机制来实现的。当 Java 虚拟机收到 Stop-the-world 请求，它便会等待所有的线程都到达安全点，才允许请求 Stop-the-world 的线程进行独占的工作。具体垃圾回收的方式： 清除： 即把死亡对象所占据的内存标记为空闲内存，并记录在一个空闲列表（free list）之中。当需要新建对象时，内存管理模块便会从该空闲列表中寻找空闲内存，并划分给新建的对象。一是会造成内存碎片。由于 Java 虚拟机的堆中对象必须是连续分布的，因此可能出现总空闲内存足够，但是无法分配的极端情况。二是分配效率较低。如果是一块连续的内存空间，那么我们可以通过指针加法（pointer bumping）来做分配。而对于空闲列表，Java 虚拟机则需要逐个访问列表中的项，来查找能够放入新建对象的空闲内存。 压缩： 即把存活的对象聚集到内存区域的起始位置，从而留下一段连续的内存空间。这种做法能够解决内存碎片化的问题，但代价是压缩算法的性能开销。 复制：把内存区域分为两等分，分别用两个指针 from 和 to 来维护，并且只是用 from 指针指向的内存区域来分配内存。当发生垃圾回收时，便把存活的对象复制到 to 指针指向的内存区域中，并且交换 from 指针和 to 指针的内容。但缺点是空间使用率低。 JVM堆内存的使用是符合二八原则的，JVM将堆内存划分为了新生代和老生代，新生代用来存储新建的对象。当对象存活时间够长时，则将其移动到老年代。其中，新生代又被划分为Eden区，以及两个大小相同的Survivor区, 非空的那个用form指针指向，空的那个用to指针指向。为了解决Eden区堆内存线程共享导致两个对象共同引用一段内存的问题，JVM使用了一种叫Thread Local Allocation Buffer，对应虚拟机参数 -XX:+UseTLAB，默认开启）。具体来说，每个线程可以向 Java 虚拟机申请一段连续的内存，比如 2048 字节，作为线程私有的 TLAB。这个操作需要加锁，线程需要维护两个指针（实际上可能更多，但重要也就两个），一个指向 TLAB 中空余内存的起始位置，一个则指向 TLAB 末尾。如果加法后空余内存指针的值仍小于或等于指向末尾的指针，则代表分配成功。否则，TLAB 已经没有足够的空间来满足本次新建操作。这个时候，便需要当前线程重新申请新的 TLAB。默认情况下，Java 虚拟机采取的是一种动态分配的策略（对应 Java 虚拟机参数 -XX:+UsePSAdaptiveSurvivorSizePolicy），根据生成对象的速率，以及 Survivor 区的使用情况动态调整 Eden 区和 Survivor 区的比例。当Eden区空间耗尽时，Java 虚拟机便会触发一次 Minor GC，来收集新生代的垃圾。存活下来的对象，则会被送到 Survivor 区。MonitorGC其实就是上面的复制操作。Java 虚拟机会记录 Survivor 区中的对象一共被来回复制了几次。如果一个对象被复制的次数为 15（对应虚拟机参数 -XX:+MaxTenuringThreshold），那么该对象将被晋升（promote）至老年代。另外，如果单个 Survivor 区已经被占用了 50%（对应虚拟机参数 -XX:TargetSurvivorRatio），那么较高复制次数的对象也会被晋升至老年代。MonitorGC避免了垃圾回收中的全堆扫描问题，但是老年代的对象可能引用新生代的对象还是会导致全堆扫描，所以JVM引入了一个卡表的概念，即将整个堆划分为一个个大小为 512 字节的卡，并且维护一个卡表，用来存储每张卡的一个标识位。这个标识位代表对应的卡是否可能存有指向新生代对象的引用。如果可能存在，那么我们就认为这张卡是脏的。在进行 Minor GC 的时候，我们便可以不用扫描整个老年代，而是在卡表中寻找脏卡，并将脏卡中的对象加入到 Minor GC 的 GC Roots 里。当完成所有脏卡的扫描之后，Java 虚拟机便会将所有脏卡的标识位清零。 JAVA常用工具 jps 打印所有正在运行的 Java 进程的相关信息 jstat 打印目标 Java 进程的性能数据 1jstat -gcutil 17079 5000 10 jmap 允许用户统计目标 Java 进程的堆中存放的 Java 对象，并将它们导出成二进制文件。配合eclipse MAT使用比较好，可以图形展示。 123# -clstats，该子命令将打印被加载类的信息。# -histo 该子命令将统计各个类的实例数目以及占用内存，并按照内存使用量从多至少的顺序排列。此外，-histo:live只统计堆中的存活对象。jmap -dump:live,format=b,file=filename.bin jinfo 查看和修改目标 Java 进程的参数 jstack 用来打印目标 Java 进程中各个线程的栈轨迹，以及这些线程所持有的锁。 javap java-&gt;javac-&gt;javap 查阅 Java 字节码 OPENJDK工具集http://openjdk.java.net/projects/code-tools/ jcmd 可以替换除了jstat外的所有命令https://docs.oracle.com/en/java/javase/11/tools/jcmd.html ASM Java Mission Control 12# JFR 将在 Java 虚拟机启动之后持续收集数据，直至进程退出java -XX:StartFlightRecording=dumponexit=true,filename=myrecording.jfr MyApp]]></content>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty框架学习]]></title>
    <url>%2F2020%2F04%2F18%2FNetty%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven学习记录]]></title>
    <url>%2F2020%2F04%2F13%2FMaven%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[MAVEN常用基础知识。 命令可以查看maven的依赖树，以树状结构输出打印maven的依赖。 1mvn dependency:tree Maven发布jar包的功能，默认该插件已经配置在Maven的deploy阶段使用，而且该插件也没有配置参数，所以无需在项目的POM中配置该插件，直接执行如下命令即可。 1mvn deploy org.apache.maven.plugins maven-deploy-plugin 3.0.0-M1 true maven-source-plugin提供项目自动将源码打包并发布的功能，而这个contract模块就是打成了三个jar包，注意在多项目模块中，必须将maven-source-plugin和maven-javadoc-plugin配置在具体模块的Pom中，否则不起作用。 12# 发布到Maven仓库，多模块项目注意要在项目的根目录执行才能将父POM也上传，否则jar包不能正常下载。mvn clean deploy -Dmaven.test.skip=true -Dnexus.host=$&#123;NEXUS_HOST&#125; 123456789101112131415161718192021222324252627282930313233&lt;build&gt;&lt;plugins&gt; &lt;!--用来生成Source Jar文件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--用来生成 javadoc 文档--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt; maven发布jar包的常用命令 123mvn install - 将jar包install到本地的maven仓库mvn deploy - 将jar包deploy到远程maven仓库mvn source:jar - 单独打包源码]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识-异步回调模式]]></title>
    <url>%2F2020%2F03%2F18%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-Future%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[异步回调模式。 JOIN异步阻塞操作原理：阻塞当前的线程，直到准备合并的目标线程的执行完成；即线程A调用了线程B的join方法，合并线程B，线程A则进入阻塞状态，直到线程B执行完成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.leezy.future;/** * @program: NIOStudy * @description: 异步阻塞JOIN * @author: LEEZY * @create: 2020-03-18 15:28 **/public class JoinDemo &#123; public static final int SLEEP_GAP = 500; public static String getCurThreadName() &#123; return Thread.currentThread().getName(); &#125; static class HotWaterThread extends Thread &#123; public HotWaterThread() &#123; super("烧水线程"); &#125; public void run() &#123; try &#123; Thread.sleep(SLEEP_GAP); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; static class WashThread extends Thread &#123; public WashThread() &#123; super("清洗线程"); &#125; public void run() &#123; try &#123; Thread.sleep(SLEEP_GAP); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread hotWaterThread = new HotWaterThread(); Thread washThread = new WashThread(); hotWaterThread.start(); washThread.start(); try &#123; // 主线程阻塞，开启烧水和清洗线程 // 合并烧水线程 hotWaterThread.join(); // 合并清洗线程 washThread.join(); Thread.currentThread().setName("主线程"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; join方法是有三个重载版本： void join():A线程等待B线程执行结束后，A线程重新恢复执行。 void join(long millis):A线程等待B线程执行一段时间，最长等待时间为millis毫秒。超过millis毫秒后，不论B线程是否结束，A线程重新恢复执行。 void join(long millis, int nanos)：等待B线程执行一段时间，最长等待时间为millis毫秒，加nanos纳秒。超过时间后，不论B线程是否结束，A线程重新恢复执行。 JOIN被合并的线程没有返回值，如果需要异步线程的执行结果，就需要用到Java的FutureTask系列类。 FutureTask异步回调 Callable接口：Callable接口是个泛型接口，与Runnable接口类似，唯一的区别是，其抽象方法call有返回值，返回值的类型为泛型形参的实际类型。但是Callable接口的实例不能作为Thread线程实例的target来使用，而Runnable接口实例可以作为Thread线程实例的target构造参数，开启一个Thread线程。其内部进行的是异步执行的逻辑。 123456789101112package java.util.concurrent;@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;&#125; FutureTask类：就像一座搭在Callable实例与Thread线程实例之间的桥。FutureTask类的内部封装一个Callable实例，然后自身间接继承了Runnable接口可以作为Thread线程的target。 12345678public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.leezy.future;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;/** * @program: NIOStudy * @description: FutureTask类实现喝茶实例 * @author: LEEZY * @create: 2020-03-19 11:40 **/public class JavaFutureDemo &#123; public static final int SLEEP_GAP = 500; public static String getCurThreadName() &#123; return Thread.currentThread().getName(); &#125; // 实现Callable接口，并返回异步线程执行结果 static class HotWaterJob implements Callable&lt;Boolean&gt; &#123; @Override public Boolean call() throws Exception &#123; try &#123; Thread.sleep(SLEEP_GAP); &#125; catch (InterruptedException e) &#123; return false; &#125; return true; &#125; &#125; static class WashJob implements Callable&lt;Boolean&gt; &#123; @Override public Boolean call() throws Exception &#123; try &#123; Thread.sleep(SLEEP_GAP); &#125; catch (InterruptedException e) &#123; return false; &#125; return true; &#125; &#125; public static void drinkTea(boolean waterOK, boolean teacupOK) &#123; if (waterOK &amp;&amp; teacupOK) &#123; System.out.println("喝茶"); &#125; else if (!waterOK) &#123; System.out.println("烧水失败"); &#125; else &#123; System.out.println("洗杯子失败"); &#125; &#125; public static void main(String[] args) &#123; // 异步逻辑 Callable&lt;Boolean&gt; hotWaterJob = new HotWaterJob(); // 创建FutureTask实例，创建新的线程 FutureTask&lt;Boolean&gt; hotWaterTask = new FutureTask&lt;&gt;(hotWaterJob); Thread hotWaterThread = new Thread(hotWaterTask, "烧水线程"); Callable&lt;Boolean&gt; washJob = new WashJob(); FutureTask&lt;Boolean&gt; washTask = new FutureTask&lt;&gt;(washJob); Thread washThread = new Thread(washTask, "清洁线程"); hotWaterThread.start(); washThread.start(); Thread.currentThread().setName("主线程"); try &#123; Boolean waterOK = hotWaterTask.get(); Boolean teacupOK = washTask.get(); drinkTea(waterOK, teacupOK); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; P.S.FutureTask和Callable都是泛型类，泛型参数表示返回结果的类型。所以，在使用的时候，它们两个实例的泛型参数一定需要保持一致 Future接口Java将FutureTask类的一系列操作，抽象出来作为一个重要的接口，Future接口。主要提供了三个功能 判断并发任务是否执行完成 获取并发的任务完成后的结果 取消并发执行中的任务 123456789101112public interface Future&lt;V&gt; &#123; // 取消并发任务执行 boolean cancel(boolean mayInterruptIfRunning); // 获取并发任务取消状态 boolean isCancelled(); // 获取并发任务执行状态 boolean isDone(); // 获取并发任务执行结果；阻塞性的，如果并发任务没有执行完成，调用该方法会一直阻塞直到并发任务执行完成 V get() throws InterruptedException, ExecutionException; // 获取并发任务执行结果；阻塞性的，如果阻塞时间超过设定的timeout时间，该方法会抛出异常 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 以上2种办法，通过FutureTask类和Join方法都是异步阻塞模式，效率都是比较低的。 Guava的异步调用Guava增强了java.util.concurrent包，为了实现非阻塞获取异步线程的结果，Guava对Java的异步回调机制做了2个方面的增强。 ListenableFuture，继承了Java的Future接口，使Java的Future异步任务在Guava中能被监控和获取非阻塞异步执行的结果。 FutureCallback，新接口，该接口的目的是在异步任务执行完成后，根据异步结果，完成不同的回调处理，可以处理异步结果。 FutureCallBack onSuccess()： 在异步任务执行成功后被回调；调用时，异步任务的执行结果，作为onSuccess方法的参数被传入。 onFailure()：在异步任务执行过程中，抛出异常时被回调；调用时，异步任务所抛出的异常，作为onFailure方法的参数被传入。 12345public interface FutureCallback&lt;V&gt; &#123; void onSuccess(@Nullable V var1); void onFailure(Throwable var1);&#125; ListenableFuture 继承自Java的Future接口，增加了一个addListener方法，作用是将FutureCallback的回调封装成一个内部的Runnable异步回调任务，在Callable异步任务完成后，回调FutureCallback进行处理。 在实际编程中，将FutureCallback回调逻辑绑定到ListenableFuture的异步任务，可以通过Guava的Futures工具类的addCallback静态方法。 获取Guava的ListenableFuture异步任务实例，主要通过线程池ThreadPool提交Callable任务的方式来获取。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public static void nativeFuture() throws Exception &#123; // Java自带的Future模式，实现异步 ExecutorService nativeExecutor = Executors.newSingleThreadExecutor(); Future&lt;String&gt; nativeFuture = nativeExecutor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; // 使用sleep模拟调用耗时 TimeUnit.SECONDS.sleep(1); return "[" + Thread.currentThread().getName() + "]: 并发包Future返回结果"; &#125; &#125;); // Future只实现了异步，没有实现回调。此时主线程get结果时阻塞，可以轮询获取异步调用是否完成 System.out.println("[" + Thread.currentThread().getName() + "] ==&gt;" + nativeFuture.get());&#125;public static void guavaFuture() &#123; // Guava异步回调 ExecutorService executorService = Executors.newSingleThreadExecutor(); ListeningExecutorService guavaExecutor = MoreExecutors.listeningDecorator(executorService); final ListenableFuture&lt;String&gt; listenableFuture = guavaExecutor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; TimeUnit.SECONDS.sleep(1); return "[" + Thread.currentThread().getName() + "]: guava的Future返回结果"; &#125; &#125;); // 注册监听器，即异步调用完成时回在指定的线程Executors.newSingleThreadExecutor()中执行注册的监听器 listenableFuture.addListener(new Runnable() &#123; @Override public void run() &#123; try &#123; String str = "[" + Thread.currentThread().getName() + "]: guava对返回结果进行异步CallBack(Runnable):" + listenableFuture.get(); System.out.println(str); &#125; catch (ExecutionException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, Executors.newSingleThreadExecutor()); // 主线程可以继续执行，异步完成后会执行注册的监听器任务. System.out.println("[" + Thread.currentThread().getName() + "]: guavaFuture执行结束");&#125;public static void guavaFuture2() &#123; // 除了ListenableFuture，guava还提供了FutureCallback接口 ExecutorService executorService2 = Executors.newSingleThreadExecutor(); ListeningExecutorService guavaExecutor2 = MoreExecutors.listeningDecorator(executorService2); final ListenableFuture&lt;String&gt; listenableFuture2 = guavaExecutor2.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; TimeUnit.SECONDS.sleep(1); return "[" + Thread.currentThread().getName() + "]: guava的Future返回结果"; &#125; &#125;); Futures.addCallback(listenableFuture2, new FutureCallback&lt;String&gt;() &#123; @Override public void onSuccess(@Nullable String result) &#123; String str = "[" + Thread.currentThread().getName() + "]=======&gt;对回调结果【" + result + "】进行FutureCallback"; System.out.println(str); &#125; @Override public void onFailure(Throwable throwable) &#123; &#125; &#125;, Executors.newSingleThreadExecutor()); // 主线程可以继续执行,异步完成后会执行注册的监听器任务. System.out.println( "[" + Thread.currentThread().getName() +"]: guavaFuture2执行结束");&#125; 执行结果： script12345[main] ==&gt;[pool-1-thread-1]: 并发包Future返回结果[main]: guavaFuture执行结束[pool-3-thread-1]: guava对返回结果进行异步CallBack(Runnable):[pool-2-thread-1]: guava的Future返回结果[main]: guavaFuture2执行结束[pool-5-thread-1]=======&gt;对回调结果【[pool-4-thread-1]: guava的Future返回结果】进行FutureCallback Netty的异步回调模式Netty对JavaFuture异步任务拓展如下： 继承Java的Future接口； 定义GenericFutureListener接口，异步执行结果监听器。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识-Reactor反应器模式]]></title>
    <url>%2F2020%2F03%2F15%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-Reactor%E5%8F%8D%E5%BA%94%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[高性能的网络编程都离不开反应器模式，Nginx、Redis、Netty都采用了反应器模式。 Reactor反应器模式反应器模式由Reactor反应器线程、Handlers处理器两大角色组成： Reactor反应器线程的职责：负责响应NIO选择器监控的IO事件，并且分发到Handlers处理器。 Handlers处理器的职责：非阻塞的执行业务处理逻辑。 单线程的Reactor反应器 void attach(Object object): 此方法可以将任何Java的POJO对象作为附件添加到SelectionKey实例 Object attachement(): 此方法的作用是取出通过attach(Object)添加到SelectionKey选择键实例的附件 代码演示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// Reactor.javaimport java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;/** * @program: NIOStudy * @description: 单线程Reactor反应器模型 * @author: LEEZY * @create: 2020-03-17 10:39 **/public class Reactor implements Runnable&#123; final Selector selector; final ServerSocketChannel serverSocket; Reactor(int port) throws IOException &#123; // 打开选择器、ServerSocketChannel连接监听通道 selector = Selector.open(); serverSocket = ServerSocketChannel.open(); serverSocket.socket().bind(new InetSocketAddress(port)); serverSocket.configureBlocking(false); SelectionKey selectionKey = serverSocket.register(selector, SelectionKey.OP_ACCEPT); // 将新连接处理器作为附件，绑定到serverSocket选择键 selectionKey.attach(new AcceptorHandler()); &#125; @Override public void run() &#123; // 选择器轮询 try &#123; while(!Thread.interrupted()) &#123; selector.select(); Set selected = selector.selectedKeys(); Iterator iterator = selected.iterator(); while(iterator.hasNext()) &#123; // 反应器负责dispatch收到的事件 SelectionKey selectionKey = (SelectionKey) iterator.next(); dispatcher(selectionKey); &#125; &#125; &#125; catch(IOException e) &#123; &#125; &#125; // 反应器分发类 void dispatcher(SelectionKey selectionKey) &#123; Runnable handler = (Runnable) (selectionKey.attachment()); if (handler != null) &#123; handler.run(); &#125; &#125; // 新连接处理类 class AcceptorHandler implements Runnable &#123; @Override public void run() &#123; try &#123; SocketChannel socketChannel = serverSocket.accept(); if (socketChannel != null) &#123; // 调用Handler的构造方法，将SocketChannel注册到反应器Reactor类的同一个选择器，保证Reactor类和Handler类在同一个线程执行 new Handler(selector, socketChannel); &#125; &#125; catch (IOException e) &#123; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// Handler.javaimport com.sun.media.jfxmedia.logging.Logger;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;/** * @program: NIOStudy * @description: 处理器 * @author: LEEZY * @create: 2020-03-17 12:08 **/final class Handler implements Runnable &#123; final SocketChannel socketChannel; final SelectionKey selectionKey; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); static final int READING = 0, SENDING = 1; int state = READING; Handler(Selector selector, SocketChannel socketChannel) throws IOException &#123; this.socketChannel = socketChannel; socketChannel.configureBlocking(false); // 设置感兴趣的IO事件 selectionKey = socketChannel.register(selector, 0); // 将Handler自身作为选择键的附件，这样在Reactor类分发事件时能执行到该Handler的run方法 selectionKey.attach(this); // 注册Read就绪事件 selectionKey.interestOps(SelectionKey.OP_READ); selector.wakeup(); &#125; @Override public void run() &#123; try &#123; if (state == SENDING) &#123; // 写入通道 socketChannel.write(byteBuffer); // 转换为写入模式 byteBuffer.clear(); // 写入完成后注册read就绪事件 selectionKey.interestOps(SelectionKey.OP_READ); // 更改状态 state = READING; &#125; else if (state == READING) &#123; int length = 0; // 从通道读取 while((length = socketChannel.read(byteBuffer)) &gt; 0) &#123; System.out.println(new String(byteBuffer.array(), 0, length)); &#125; byteBuffer.flip(); selectionKey.interestOps(SelectionKey.OP_WRITE); state = SENDING; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 多线程Reactor反应器总体来说，多线程池反应器的模式，大致如下： 将负责输入输出处理的IOHandler处理器的执行，放入独立的线程池中。这样，业务处理线程与负责服务监听和IO事件查询的反应器线程相隔离，避免服务器的连接监听受到阻塞。 如果服务器为多核的CPU，可以将反应器线程拆分为多个子反应器（SubReactor）线程；同时，引入多个选择器，每一个SubReactor子线程负责一个选择器。这样，充分释放了系统资源的能力；也提高了反应器管理大量连接，提升选择大量通道的能力。 多线程Reactor反应器实践 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687//...．反应器class MultiThreadEchoServerReactor &#123; ServerSocketChannelserverSocket; AtomicInteger next = new AtomicInteger(0); //选择器集合，引入多个选择器 Selector[] selectors = new Selector[2]; //引入多个子反应器 SubReactor[] subReactors = null; MultiThreadEchoServerReactor() throws IOException &#123; //初始化多个选择器 selectors[0] = Selector.open(); selectors[1] = Selector.open(); serverSocket = ServerSocketChannel.open(); InetSocketAddress address = new InetSocketAddress(NioDemoConfig.SOCKET_SERVER_IP, NioDemoConfig.SOCKET_SERVER_PORT); serverSocket.socket().bind(address); //非阻塞 serverSocket.configureBlocking(false); //第一个选择器，负责监控新连接事件 SelectionKeysk = serverSocket.register(selectors[0], SelectionKey.OP_ACCEPT); //绑定Handler:attach新连接监控handler处理器到SelectionKey（选择键） sk.attach(new AcceptorHandler()); //第一个子反应器，一子反应器负责一个选择器 SubReactor subReactor1 = new SubReactor(selectors[0]); //第二个子反应器，一子反应器负责一个选择器 SubReactor subReactor2 = new SubReactor(selectors[1]); subReactors = new SubReactor[]&#123;subReactor1, subReactor2&#125;; &#125; private void startService() &#123; // 一子反应器对应一个线程 new Thread(subReactors[0]).start(); new Thread(subReactors[1]).start(); &#125; //子反应器 class SubReactor implements Runnable &#123; //每个线程负责一个选择器的查询和选择 final Selector selector; public SubReactor(Selector selector) &#123; this.selector = selector; &#125; public void run() &#123; try &#123; while (! Thread.interrupted()) &#123; selector.select(); Set&lt;SelectionKey&gt;keySet = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keySet.iterator(); while (it.hasNext()) &#123; //反应器负责dispatch收到的事件 SelectionKeysk = it.next(); dispatch(sk); &#125; keySet.clear(); &#125; &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; void dispatch(SelectionKeysk) &#123; Runnable handler = (Runnable) sk.attachment(); //调用之前attach绑定到选择键的handler处理器对象 if (handler ! = null) &#123; handler.run(); &#125; &#125; &#125; // Handler：新连接处理器 class AcceptorHandler implements Runnable &#123; public void run() &#123; try &#123; SocketChannel channel = serverSocket.accept(); if (channel ! = null) new MultiThreadEchoHandler(selectors[next.get()], channel); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (next.incrementAndGet() == selectors.length) &#123; next.set(0); &#125; &#125; &#125; public static void main(String[] args) throws IOException &#123; MultiThreadEchoServerReactor server = new MultiThreadEchoServerReactor(); server.startService(); &#125;&#125; 多线程Handler处理器实践 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MultiThreadEchoHandler implements Runnable &#123; final SocketChannel channel; final SelectionKeysk; final ByteBufferbyteBuffer = ByteBuffer.allocate(1024); static final int RECIEVING = 0, SENDING = 1; int state = RECIEVING; //引入线程池 static ExecutorService pool = Executors.newFixedThreadPool(4); MultiThreadEchoHandler(Selector selector, SocketChannel c) throwsIOException &#123; channel = c; c.configureBlocking(false); //取得选择键，、再设置感兴趣的IO事件 sk = channel.register(selector, 0); //将本Handler作为sk选择键的附件，方便事件分发（dispatch） sk.attach(this); //向sk选择键注册Read就绪事件 sk.interestOps(SelectionKey.OP_READ); selector.wakeup(); &#125; public void run() &#123; //异步任务，在独立的线程池中执行 pool.execute(new AsyncTask()); &#125; //业务处理，不在反应器线程中执行 public synchronized void asyncRun() &#123; try &#123; if (state == SENDING) &#123; //写入通道 channel.write(byteBuffer); //写完后，准备开始从通道读，byteBuffer切换成写入模式 byteBuffer.clear(); //写完后，注册read就绪事件 sk.interestOps(SelectionKey.OP_READ); //写完后，进入接收的状态 state = RECIEVING; &#125; else if (state == RECIEVING) &#123; //从通道读 int length = 0; while ((length = channel.read(byteBuffer)) &gt; 0) &#123; Logger.info(new String(byteBuffer.array(), 0, length)); &#125; //读完后，准备开始写入通道，byteBuffer切换成读取模式 byteBuffer.flip(); //读完后，注册write就绪事件 sk.interestOps(SelectionKey.OP_WRITE); //读完后，进入发送的状态 state = SENDING; &#125; //处理结束了，这里不能关闭select key，需要重复使用 //sk.cancel(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; //异步任务的内部类 class AsyncTask implements Runnable &#123; public void run() &#123; MultiThreadEchoHandler.this.asyncRun(); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识-NIO]]></title>
    <url>%2F2020%2F03%2F09%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-NIO%2F</url>
    <content type="text"><![CDATA[Java中IO模型。 常用的四种IO模型同步阻塞IO在Java应用程序进程中，默认情况下，所有的socket连接的IO操作都是同步阻塞IO(BlockingIO);在阻塞式IO模型中，Java应用程序从IO系统调用开始，直到系统调用返回，在这段时间内，Java进程是阻塞的。返回成功后，应用进程开始处理用户空间的缓存区数据。在Java中发起一个socket的read读操作的系统调用，流程大致如下： 从Java启动IO读read系统调用开始，用户线程就进入阻塞状态。 当系统内核收到read系统调用，就开始准备数据。一开始，数据可能还没开始到达内核缓冲区（例如，还没有收到一个完整的socket数据包），这个时候内核就要等待。 内核一直等到完整的数据到达，就会将数据从内核缓冲区中复制到用户缓冲区（用户空间的内存），然后内核返回结果（例如返回复制到用户缓冲区中的字节数）。 直到内核返回后，用户线程才会接触阻塞的状态，重新运行起来。总之，阻塞IO的特点是：在内核进行IO执行的两个阶段，用户线程都被阻塞了。 同步非阻塞NIOsocket连接默认是阻塞模式，在Linux系统下，可以通过设置将socket变成为非阻塞的模式（Non-Blocking）。使用非阻塞模式的IO读写，叫作同步非阻塞IO（None Blocking IO），简称为NIO模式。在NIO模型中，应用程序一旦开始IO系统调用，会出现以下两种情况： 在内核数据没有准备好的阶段，用户线程发起IO请求时，立即返回。所以，为了读取到最终的数据，用户线程需要不断地发起IO系统调用。 在内核缓冲区中有数据的情况下，是阻塞的，直到数据从内核缓冲复制到用户进程缓冲。复制完成后，系统调用返回成功，应用进程开始处理用户空间的缓存数据。 举个例子。发起一个非阻塞socket的read读操作的系统调用，流程如下： 在内核数据没有准备好的阶段，用户线程发起IO请求时，立即返回。所以，为了读取到最终的数据，用户线程需要不断地发起IO系统调用。 内核数据到达后，用户线程发起系统调用，用户线程阻塞。内核开始复制数据，它会将数据从内核缓冲区复制到用户缓冲区（用户空间的内存），然后内核返回结果（例如返回复制到的用户缓冲区的字节数）。 用户线程读到数据后，才会解除阻塞状态，重新运行起来。也就是说，用户进程需要经过多次的尝试，才能保证最终真正读到数据，而后继续执行。同步非阻塞IO的特点：应用程序的线程需要不断地进行IO系统调用，轮询数据是否已经准备好，如果没有准备好，就继续轮询，直到完成IO系统调用为止。同步非阻塞IO的优点：每次发起的IO系统调用，在内核等待数据过程中可以立即返回。用户线程不会阻塞，实时性较好。同步非阻塞IO的缺点：不断地轮询内核，这将占用大量的CPU时间，效率低下。 IO多路复用模型举个例子来说明IO多路复用模型的流程。发起一个多路复用IO的read读操作的系统调用，流程如下： 选择器注册。在这种模式中，首先，将需要read操作的目标socket网络连接，提前注册到select/epoll选择器中，Java中对应的选择器类是Selector类。然后，才可以开启整个IO多路复用模型的轮询流程。 就绪状态的轮询。通过选择器的查询方法，查询注册过的所有socket连接的就绪状态。通过查询的系统调用，内核会返回一个就绪的socket列表。当任何一个注册过的socket中的数据准备好了，内核缓冲区有数据（就绪）了，内核就将该socket加入到就绪的列表中。当用户进程调用了select查询方法，那么整个线程会被阻塞掉。 用户线程获得了就绪状态的列表后，根据其中的socket连接，发起read系统调用，用户线程阻塞。内核开始复制数据，将数据从内核缓冲区复制到用户缓冲区。 复制完成后，内核返回结果，用户线程才会解除阻塞的状态，用户线程读取到了数据，继续执行。 IO多路复用模型的优点：与一个线程维护一个连接的阻塞IO模式相比，使用select/epoll的最大优势在于，一个选择器查询线程可以同时处理成千上万个连接（Connection）。系统不必创建大量的线程，也不必维护这些线程，从而大大减小了系统的开销。Java语言的NIO（New IO）技术，使用的就是IO多路复用模型。在Linux系统上，使用的是epoll系统调用。 IO多路复用模型的缺点：本质上，select/epoll系统调用是阻塞式的，属于同步IO。都需要在读写事件就绪后，由系统调用本身负责进行读写，也就是说这个读写过程是阻塞的。 异步IO模型 - AIO (Asynchronous IO)在异步IO模型中，在整个内核的数据处理过程中，包括内核将数据从网络物理设备（网卡）读取到内核缓冲区、将内核缓冲区的数据复制到用户缓冲区，用户程序都不需要阻塞。发起一个异步IO的read读操作的系统调用，流程如下： 当用户线程发起了read系统调用，立刻就可以开始去做其他事情，用户线程不阻塞。 内核就开始了IO的第一个阶段：准备数据。等到数据准备好了，内核就会将数据从内核缓冲区复制到用户缓冲区（用户空间的内存）。 内核会给用户线程发送一个信号（Signal），或者回调用户线程注册的回调接口，告诉用户线程read操作完成了。 用户线程读取用户缓冲区的数据，完成后续的业务操作。 异步IO模型的特点：在内核等待数据和复制数据的两个阶段，用户线程都不是阻塞的。用户线程需要接收内核的IO操作完成的事件，或者用户线程需要注册一个IO操作完成的回调函数。正因为如此，异步IO有的时候也被称为信号驱动IO。 异步IO异步模型的缺点：应用程序仅需要进行事件的注册与接收，其余的工作都留给了操作系统，也就是说，需要底层内核提供支持。 JAVA NIO 模型Java NIO由以下三个核心组件组成： Channel (通道) Buffer (缓冲区) Selector (选择器) 从Java 1.4版本之后，Java的IO类库从阻塞IO升级为了非阻塞IO，即-JAVA NIO(New IO)，底层使用的是IO多路复用模型。 NIO与OIO的区别，主要体现在三个方面： OIO是面向流的，NIO是面向缓冲区的。 OIO操作中，我们以流式的方式顺序地从一个流（stream）中读取字节，不能随意改变读取指针的位置。在NIO中，引入了Channel和Buffer的概念，读取和写入只需要从通道中读取数据到缓冲区，或将数据从缓冲区中写入到通道中。 OIO的操作是阻塞的，而NIO的操作是非阻塞的。 OIO的阻塞体现在调用一个read方法读取一个文件内容，那么调用read的线程会被阻塞，直到read操作完成。 OIO没有选择器，而NIO是有选择器的概念的。 NIO的实现，是基于底层的选择器的系统调用。NIO的选择器，需要底层操作系统提供支持, 而OIO不需要用到选择器。 NIO Buffer类Buffer类是一个非线程安全的类，Buffer类是一个抽象类，对应于Java的主要数据类型，在NIO中有8种缓冲区类，分别如下：ByteBuffer、CharBuffer、DoubleBuffer、FloatBuffer、IntBuffer、LongBuffer、ShortBuffer、MappedByteBuffer。前7种Buffer类型，覆盖了能在IO中传输的所有的Java基本数据类型。第8种类型MappedByteBuffer是专门用于内存映射的一种ByteBuffer类型。Buffer类的属性： capacity （容量）： capacity容量指的是写入的数据对象的数量； positiohn （读写位置）： 缓冲区中喜爱一个要被读或者写的元素的索引； limit （上限）： 缓冲区当前的数据量； mark （标记）：调用mark()方法设置mark=position，再调用reset()可以让position恢复到mark标记的位置即postion=mark; NIO Buffer的操作方法 allocate() 创建缓冲区 put() 写入到缓冲区；要写入缓冲区，需要调用put方法。put方法很简单，只有一个参数，即为所需要写入的对象。不过，写入的数据类型要求与缓冲区的类型保持一致。 flip() 读写模式反转；调用flip方法后，之前写入模式的position的值会变成可读上限的值，新的读取模式下的position，会变成0，表示从头开始读取。清除之前的mark标记，因为mark保存的是写模式下的临时位置 get() 从缓冲区读取；读取操作会改变可读位置position的值，而limit值不会改变。如果position值和limit的值相等，表示所有数据读取完成，position指向了一个没有数据的元素位置，已经不能再读了。此时再读，会抛出BufferUnderflowException异常。 rewind() 数据倒带；已经读完的数据，如果需要再读一遍，可以调用rewind()方法。rewind调整了缓冲区position属性，position重置为0，可以重读缓冲区中所有的数据，limit保持不变，数据量还是一样的，仍然表示能从缓冲区中读取多少个元素。 mark()和reset() mark()将当前的position的值保存起来，放到mark属性中，reset()方法将mark的值恢复到position中。 clear() 清空缓冲区；在读取模式下，调用clear方法将缓冲区切换为写入模式，此方法会将position清零，limit设置为capacity最大容量值，可以一直写入，直到缓冲区写满。 NIO Channel类最为重要的四种Channel（通道）实现：FileChannel、SocketChannel、ServerSocketChannel、DatagramChannel。 FileChannel文件通道 获取FileChannel文件通道 123456789// 创建文件输入流FileInputStream fileInputStream = new FileInputStream("filePath");// 获取文件流的通道FileChannel inChannel = fileInputStream.getChannel();// 创建文件输出流FileOutputStream fileOutputStream = new FileOutputStream("filePath");// 获取文件流通道FileChannel outChannel = fileOutputStream.getChannel(); 读取FileChannel通道 在大部分应用场景从通道读取数据都会调用通道的int read（ByteBufferbuf）方法，它从通道读取到数据写入到ByteBuffer缓冲区，并且返回读取到的数据量。 1234// 获取一个字节缓冲区 注意，新建的ByteBuffer默认是写入模式。在读取数据时需要调用flip或者clear方法切换ByteBuffer byteBuffer = ByteBuffer.allocate(20);// 调用通道的read方法，读取数据并传入字节类型的缓冲区int length = inChannel.read(byteBuffer); 写入FileChannel通道 写入数据到通道，在大部分应用场景，都会调用通道的int write（ByteBufferbuf）方法。此方法的参数——ByteBuffer缓冲区，是数据的来源。write方法的作用，是从ByteBuffer缓冲区中读取数据，然后写入到通道自身，而返回值是写入成功的字节数。 12byteBuffer.flip();int length = outChannel.write(byteBuffer); 关闭通道 当通道使用完成后，必须将其关闭。 1channel.close(); 强制刷新到磁盘 由于性能原因，要保证写入的通道的缓存数据最终都写入磁盘，要调用FileChannel的force()方法。 1channel.force(true); SocketChannel套接字通道/ServerSocketChannel在NIO中，涉及网络连接的通道有两个，一个是SocketChannel负责连接传输，一个ServerSocketChannel负责连接监听。NIO的SocketChannel对应OIO的Socket类， 一般同时位于服务器端和客户端。对应于一个连接，两端都有一个负责传输的SocketChannel。NIO的ServerSocket对应OIO的ServerSocket类，一般位于服务器端。无论是SocketChannel还是ServerSocketChannel都支持阻塞和非阻塞两种模式，调用configureBlocking方法。socketChannel.configureBlocking(false)设置为非阻塞模式，socketChannel.configureBlocking(true)设置为阻塞模式。 获取SocketChannel传输通道12345678910111213/* 客户端 */// 获得一个套接字传输通道SocketChannel socketChannel = SocketChannel.open();// 设置为非阻塞模式socketChannel.configureBlocking(false);// 对服务器的IP和端口发起连接socketChannel.connect(new InetSocketAddress("127.0.0.1", 80));```java非阻塞情况下，与服务器的连接可能还没有真正建立，socketChannel.connect方法就返回了，因此需要不断地自旋，检查当前是否是连接到了主机：```javawhile(!socketChannel.finishConnect()) &#123; //...&#125; 1234567/* 服务器端 */// 通过事件，获取服务器监听通道ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();// 获取新连接的套接字通道SocketChannel socketChannel = serverSocketChannel.accept();// 设置为非阻塞模式socketChannel.configureBlocking(false); 读取SocketChannel传输通道 1234// 获取一个字节缓冲区 - 写入模式ByteBuffer byteBuffer = ByteBuffer.allocate(20);// 如果返回-1，表示读取到了对方的输出结束标志int read_length = socketChannel.read(byteBuffer); 写入到SocketChannel传输通道 123// 默认的写入模式切换为读取模式byteBuffer.flip();socketChannel.write(buffer); 关闭SocketChannel传输通道1234// 终止输出方法，向对方发送一个输出的结束标志socketChannel.shutdownOutput();// 关闭套接字连接IOUtil.closeQuietly(socketChannel); DatagramChannel数据报通道DatagramChannel是采用UDP进行传输的面向非连接的协议，只要直到服务器的IP和端口，就可以直接向对方发送数据。 获取DatagramChannel数据报通道 12345// 获取通道DatagramChannel datagramChannel = DatagramChannel.open();// 设置为非阻塞模式datagramChannel.configureBlocking(false);datagramChannel.bind(new InetSocketAddress(18080)); 读取DatagramChannel数据报通道数据 12ByteBuffer byteBuffer = ByteBuffer.allocate(1024);SocketAddress address = datagramChannel.receive(byteBuffer); 写入DatagramChannel数据通道 123456// 把缓冲区切换到读取模式byteBuffer.flip()// 调用send方法，把数据发送到目标IP和端口datagramChannel.send(byteBuffer, new InetSocketAddress(NIODemoConfig.SOCKET_SERVER_IP, NIODemoConfig.SOCKET+SERVER_PORT));// 清空缓冲区，切换到写入模式byteBuffer,clear(); 关闭DatagramChannel数据报通道 1datagramChannel.close(); NIO Selector 选择器选择器的作用是完成IO的多路复用，一个通道代表一个连接通路，通过选择器可以同时监控多个通道的IO状况，选择器和通道的关系，是监控和被监控的关系。通道和选择器之间的关系，通过register（注册）的方式完成。调用通道的Channel.register(Selector selector, int ops)方法，可以将通道实例注册到一个选择器中。 Selector selector: 指定通道注册到的选择器实例； int operation 指定选择器要监控的IO事件类型。可供选择器监控的通道IO事件类型，包括以下四种： 可读就绪：SelectionKey.OP_READ 可写就绪：SelectionKey.OP_WRITE 连接就绪：SelectionKey.OP_CONNECT 接收就绪：SelectionKey.OP_ACCEPT 除了FileChannel文件通道外，其他选择器都是可选择的。这是因为其他三个通道都继承了一个SelectableChannel这个抽象类。 SelectionKey选择键: 指的是被选择器选中的IO事件；一个IO事件发生（就绪状态达成）后，如果之前在选择器中注册过，就会被选择器选中，并放入SelectionKey选择键集合；如果之前没有注册过，即使发生了IO事件，也不会被选择器选中。 选择器使用流程： 获取选择器实例Selector选择器的类方法open()的内部，是向选择器SPI（SelectorProvider）发出请求，通过默认的SelectorProvider（选择器提供者）对象，获取一个新的选择器实例。 12// 调用静态工厂方法open()来获取Selector selector = Selector.open(); 将通道注册到选择器中其次，还需要注意：一个通道，并不一定要支持所有的四种IO事件。例如服务器监听通道ServerSocketChannel，仅仅支持Accept（接收到新连接）IO事件；而SocketChannel传输通道，则不支持Accept（接收到新连接）IO事件。如何判断通道支持哪些事件呢？可以在注册之前，可以通过通道的validOps()方法，来获取该通道所有支持的IO事件集合。 12345678// 获取通道ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 设置为非阻塞serverSocketChannel.configureBlocking(false);// 绑定连接serverSocketChannel.bind(new InetSocketAddress(SystemConfig.SOCKET_SERVER_PORT));// 将通道注册到选择器上，并制定监听事件为接收就绪事件serverSocketChannel.register(Selector, SelectionKey.OP_ACCEPT); 轮询感兴趣的IO就绪事件通过Selector选择器的select()方法，选出已经注册的、已经就绪的IO事件，保存到SelectionKey选择键集合中, 该方法返回int类型的IO事件通道数量；SelectionKey集合保存在选择器实例内部，是一个元素为SelectionKey类型的集合（Set）。调用选择器的selectedKeys()方法，可以取得选择键集合。 1234567891011121314151617181920// 轮询，选择IO就绪事件 有多个重载的实现方法while (selector.select() &gt; 0) &#123; Set selectKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); // 根据具体IO事件类型，执行对应的业务逻辑 if(key.isAcceptable()) &#123; // IO事件：ServerSocketChannel服务器监听通道有新连接 &#125; else if (key.isConnectable()) &#123; // IO事件：传输通道连接成功 &#125; else if (key.isReadable()) &#123; // IO事件：传输通道可读 &#125; else if (key.isWritable()) &#123; // IO事件：传输通道可写 &#125; //处理完成后，移除选择键 keyIterator.remove(); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka学习指南]]></title>
    <url>%2F2020%2F02%2F07%2FKafka%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Kafka相关的技术知识，本文内容均基于 Ubuntu 18.04 虚拟机进行。说明，本文档共涉及6台服务器192.168.56.101 - kafka0192.168.56.102 - kafka1192.168.56.103 - kafka2192.168.56.104 - zookeeper0192.168.56.105 - zookeeper1192.168.56.106 - zookeeper2 Kafka将所有消息组织成多个topic的形式存储，而每个topic又可以拆分为多个partition，每个partition又由一个一个的消息组成，每个消息都被标识了一个递增的序列号代表其进来的先后顺序，并按照顺序存储到partition； producer选择一个topic，生产消息，消息会通过分配策略append到某个partition末尾 consumer选择一个topic, 通过id指定从那个位置开始消费消息。消费完成之后保留id，下次可以从这个位置开始继续消费，也可以从其他任意位置开始消费，这里的id即为offset。一个典型的 Kafka 体系架构包括若干 Producer、若干 Broker、若干 Consumer，以及一个ZooKeeper集群，其中ZooKeeper是Kafka用来负责集群元数据的管理、控制器的选举等操作的。Producer将消息发送到Broker，Broker负责将收到的消息存储到磁盘中，而Consumer负责从Broker订阅并消费消息。 Kafka的安装配置Ubuntu服务器环境下Kafka安装与配置Zookeeper安装与配置 - standalone模式 首先安装JAVA环境，下载jdk tar.gz安装包，上传到/usr/local路径下，并执行tar -zxvf jdk-8u241-linux-x64.tar.gz解压。然后修改系统配置文件vim /etc/profile。123export JAVA_HOME=/usr/local/jdk1.8.0_241export CLASSPATH=.:$&#123;JAVA_HOME&#125;/jre/lib/rt.jar:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin 使得配置文件生效source /etc/profile 下载zookeeper, 这里, 解压到/usr/local/, tar -zxvf /usr/local/apache-zookeeper-3.5.6-bin.tar.gz, 修改config目录下的zoo_sample.cfg重命名为zoo.cfg1234567891011121314151617# zk中的时间单元，zk中所有时间都以此时间单元为基准，进行整数倍配置tickTime=2000# follower在启动过程中，会从leader同步所有最新数据，确定自己能够对外服务的起始状态。# 当follower在initLimit个tickTime还没完成数据同步时，则leader认为follower连接失败。initLimit=10# leader与Follower之间通信请求和应答的时间长度。# 当leader在syncLimit个tickTime还没有收到follower的应答，则认为leader已下线。syncLimit=5# 快照文件存储目录，如果不配置dataLogDir，则事务日志也会保存在这个目录（不推荐）dataDir=/opt/data/zookeeper/data# 事务日志存储目录dataLogDir=/opt/data/zookeeper/logs# zk对外提供服务端口clientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60 修改zookeeper环境变量，节省操作步骤vim /etc/profile 123export ZOOKEEPER_HOME=/usr/local/apache-zookeeper-3.5.6# 在文件的Path配置项里添加下列配置，注意有:号:$&#123;ZOOKEEPER_HOME&#125;/bin 更新环境变量source /etc/profile。启动ZookeeperzkServer.sh start。如果遇到Permission denied的问题就授权给zk的安装目录chmod -R 755 /usr/local/apache-zookeeper-3.5.6/。可以通过zkServer.sh status查看运行状态。通过jps可以看到zk对应的java进程。 还可以通过以下命令通过zk客户端进行连接。 12# 登录zk服务器zkCli.sh -server 127.0.0.1:2181 执行ls / 123[zk: 127.0.0.1:2181(CONNECTED) 0] ls /# 只有一个zookeeper节点[zookeeper] Zookeeper安装与配置 - 集群模式与单机模式类似，集群模式需要对机器进行映射。我本地有三台zk的虚拟机，单机的配置，这三台集群都要有。 123192.168.56.104 - zookeeper0192.168.56.105 - zookeeper1192.168.56.106 - zookeeper2 然后进入其中一台机器的ZooKeeper安装路径conf目录。这里我们选择先在IP为192.168.56.104的机器上进行配置，编辑conf/zoo.cfg文件，在该文件中添加以下配置： server.N=N-server-IP:A:B 其中N是一个数字, 表示这是第几号server，它的值和myid文件中的值对应。N-server-IP是第N个server所在的IP地址。A是配置该server和集群中的leader交换消息所使用的端口。B配置选举leader时服务器相互通信所使用的端口。 1234# 在每个zk的配置文件里都同时配置三台机器server.1=192.168.56.104:2888:3888server.2=192.168.56.105:2888:3888server.3=192.168.56.106:2888:3888 接着在${dataDir}路径下创建一个myid文件。myid里存放的值就是服务器的编号，即对应上述公式中的N，在这里第一台机器myid存放的值为1。ZooKeeper在启动时会读取myid文件中的值与zoo.cfg文件中的配置信息进行比较，以确定是哪台服务器。 123cd /opt/data/zookeeper/datatouch myidecho 1 &gt; myid 同理在其它两个机器上分别修改zoo.cfg以及myid文件。然后在三台机器上分别执行zkServer.sh start以及zkServer.sh status, 打印出如下日志： 12345678910111213141516171819202122232425262728293031# 192.168.56.104 - zookeeper0root@zookeeper0:/opt/data/zookeeper/data# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@zookeeper0:/opt/data/zookeeper/data# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost.Mode: follower# 192.168.56.105 - zookeeper1root@zookeeper1:/opt/data/zookeeper/data# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@zookeeper1:/opt/data/zookeeper/data# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost.Mode: leader# 192.168.56.106 - zookeeper2 ZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDroot@zookeeper2:/opt/data/zookeeper/data# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/apache-zookeeper-3.5.6/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost.Mode: follower 可以看到，这3台机器中，一台机器作为Leader，其他两台服务器作为Follower。 Kafka安装与配置 - 单机模式 下载kafka, 这里, 解压到/usr/local/, tar -zxvf /usr/local/kafka_2.13-2.4.0.tgz 配置环境变量, vim /etc/profile，按照下图配置后保存文件退出，执行source /etc/profile命令让刚才新增的Kafka环境变量设置生效。再在任一路径下输入kafka然后按Tab键，会提示补全Kafka运行相关脚本．sh文件，表示Kafka环境变量配置成功。 修改kafka配置，修改$KAFKA_HOME/config目录下的server.properties文件，为了便于后续集群环境搭建的配置，需要保证同一个集群下broker.id要唯一，因此这里手动配置broker.id，直接保持与zk的myid值一致，同时配置日志存储路径。server.properties修改的配置如下：123456# 指定的代理ID，由于是单机模式，这里指定zk节点id为1，及zookeeper0那台机器。broker.id=1# 指定Log存储路径log.dirs=/opt/data/kafka-logs# 指定kafka的安装路径，由于我zk没和kafka安装在同一台机器上所以这里要修改。zookeeper.connect=192.168.56.104:2181 修改完后，保存文件然后启动Kafka，进入Kafka安装路径$KAFKA_HOME/bin目录下，执行启动KafkaServer命令。 12# -daemon参数表示使程序以守护进程的方式后台运行kafka-server-start.sh -daemon /usr/local/kafka_2.13-2.4.0/config/server.properties 执行jps命令查看Java进程，可以看到kafka的进程名，同时进入$KAFKA_HOME/logs目录下，查看server.log会看到KafkaServer启动日志，在启动日志中会记录KafkaServer启动时加载的配置信息。此时登录192.168.56.104这台zk可以再次查看目录结构： 1zkCli.sh -server 192.168.56.104:2181 通过zk客户端登录 123456# 在Kafka启动之前ZooKeeper中只有一个zookeeper目录节点，Kafka启动后目录节点如下：[zk: 127.0.0.1:2181(CONNECTED) 0] ls /[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]# 查看当前已启动的Kafka代理节点：输出信息显示当前只有一个Kafka代理节点，当前代理的brokerId为1[zk: 127.0.0.1:2181(CONNECTED) 1] ls /brokers/ids[1] Kafka安装与配置 - 集群模式集群与单机类似，这里只需修改server.properties文件中Kafka连接ZooKeeper的配置，将Kafka连接到ZooKeeper集群，配置格式为ZooKeeper服务器IP:ZooKeeper的客户端端口，多个ZooKeeper机器之间以逗号分隔开。 1zookeeper.connect=192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181 执行下列命令复制kafka整个目录： 12345cd /usr/local# 复制文件到kafka1scp -r kafka_2.13-2.4.0 root@192.168.56.102:/usr/local/# 复制文件到kafka2scp -r kafka_2.13-2.4.0 root@192.168.56.103:/usr/local/ 分别登录另外两台机器，修改server.properties文件中的broker.id依次为2和3, 并安装java环境，配置环境变量，同时也添加上述zk配置。同样的，修改三台机器的advertised.listeners=PLAINTEXT://your.host.name:9092属性为具体的ip和端口。 listeners：kafka的连接协议名、主机名和端口，如果没有配置，将使用java.net.InetAddress.getCanonicalHostName()的返回值作为主机名advertised.listeners：生产者和消费者使用的主机名和端口，如果没有配置，将使用listeners的配置，如果listeners也没有配置，将使用java.net.InetAddress.getCanonicalHostName()的返回值然后在3台机器上启动kafka 1kafka-server-start.sh -daemon /usr/local/kafka_2.13-2.4.0/config/server.properties 这个时候在任意一台zk服务器上执行ls /brokers/ids都会得到一下结果 12[zk: 127.0.0.1:2181(CONNECTED) 4] ls /brokers/ids[1, 2, 3] Docker环境安装与配置镜像下载123docker pull wurstmeister/zookeeperdocker pull wurstmeister/kafka zookeeper容器启动1234# -d参数 表示后台运行容器，并返回容器IDdocker run -d --name zookeeper -p 2181:2181 -t wurstmeister/zookeeper# 查看zk服务器目录结构ls / kafka容器启动单节点部署12345678# 启动Kafka(注意 修改IP为镜像安装IP)docker run -d --name kafka \-p 9092:9092 \-e KAFKA_BROKER_ID=0 \-e KAFKA_ZOOKEEPER_CONNECT=192.168.56.101:2181 \-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.56.101:9092 \-e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \-t wurstmeister/kafka 注意有以下四个参数： KAFKA_BROKER_ID=0 KAFKA_ZOOKEEPER_CONNECT=&lt;zookeeper IP&gt;:&lt;zookeeper port&gt; KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://:9092 KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 进入kafka容器内部 1234567docker exec -it kafka /bin/bash# 查看Kafka版本，进入Kafka所在目录cd /opt/kafka_2.12-2.4.0# 启动消息发送方./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka# 启动消息接收方./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning Kafka伪分布式环境部署在同一台机器上启动多个Kafka Server 在单节点搭建的基础上再搭建一个节点，只需修改KAFKA_BROKER_ID以及端口 123456789docker run -d --name kafka1 \-p 9093:9093 \# 修改broker_id-e KAFKA_BROKER_ID=1 \-e KAFKA_ZOOKEEPER_CONNECT=192.168.56.101:2181 \# 修改端口-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.56.101:9093 \-e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093 \-t wurstmeister/kafka 然后再在/opt/kafka_2.12-2.4.0目录下执行 1./bin/kafka-topics.sh --create --zookeeper 192.168.56.101:2181 --replication-factor 2 --partitions 2 --topic mytopic 查看topic状态 1./bin/kafka-topics.sh --describe --zookeeper 192.168.56.101:2181 --topic mytopic Isr表示存活的备份 Kafka Manager安装下载kafka Manager这里，上传到/usr/local文件下，并解压tar -zxvf CMAK-2.0.0.2.tar.gz。Kafka Manager是用Scala语言开发的，通过sbt(Simple Build Tool)构建，sbt是对Scala或Java语言进行编译的一个工具，它类似于Maven, Gradle。需要通过以下方式进行源码编译 123cd ./CMAK-2.0.0.2# 此过程巨慢无比，推荐直接搜索打包好的kafka-manager./sbt clean dist 在下载了一晚上无果后，打开/root/.sbt看了看发现就下载了一个jar包，网上找了一份别人编译好的kafka-manager-2.0.0.2.zip。解压好上传，进行如下配置： 1234cd /usr/local/kafka-manager-2.0.0.2/confvim ./application.conf# 修改以下配置为真正的zk集群地址，注意是修改倒数第二行的这个配置才可以生效kafka-manager.zkhosts="192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181" 修改logback.xml文件中的${application.home}为..，即logs日志存储位置。启动kafka-manager 1234# 进入bin目录输入如下启动命令nohup ./kafka-manager -Dconfig.file=../conf/application.conf# 权限不够chmod -R 755 ../../kafka-manager-2.0.0.2/ 关闭Kafka Manager。Kafka Manager没有提供关闭操作的执行脚本及命令，当希望关闭Kafka Manager时，可直接通过kill命令强制杀掉Kafka Manager进程。查看Kafka Manager进程，输入jps命令，其中ProdServerStart即为Kafka Manager进程。通过kill命令关闭Kafka Manager。同时，由于Kafka Manager运行时有一个类似锁的文件RUNNING_PID，位于Kafka Manager安装路径bin同目录下，为了不影响下次启动，在执行kill命令后同时删除RUNNING_PID文件，rm -f RUNNING_PID.完成以上配置后打开http://192.168.56.101:9000/即可。 Kafka概念说明基础概念Kafka系统中有四种核心应用接口——生产者、消费者、数据流、连接器。 生产者Kafka生产者可以理解成Kafka系统与外界进行数据交互的应用接口。生产者应用接口的作用是写入消息数据到Kafka中。Kafka系统提供了一系列的操作脚本，这些脚本放置在$KAFKA_HOME/bin目录中。其中，kafka-console-producer.sh脚本可用来作为生产者客户端。 生产者属性如下： 这里重点说明以下acks属性： 当acks=0时，生产者不用等待代理返回确认信息，而连续发送消息。显然这种方式加快了消息投递的速度，然而无法保证消息是否已被代理接受，有可能存在丢失数据的风险。 当acsk=1时，生产者需要等待Leader副本已成功将消息写入日志文件中。这种方式在一定程度上降低了数据丢失的可能性，但仍无法保证数据一定不会丢失。如果在Leader副本成功存储数据后，Follower副本还没有来得及进行同步，而此时Leader宕机了，那么此时虽然数据已进行了存储，由于原来的Leader已不可用而会从集群中下线，同时存活的代理又再也不会有从原来的Leader副本存储的数据，此时数据就会丢失。 当acks=-1时，Leader副本和所有ISR列表中的副本都完成数据存储时才会向生产者发送确认信息，这种策略保证只要Leader副本和Follower副本中至少有一个节点存活，数据就不会丢失。为了保证数据不丢失，需要保证同步的副本至少大于1，通过参数min.insync.replicas设置，当同步副本数不足此配置值时，生产者会抛出异常，但这种方式同时也影响了生产者发送消息的速度以及吞吐量。 消费者 与 消费组Kafka消费著可以理解成，外界从Kafka系统中获取消息数据的一种应用接口。消费者应用接口的主要作用是读取消息数据。Kafka系统提供了一系列的可操作脚本，这些脚本放置在$KAFKA_HOME/bin目录下。其中，有一个脚本可用来作为消费者客户端，即kafka-console-consumer.sh。消费者属性如下： 消费者（Comsumer）以拉取（pull）方式拉取数据，它是消费的客户端。在Kafka中每一个消费者都属于一个特定消费组（ConsumerGroup），我们可以为每个消费者指定一个消费组，以groupId代表消费组名称，通过group.id配置设置。如果不指定消费组，则该消费者属于默认消费组test-consumer-group。同时，每个消费者也有一个全局唯一的id，通过配置项client.id指定，如果客户端没有指定消费者的id, Kafka会自动为该消费者生成一个全局唯一的id，格式为${groupId}-${hostName}-${timestamp}-${UUID前8位字符}。同一个主题的一条消息只能被同一个消费组下某一个消费者消费，但不同消费组的消费者可同时消费该消息。 broker - 代理对于kafka而言，broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。 topic - 主题 partition - 分区 以及 Replica - 副本Kafka中的消息以主题(topic)为单位进行归类，生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。每一个代理都有唯一的标识id，这个id是一个非负整数。在一个Kafka集群中，每增加一个代理就需要为这个代理配置一个与该集群中其他代理不同的id, id值可以选择任意非负整数即可，只要保证它在整个Kafka集群中唯一，这个id就是代理的名字，也就是在启动代理时配置的broker.id对应的值。 主题是一个逻辑上的概念，它还可以细分为多个分区(partition)，一个分区只属于单个主题。每个分区由一系列有序、不可变的消息组成，是一个有序队列。每个分区在物理上对应为一个文件夹，分区的命名规则为主题名称后接—连接符，之后再接分区编号，分区编号从0开始，编号最大值为分区的总数减1。每个分区又有一至多个副本(Replica)，分区的副本分布在集群的不同代理上，以提高可用性。从存储角度上分析，分区的每个副本在逻辑上抽象为一个日志（Log）对象，即分区的副本与日志对象是一一对应的。每个主题对应的分区数可以在Kafka启动时所加载的配置文件中配置，也可以在创建主题时指定。当然，客户端还可以在主题创建后修改主题的分区数。由于Kafka副本的存在，就需要保证一个分区的多个副本之间数据的一致性，Kafka会选择该分区的一个副本作为Leader副本，而该分区其他副本即为Follower副本，只有Leader副本才负责处理客户端读/写请求，Follower副本从Leader副本同步数据。副本Follower与Leader的角色并不是固定不变的，如果Leader失效，通过相应的选举算法将从其他Follower副本中选出新的Leader副本。同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。 每一条消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区。如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中。如果一个主题只对应一个文件，那么这个文件所在的机器 I/O 将会成为这个主题的性能瓶颈，而分区解决了这个问题。在创建主题的时候可以通过指定的参数来设置分区的个数，当然也可以在主题创建完成之后去修改分区的数量，通过增加分区的数量可以实现水平扩展。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转移，当Kafka集群中某个broker失效时仍然能保证服务可用。 日志段一个日志又被划分为多个日志段（LogSegment），日志段是Kafka日志对象分片的最小单位。与日志对象一样，日志段也是一个逻辑概念，一个日志段对应磁盘上一个具体日志文件和两个索引文件。日志文件是以“.log”为文件名后缀的数据文件，用于保存消息实际数据。两个索引文件分别以“.index”和“.timeindex”作为文件名后缀，分别表示消息偏移量索引文件和消息时间戳索引文件。 ISRKafka在ZooKeeper中动态维护了一个ISR（In-sync Replica），即保存同步的副本列表，该列表中保存的是与Leader副本保持消息同步的所有副本对应的代理节点id。分区中的所有副本统称为AR（Assigned Replicas）。所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。如果一个Follower副本宕机或是落后太多，则该Follower副本节点将从ISR列表中移除。与leader副本同步滞后过多的副本（不包括leader副本）组成OSR（Out-of-Sync Replicas），由此可见，AR=ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即 AR=ISR，OSR集合为空。 LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的offset，offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。 Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。 Kafka的基础操作KafkaServer管理单节点启动12# 进入bin目录kafka-server-start.sh -daemon ../config/server.properties bin目录下的kafka-server-start.sh即为启动脚本。启动后会在$KAFKA_HOME/logs目录下创建相应的日志文件。 启动完毕后，登录ZooKeeper客户端查看相应节点信息。 12345# 启动zk客户端zkCli.sh -server 192.168.56.104:2181[zk: 192.168.56.104:2181(CONNECTED) 0] get /controller&#123;"version":1,"brokerid":1,"timestamp":"1581667736716"&#125;[zk: 192.168.56.104:2181(CONNECTED) 1] JMX监控开启，需要将JMX_PORT配置添加到KafkaServer启动脚本kafka-server-start.sh文件中，该项监控可以在kafka-manager中看到。 12# 在启动脚本中首行添加export JMX_PORT=9999 也可以在启动命令中配置 1JMX_PORT=9999 kafka-server-start.sh -daemon ../config/server.properties 集群启动可以编写个脚本来启动集群中所有节点 # $?是指上一次命令执行的成功或者失败的状态。如果成功就是0，失败为1 123456789101112131415# kafka-cluster-start.sh# !/bin/bashbrokers="192.168.56.101 192.168.56.102 192.168.56.103"KAFKA_HOME="/usr/local/kafka_2.13-2.4.0"echo "INFO: Begin to start kafka cluster..."for broker in $brokersdo echo "INFO:Start kafka on $&#123;broker&#125;..." ssh $broker -C "source /etc/profile; sh $&#123;KAFKA_HOME&#125;/bin/kafka-server-start.sh -daemon $&#123;KAFKA_HOME&#125;/config/server.properties" if [ $? -eq 0 ]; then echo "INFO:[$&#123;broker&#125;] Start successfully..." fidoneecho "INFO:Kafka cluster starts successfully!" 单节点关闭执行bin目录下的kafka-server-stop.sh即可停止kafka。 123456789SIGNAL=$&#123;SIGNAL:-TERM&#125;PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')if [ -z "$PIDS" ]; then echo "No kafka server to stop" exit 1else kill -s $SIGNAL $PIDSfi 停止的原理是kill kafka的PID，由于我的kafka-manager和kafka0节点装在一起，所以会连代停止我的kafka-manager。如果想准确的停止kafka，获取PID时可以使用PIDS=$(jps | grep -i &#39;Kafka&#39; | awk &#39;{print $1}&#39;) 集群关闭与集群启动类似，编写一个调用bin目录下的kafka-server-stop.sh的脚本即可停止。 123456789101112131415# kafka-cluster-stop.sh# !/bin/bashbrokers="192.168.56.101 192.168.56.102 192.168.56.103"KAFKA_HOME="/usr/local/kafka_2.13-2.4.0"echo "INFO: Begin to shut down kafka cluster..."for broker in $brokersdo echo "INFO:Shut down kafka on $&#123;broker&#125;..." ssh $broker -C "$&#123;KAFKA_HOME&#125;/bin/kafka-server-stop.sh" if [ $? -eq 0 ]; then echo "INFO:[$&#123;broker&#125;] Shut down successfully..." fidoneecho "INFO:Kafka cluster shut down successfully!" 主题管理主题创建客户端通过执行kafka-topics.sh脚本创建一个主题。若开启了自动创建主题配置项auto.create.topics.enable=true，当生产者向一个还不存在的主题发送消息时，Kafka会自动创建该主题。 123456789# 直接输入该脚本的名字可以查看有哪些命令参数kafka-topics.sh# 创建一个名为 kafka-action的主题，该主题拥有2个副本，3个分区kafka-topics.sh --create --zookeeper 192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181 --replication-factor 2 --partitions 3 --topic kafka-action# 登录ZooKeeper客户端查看所创建的主题元数据信息[zk: 192.168.56.104:2181(CONNECTED) 4] ls /brokers/topics/kafka-action/partitions[0, 1, 2][zk: 192.168.56.104:2181(CONNECTED) 5] get /brokers/topics/kafka-action&#123;"version":2,"partitions":&#123;"0":[1,2],"1":[2,3],"2":[3,1]&#125;,"adding_replicas":&#123;&#125;,"removing_replicas":&#123;&#125;&#125; zookeeper参数是必传参数，用于配置Kafka集群与ZooKeeper连接地址，这里并不要求传递${ zookeeper.connect }配置的所有连接地址。为了容错，建议多个ZooKeeper节点的集群至少传递两个ZooKeeper连接配置，多个配置之间以逗号隔开。 partitions参数用于设置主题分区数，该配置为必传参数。Kafka通过分区分配策略，将一个主题的消息分散到多个分区并分别保存到不同的代理上，以此来提高消息处理的吞吐量。Kafka的生产者和消费者可以采用多线程并行对主题消息进行处理，而每个线程处理的是一个分区的数据，因此分区实际上是Kafka并行处理的基本单位。分区数越多一定程度上会提升消息处理的吞吐量，然而Kafka消息是以追加的形式存储在文件中的，这就意味着分区越多需要打开更多的文件句柄，这样也会带来一定的开销。 replication-factor参数用来设置主题副本数，该配置也是必传参数。副本会被分布在不同的节点上，副本数不能超过节点数，否则创建主题会失败 进入在server.properties中配置的log.dirs=/opt/data/kafka-logs对应的目录下，创建主题后会在${log.dir}目录下创建相应的分区文件目录，副本分别分布在不同的节点上。 12345678910111213# 以kafka0节点为例子，查看分区文件cd /opt/data/kafka-logsls -ldrwxr-xr-x 2 root root 4096 Feb 16 04:08 kafka-action-0drwxr-xr-x 2 root root 4096 Feb 16 04:08 kafka-action-2root@kafka0:/opt/data/kafka-logs# cd ./kafka-action-0root@kafka0:/opt/data/kafka-logs/kafka-action-0# ls -ltotal 4-rw-r--r-- 1 root root 10485760 Feb 16 04:08 00000000000000000000.index-rw-r--r-- 1 root root 0 Feb 16 04:08 00000000000000000000.log-rw-r--r-- 1 root root 10485756 Feb 16 04:08 00000000000000000000.timeindex-rw-r--r-- 1 root root 8 Feb 16 04:08 leader-epoch-checkpoint 主题删除执行kafka-topics.sh脚本进行删除，若希望通过该脚本彻底删除主题，则需要保证在启动Kafka时所加载的server.properties文件中配置delete.topic.enable=true，该配置默认为false。否则执行该脚本并未真正删除主题，而是在ZooKeeper的/admin/delete_topics目录下创建一个与待删除主题同名的节点，将该主题标记为删除状态。主题在${log.dir}目录下对应的分区文件及在ZooKeeper中的相应节点并未被删除，这个时候需要你手动删除。 1kafka-topics.sh --delete --zookeeper 192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181 --topic kafka-action 直接执行的话，用zk客户端去看 12[zk: 192.168.56.104:2181(CONNECTED) 8] ls /admin/delete_topics[kafka-action] 查看主题 查看该集群下所有主题 1kafka-topics.sh --list --zookeeper 192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181 查看特定主题的信息 1kafka-topics.sh --topic kafka-action --describe --zookeeper 192.168.56.104:2181,192.168.56.105:2181,192.168.56.106:2181 查看消息Kafka生产的消息以二进制的形式存在文件中，Kafka提供了一个查看日志文件的工具类kafka.tools.DumpLogSegments。通过kafka-run-class.sh脚本，可以直接在终端运行该工具类 12# 查看kafka-action-test主题下的消息内容kafka-run-class.sh kafka.tools.DumpLogSegments --files /opt/data/kafka-logs/kafka-action-test-1/00000000000000000000.log 生产者启动生产者12# broker-list 指定Kafka的代理地址列表 topic 指定消息被发送的目标主题 key.separator 指定key 和 消息之间的分隔符kafka-console-producer.sh --broker-list 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --topic kafka-action-test --property parse.key=true --property key.separator=' ' 生产者性能测试1kafka-producer-perf-test.sh --num-records 10000 --record-size 1000 --topic kafka-action-test --throughput 10000 --producer-props bootstrap.servers=192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 kafka-producer-perf-test.sh脚本调用的是org.apache.kafka.tools.ProducerPerformance类 topic 指定了生产者发送消息的目标主题 num-records 测试时发送消息的总条数 record-size 每条消息的字节数 throughput 限流控制 throughput值小于0时则不进行限流；若该参数值大于0时，当已发送的消息总字节数与当前已执行的时间取整大于该字段时生产者线程会被阻塞一段时间。生产者线程被阻塞时，在控制台可以看到输出一行吞吐量统计信息；若该参数值等于0时，则生产者在发送一次消息之后检测满足阻塞条件时将会一直被阻塞。 上述命令执行结果如下： 110000 records sent, 4618.937644 records/sec (4.40 MB/sec), 895.99 ms avg latency, 1232.00 ms max latency, 940 ms 50th, 1199 ms 95th, 1220 ms 99th, 1232 ms 99.9th. recores send 测试时发送的消息总数 records/sec 每秒发送的消息数 - 吞吐量 avg latency 消息处理的平均耗时 ms max latency 消息处理的最大耗时 ms X th %Xd的消息处理耗时 消费者Kafka采用了消费组的模式，每个消费者都属于某一个消费组，在创建消费者时，若不指定消费者的groupId，则该消费者属于默认消费组。消费组是一个全局的概念，因此在设置group.id时，要确保该值在Kafka集群中唯一。同一个消费组下的各消费者在消费消息时是互斥的，也就是说，对于一条消息而言，就同一个消费组下的消费者来讲，只能被同组下的某一个消费者消费，但不同消费组的消费者能消费同一条消息。 启动消费者kafka-console-consumer.sh脚本调用的是Kafka core工程下kafka.tools包下的ConsoleConsumer对象，该对象调用（org.apache.kafka.clients.consumer.KafkaConsumer）消费消息。 1kafka-console-consumer.sh --bootstrap-server 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --consumer-property group.id=consumer-test --topic kafka-action-test --from-beginning 查看消费者组的信息123456# 查看所有消费者组kafka-consumer-groups.sh --bootstrap-server 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --list# 查看指定消费者组信息kafka-consumer-groups.sh --bootstrap-server 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --describe --group hello# 删除指定消费者组kafka-consumer-groups.sh --bootstrap-server 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --delete --group hello 消费者性能测试工具1kafka-consumer-perf-test.sh --broker-list 192.168.56.101:9092,192.168.56.102:9092,192.168.56.103:9092 --threads 5 --messages 10000 --socket-buffer-size 10000 --num-fetch-threads 2 --group consumer-perf-test --topic kafka-action-test 输出如下： 12start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec2020-02-20 16:07:33:457, 2020-02-20 16:07:34:668, 9.5369, 7.8752, 10029, 8281.5855, 1582214853971, -1582214852760, -0.0000, -0.0000 Kafka的源码编译环境搭建安装ScalaWindows环境下，下载并安装Scala。先进入Scala官方网站这里下载相应的安装包并安装。 12# 查询Scala版本scala -version 安装Gradle进入Gradle官方网站这里下载Gradle安装包。将下载好的gradle-6.1.1-bin解压后，配置GRADLE_HOME以及%GRADLE_HOME%\bin到环境变量。 12# 查询gradle版本gradle -version Kafka源码编译先进入这里下载Kafka src 源码文件。进入源码根目录，执行gradle idea。]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring循环依赖三级缓存解析]]></title>
    <url>%2F2020%2F02%2F05%2FSpring%E7%9A%84%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[Spring循环依赖三级缓存解析。 Spring循环依赖使用场景 对象A的构造方法中依赖了B的实例对象， 同时B的field或者setter需要A的实例对象 对象A的某个field或者setter依赖了B的实例对象，同时B的某个field或者setter依赖了A的实例对象(不能解决的使用场景) 3. 对象A的构造函数依赖了B的实例对象，B的构造函数依赖了A的实例对象 Spring单体对象初始化的流程 createBeanInstance 实例化，仅仅调用对应的构造方法，没有传入指定的sping.xml populate populateBean 填充属性，对spring.xml指定的property进行populate initializeBean 调用spring.xml中指定的init方法，或者AfterPropertiesSet方法 Spring循环依赖的三级缓存DefaultSingletonBeanRegistry中的三级缓存： // 存放初始化好的可以直接使用的bean，单例对象的cache - 一级缓存 private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256); // 提前曝光的单例对象cache- 二级缓存 private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16); // 存放单例对象工厂的cache - 三级缓存 private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); protected Object getSingleton(String beanName, boolean allowEarlyReference) { // 从一级缓存中取实例 Object singletonObject = this.singletonObjects.get(beanName); // 一级缓存中没有，并且对象在创建中(isSingletonCurrentlyInCreation判断对应的单例对象是否在创建中) if (singletonObject == null &amp;&amp; this.isSingletonCurrentlyInCreation(beanName)) { synchronized(this.singletonObjects) { // 从二级缓存中拿数据 singletonObject = this.earlySingletonObjects.get(beanName); // 二级缓存中没有并且允许从三级缓存中拿数据(allowEarlyReference是否允许从singletonFactories中拿数据) if (singletonObject == null &amp;&amp; allowEarlyReference) { ObjectFactory&lt;?&gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); // 将三级缓存提到二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject; } 三级缓存的作用二级缓存的存在有两个作用1.三级缓存中获取bean,需要循环所有的后置处理器,调用它们实现的方法,效率低下。 2. 为了保护单例对象。三级缓存在使用时，beanA和beanB循环依赖，beanA和beanC循环依赖，从三级缓存中获取beanA需要循环所有的后置处理器,但是程序员可以扩展后置处理器,在实现的方法里面可能重新new了一个bean,这时候返回的就不是spring给我们创建的bean了,这样就导致beanB和beanC注入beanA的时候都在后置处理器中new了一个beanA,这样beanB和beanC中注入的beanA并不是同一个对象,这样就破坏了单例,所以二级缓存存在为了防止破坏单例是必须的。 参考文档： https://blog.csdn.net/github_38687585/article/details/82317674]]></content>
      <tags>
        <tag>JavaWEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring自定义标签的使用]]></title>
    <url>%2F2020%2F02%2F05%2FSpring%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A0%87%E7%AD%BE%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在Spring中完成一个自定义标签所需要的步骤。 设计配置属性和JavaBean 编写XSD文件 编写BeanDefinitionParser标签解析类 编写调用标签解析类的NamespaceHandler类 编写spring.handlers和spring.schemas以供Spring读取 在Spring中使用]]></content>
      <tags>
        <tag>JavaWEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu设置SSH免密码登录]]></title>
    <url>%2F2020%2F02%2F04%2FUbuntu%E8%AE%BE%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[通过SSH公钥（锁🔒）和密钥（钥匙🔑）来实现Putty免输入密码登录操作。 生成公钥和私钥12# -t 指定密钥烈性ssh-keygen -t rsa 用WinSCP打开文件保存路径/root/.ssh/，上述命令会在该目录下生成 id_rsa和 id_rsa.pub两个文件。 注意 设置/etc/ssh/sshd_config 文件下的 PermitRootLogin yesPasswordAuthentication yes 才可以root登录，并重启ssh服务 systemctl restart ssh.service 将/root/.ssh/路径下的id_rsa导入到PuTTYgen (Conversions -&gt; Import key)，并生成及保存私钥。在PuTTY SSH -&gt; Auth 中添加私钥文件。 再次修改SSH配置文件/etc/ssh/sshd_config，取消下面几项配置的注释 12#PubkeyAuthentication yes#AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 执行下面这条命令，将公钥添加到.ssh/authorized_keys 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 至此完成Ubuntu SSH 免密码登录设置。 集群的配置常见的分布式集群，通常由多台机器构成，为了便于操作管理，通过ssh方式启动集群代理，需要在多个服务器上进行ssh免登录配置。将第一台机器上的authorized_keys复制到第二台机器上。 1scp authorized_keys root@192.168.56.101:~/.ssh 然后将第二台机器的公钥也追加到授权文件中 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 同样的操作在第三台第四台机器上分别执行。配置完成后再任意一台机器上都可以免密登录到其他机器上。ssh 192.168.56.102即可。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2F2020%2F01%2F11%2FDocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Docker 常用命令总结，坚持就是胜利！ docker 常见命令总结123456789101112# 查看已经安装的镜像docker images# 查看正在运行的容器docker ps# 查看已经退出的容器docker ps -a# 启动容器docker start &lt;container ID&gt;# 停止容器docker stop &lt;container ID&gt;# 开启一个容器内部的交互式终端 /bin/bashdocker exec -it &lt;container ID&gt; /bin/bash Docker安装MySQL123456789101112docker pull mysql# 将容器的 3306 端口映射到主机的 3306 端口, 容器conf目录和logs目录、data目录都挂载在本地，并设置root密码 rootdocker run -p 3306:3306 --name mymysql -v $PWD/conf:/etc/mysql/conf.d -v $PWD/logs:/logs -v $PWD/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -d mysql# 这里会输出容器ID# 进入容器docker exec -it &lt;container ID&gt; /bin/bash# 登录Mysqlmysql -uroot -p# 授权所有权限给root用户，并允许远程连接GRANT ALL PRIVILEGES ON *.* TO 'root'@'%';# 刷新权限flush privileges; Docker安装Tomcat123456789101112# 安装Tomcatdocker pull tomcat# 运行容器# 命令说明# -p 8080:8080：将容器的 8080 端口映射到主机的 8080 端口。# -v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的 test 挂载到容器的 /test。docker run --name tomcat -p 8080:8080 -v $PWD/test:/usr/local/tomcat/webapps/test -d tomcatdocker exec -it &lt;container ID&gt; /bin/bashls -lmv webapps.dist webapps Docker安装Nginx1234567891011121314# 安装nginxdocker pull nginx# 查看镜像docker ps# --name 容器名称 -p 8080:80 端口进行映射，将本地8080端口映射到容器内部的80端口 - 简易启动方式docker run --name nginx-test -p 8080:80 -d nginx# -v 使用本地配置覆盖docker镜像配置 --net host 参数加上后不会使用容器自己虚拟的网卡，而会使用宿主机自己的IP和端口docker run \--name my_nginx \-d -p 30002:80 --net host\-v /home/nginx/log:/var/log/nginx \-v /home/nginx/html:/usr/share/nginx/html \-v /home/nginx/conf.d:/etc/nginx/conf.d \nginx 常见问题在安装docker的虚拟机重启后，再次进入会发现docker 容器的状态为退出 docker ps -a，这个时候有两个选择 删除容器 docker rm &lt;container ID&gt;， 如果有容器中的数据很重要可以保存为images docker commit &lt;container ID&gt; 重启docker容器 docker start &lt;container ID&gt;]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>Tomcat</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识-并发编程]]></title>
    <url>%2F2019%2F11%2F30%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[并发编程的基础、并发模拟工具及代码。 并发编程的基础 CPU多级缓存 时间局部性 空间局部性缓存一致性： M E S I 用于保证多个CPU Cache之间共享数据的一致M 修改状态E 独享状态S 共享状态I 无效状态 乱序执行优化 为了提高运算速度而做出违背代码原有顺序的优化 JAVA内存模型 (Java Memory Model - JMM) Heap 堆可动态分配内存大小，生存期不需事先告诉编译器，存取速度慢，常用于存放对象Thread Stack 栈存取速度快，仅次于寄存器，数据大小和生存期必须确定，常用于存取基本类型变量，本地变量 栈上的引用变量数据可以访问引用的堆对象，如果两个线程同时调用了同一个对象，都会访问成员变量，但是这两个线程会有自己变量的私有拷贝。 每个线程都有自己的“本地内存”，Java中线程的“本地内存”指的是CPU寄存器和高速缓存的抽象描述。两个线程之间的变量共享是要通过主内存的。 Java内存模型-同步八种操作 lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态 unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中以便随后的load动作使用。 load（载入）: 作用于工作内存的变量,它把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用）：作用于工作内存的变量,把工作内存中的一个变量值传递给执行引擎 assign（赋值）：作用于工作内存的变量,它把一个从执行引擎接收到的值赋值给工作内存的变量 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中,以便随后的write的操作 write（写入）：作用于主内存的变量,它把store操作从工作内存中一个变量的值传送到主内存的变量中 工作规则： 如果要把一个变量从主内存中复制到工作内存,就需要按顺序地执行read和load操作,如果把变量从工作内存中同步回主内存中,就要按顺序地执行store和write操作。但Java内存模型只要求上述操作必须按顺序执行,而没有保证必须是连续执行。 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作,即变量在工作内存中改变了之后必须同步到主内存中 不允许—个线程无原因地(没有发生过任何assign操作)把数据从工作内存同步回主内存中 一个新的变量只能在主内存中诞生,不允许在工作内存中直接使用一个未被初始化(load或assign )的变量。即就是对一个变量实施use和store操作之前,必须先执行过了assign和load操作 一个变量在同_时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次,多次执行lock后,只有执行相同次数的unlock操作,变量才会被解锁。lock和unlock必须成对出现 如果对一个变量执行lock操作,将会清空工作内存中此变量的值,在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中 一个新的变量只能在主内存中诞生/不允许在工作内存中直接使用一个未被初始化（load或assign ）的变量。即就是对一个变量实施use和store操作之前, 必须先执行过了assign和load操作 并发模拟工具及代码:工具： PostMan Apache Bench Apache JMeter 代码： CountDownLatch - 闭锁：1. await() 2. countDown() 阻塞线程，满足某种特定的条件继续执行，常与与线程池一同使用。 Semaphore - 信号量：可以阻塞线程，控制同一时间请求的并发量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.leezy.concurrency;import com.leezy.concurrency.annoations.NotThreadSafe;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;/** * @program: spring-cloud-leezy * @description: 测试并发 * @author: LEEZY * @create: 2019-11-30 16:03 **/@Slf4j@NotThreadSafe// count:4962 结果不准确，线程不安全public class ConcurrencyTest &#123; // 请求总数 public static int clientTotal = 5000; // 同时并发执行的线程数 public static int threadTotal = 200; public static int count = 0; private static void add() &#123; count++; &#125; public static void main(String[] args) throws InterruptedException &#123; // 线程池 ExecutorService executorService = Executors.newCachedThreadPool(); // 信号量以及并发线程数 final Semaphore semaphore = new Semaphore(threadTotal); // 计数器-闭锁 放入请求总数 final CountDownLatch countDownLatch = new CountDownLatch(clientTotal); for (int i = 0; i &lt; clientTotal; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; // 根据并发的限制数量, 判断当前线程是否允许被执行 semaphore.acquire(); add(); // 释放线程 semaphore.release(); &#125; catch (InterruptedException e) &#123; log.error("exception", e); &#125; // 执行一次就进行releaseShared countDownLatch.countDown(); &#125;); &#125; // 这个方法可以保证 countDown 减为0 countDownLatch.await(); // 关闭线程池 executorService.shutdown(); log.info("count:" + count); &#125;&#125; 线程安全性 原子性：提供了互斥访问，同一时刻只能有一个线程来对它进行操作。 - Atomic包 ThreadLocal的使用1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.ArrayList;import java.util.List;import java.util.logging.Logger;public class ThreadLocalTest &#123; private Logger logger = Logger.getLogger("logger"); private ThreadLocal&lt;String&gt; mThreadLocal = new ThreadLocal&lt;&gt;(); private void testThreadLocal() throws InterruptedException &#123; mThreadLocal.set("main-thread"); logger.info("result1: " + mThreadLocal.get()); Thread thread1 = new Thread() &#123; @Override public void run() &#123; super.run(); // thread1内部设置的值只有thead1内部可以得到，所以result3为空 mThreadLocal.set("thread_one"); logger.info("result2: " + mThreadLocal.get()); &#125; &#125;; thread1.start(); thread1.join(); Thread thread2 = new Thread() &#123; public void run() &#123; super.run(); logger.info("result3: " + mThreadLocal.get()); &#125; &#125;; thread2.start(); thread2.join(); logger.info("result4: " + mThreadLocal.get()); &#125; public static void main(String[] args) &#123; ThreadLocalTest threadLocalTest = new ThreadLocalTest(); try &#123; threadLocalTest.testThreadLocal(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 输出： 12345678# 十二月 15, 2019 4:36:21 下午 ThreadLocalTest testThreadLocal信息: result1: main-thread# 十二月 15, 2019 4:36:21 下午 ThreadLocalTest$1 run信息: result2: thread_one# 十二月 15, 2019 4:36:21 下午 ThreadLocalTest$2 run信息: result3: null# 十二月 15, 2019 4:36:21 下午 ThreadLocalTest testThreadLocal信息: result4: main-thread ThreadLocal详解四个接口： public void set(T value) { }设置当前线程的ThreadLocal值为指定的Value public T get() { }获取当前线程所对应的ThreadLocal值，如果当前线程下没有值，就调用initialValue函数对其进行初始化 public void remove() { }删除当前线程ThreadLocal对应的值，当前线程结束时，系统会自动回收线程局部变量值。P.S. remove方法调用后再调用get方法会使initialValue()重新被调用，从而使ThreadLocal的值被重新被初始化。 protected T initilValue() { }当前线程通过get()方法第一次对ThreadLocal进行访问时，会调用该方法。 如果我们希望ThreadLocal拥有一个不为null的初始值，则应该为ThreadLocal定义一个子类，并重写initialValue()方法。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA8 常用的日期时间方法]]></title>
    <url>%2F2019%2F11%2F07%2FJAVA8%E5%B8%B8%E7%94%A8%E7%9A%84%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[JAVA8 更新了原本线程不安全的SimpleDateFormat, 新的DateTimeFormatter则是线程安全的。 下面列出JAVA8开发中常用的时间操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.time.DayOfWeek;import java.time.Instant;import java.time.LocalDate;import java.time.LocalDateTime;import java.time.LocalTime;import java.time.Month;import java.time.ZoneId;import java.time.format.DateTimeFormatter;import java.time.temporal.ChronoField;public class TimeTest &#123; public static void main(String[] args) &#123; // 获取当前年月日 - 2019-11-06 LocalDate nowDate = LocalDate.now(); // 2019 int nowYear = nowDate.getYear(); // 2019 int nowYear1 = nowDate.get(ChronoField.YEAR); // NOVEMBER Month nowMonth = nowDate.getMonth(); // 11 int nowMonth1 = nowDate.get(ChronoField.MONTH_OF_YEAR); // 6 int nowDay = nowDate.getDayOfMonth(); // 6 int nowDay1 = nowDate.get(ChronoField.DAY_OF_MONTH); // WEDNESDAY DayOfWeek dayOfWeek = nowDate.getDayOfWeek(); // 3 int dayOfWeek1 = nowDate.get(ChronoField.DAY_OF_WEEK); // 获取当前时分秒 - 15:16:24.401 LocalTime nowTime = LocalTime.now(); // 指定时分秒 - 12:51:10 LocalTime nowTime1 = LocalTime.of(12, 51, 10); // 指定时间 - 2019-11-06 LocalDate specificTime = LocalDate.of(2019, 11, 6); // LocalDateTime 获取年月日时分秒 LocalDateTime localDateTime = LocalDateTime.now(); // 指定时间 2019-11-11T16:14:20 LocalDateTime localDateTime1 = LocalDateTime.of(2019, Month.NOVEMBER, 11, 16, 14, 20); // 2019-11-06T16:19:51.506 LocalDateTime localDateTime2 = LocalDateTime.of(nowDate, nowTime); // LocalDateTime localDateTime3 = LocalDat // 获取毫秒数 // 获取当前时间 2019-11-06T07:18:39.956Z Instant instant = Instant.now(); // 1573028684 得到的是秒数 long currentSecond = instant.getEpochSecond(); // 1573028779924 得到毫秒数 long currentMilli = instant.toEpochMilli(); // 获取当前年月日 - 2019-11-06 LocalDate date = Instant.ofEpochMilli(instant.toEpochMilli()).atZone(ZoneId.systemDefault()).toLocalDate(); // 格式化时间 // 2019-11-06 LocalDate localDate = LocalDate.of(2019, 11, 6); // 20191106 String str1 = localDate.format(DateTimeFormatter.BASIC_ISO_DATE); // 2019-11-06 String str2 = localDate.format(DateTimeFormatter.ISO_LOCAL_DATE); // 自定义格式化 DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); // 2019-11-06 16:41:10 String str3 = localDateTime.format(dateTimeFormatter); System.out.println(str3); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL常用知识笔记]]></title>
    <url>%2F2019%2F10%2F08%2FMySQL%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MySQL的优化方法。 数据库中事务的四个特性-ACID事务是一个操作序列，执行单个逻辑功能的一组指令或操作被称为事务。 A 原子性 Atomicity - 指事务是一个不可再分割的工作单元，事务中的操作要么都执行，要么都不执行。 B 一致性 Consistency - 事务的操作不会破坏数据库的数据的完整性以及业务逻辑上的一致性。 I 隔离性 Isolation - 多个事务并发访问时，每个事务都有自己的数据空间，数据所处于的状态要么是另一个事务修改前的状态，要么是另一个事务修改后的状态，数据不会处于中间状态。 D 持久性 Durability - 事务完成后对数据的变动是持久保存在数据库中的，不会被回滚。 MySQL的标准执行顺序从上到下： FROM ON JOIN WHERE GROUP BY WITH {CUBE|ROLLUP} HAVING SELECT DISTINCT ORDER BY LIMIT MySQL引擎MyISAM和Innodb的区别 MyISAM 引擎： 不支持事务，但是所有操作都是原子性的 每次操作都是对整个表加锁 一个MyISAM表有三个文件：索引文件，表结构文件，数据文件 采用非聚集索引，索引文件的数据域存储指向数据文件的指针，辅索引和主索引基本一致，但辅索引不用保证唯一性。 -InnoDB 引擎： 支持ACID四种隔离级别 支持行级锁和外键约束 不存储总行数 主键索引采用聚集索引，辅索引的数据域存储主键的值；因此从辅索引查找数据，需要先通过辅索引找到主键值，再访问辅索引；推荐使用自增主键，防止插入数据时，为了维持B+树结构，文件的大调整。 使用场景分析：读多，写少，对索引要求低用MyISAM;读少，写多，并发写入高时，用InnoDB;常见问题 场景：含有GroupBy的语句里如何显示查询Count()为0的字段。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-从入门到精通]]></title>
    <url>%2F2019%2F09%2F29%2FGit-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%2F</url>
    <content type="text"><![CDATA[Git Bash的简单使用方法。 git 可以分三个区： 工作区(注解编辑页面)，就是本地文件夹。暂存区(数据暂时存放的位置，可以在工作区和版本库之间进行数据的友好交流)，git add 命令将他们添加到暂存区。版本库/本地仓库(存放已经提交的数据，push的时候，就是将这个区的数据push到远程仓库了)，git commit 命令则将暂存区中的文件提交到本地仓库中。 首先连接到国际互联网: 123# 不需要的就不用了export http_proxy="http://127.0.0.1:1080/"export https_proxy="http://127.0.0.1:1080/" 创建代码仓库123456git config --global user.name "SAKURA"git config --global user.email "xxx@gmail.com"# 建立代码仓库(在指定目录建立.git文件夹)git init# 查看代码仓库状态git status 提交本地代码123456789git add READEME.txt# 全部更新到暂存区git add .# 在 -m 后面加上声明, 将代码提交到本地仓库git commit -m "Wrote a READEME file"# 如果要求的提交备注特别多,可以打开一个vi编辑器git commit# 从本地仓库提交到远程仓库master分支git push -u origin master 注释可以在代码仓库的根目录下创建一个名为.gitignore的文件，然后编辑里面的内容，把不需提交的文件忽略掉！ commit前的文件状态git status查看文件状态，Git在未进行commit操作之前，存在三种状态： Untracked files Changes not staged for commit Changes to be committed 分支的建立早建分支, 多用分支 123456# 建立分支git branch newImage# 切换到新分支 newImagegit checkout newImage;# 新分支代码提交git commit 上面的操作可以简化为下面两步 1234# 创建新的分支并切换到新分支git checkout -b newImage# 新分支代码提交git commit 分支的合并方法一：Merge 123456789101112# 建立新的分支bugFixgit branch bugFix# 切换到bugFix分支git checkout bugFix# bugFix代码提交git commit# 切换到master分支git checkout master# master分支代码提交git commit# 分支合并 - 现在master分支上有了bugFix的提交git merge bugFix 方法二：Rebase简单来说Rebase就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个将修改放下去，这样的好处是会使代码库的提交历史变得清晰。 123456# 假设当前是在bugFix分支下，将bugFix分支的工作直接移动到master分支上git rebase master# 切换到master分支git checkout master# 将master分支的引用向前移动，即rebase到bugFixgit rebase bugFix 在提交树上移动HEAD 总是指向当前分支上最近一次提交记录，HEAD通常是指向分支名的,但是可以通过git checkout &lt;哈希值&gt;来将HEAD指向一个具体的提交记录。 12345678# 查看HEAD指向的两种方式cat .git/HEAD# 如若HEAD指向的是一个引用git symbolic-ref HEAD # 分离的HEAD，为了让其指向某个具体的提交记录而不是分支名 注：这里的Hash值并不需要全部40位，# 能表示出唯一标识的前7位字符就可以git checkout &lt;提交记录的哈希值&gt;` 为了避免使用git log命令查看Hash值，再通过git checkout命令分离HEAD，所以有了相对引用 ^ 表示向上移动一个提交记录 123git checkout HEAD^# 或者git checkout master^ ~&lt;num&gt;表示向上移动多个提交记录, num为1时等价与^ 1234# 寻找指定提交记录的父提交git checkout HEAD~3# 或者git checkout master~3 相对引用最常见的地方就是用来移动分支 12# 将master分支移到HEAD所指分支位置前三个节点git branch -f master HEAD~3 撤销变更方法一：git reset - 仅限在本地分支中使用 123# 把分支记录回退几个提交记录来实现撤销改动,最新的提交变成了未加入暂存区状态(git reset 缺省为 git reset --soft)即文件没有更改# 只是将git commit 的信息给回退了。推荐直接使用 git reset --hard HEAD^, --hard参数会将文件直接修改回去。git reset HEAD~1 方法二：git revert - 适用于提交到了远程分支 12# 会在远程添加一个新的提交记录，用来抵消掉远程仓库最新的更改git revert HEAD 但是如果你后悔了怎么办。这个时候需要 1234# 查看所有版本号git reflog# 退回到指定版本 包括已经删除了的修改git reset --hard XXX&lt;版本号&gt; 提交记录的整理 将一些提交复制到当前所在的位置（HEAD）的下面，cherry-pick 可以将提交树上任何地方的提交记录取过来追加到 HEAD上（只要不是HEAD上游的提交就没有问题） 12# git cherry-pick &lt;提交号&gt;...git cherry-pick d4b052 4aa0aa 交互式的rebase 你可以通过给git rebase增加-i选项来以交互方式地运行rebase。你必须通过告诉命令衍合到哪次提交，来指明你需要重写的提交的回溯深度。 12# 这是一个衍合命令——HEAD~2..HEAD范围内的每一次提交都会被重写，无论你是否修改说明。git rebase -i HEAD~2 上述命令会打开一个交互窗口： 很重要的一点是你得注意这些提交的顺序与你通常通过log命令看到的是相反的。 通过rebase UI界面, 你能做3件事: 调整提交记录的顺序 删除你不想要的提交 合并提交, 把多个提交记录合并成一个。 详情请看这里 查看修改的内容1234# "+"号表示新增内容， "-"号表示删除的内容git diff# 可视化的查看方法git difftool 查看提交记录1git log 此次提交对应的版本号 提交人：姓名 邮箱 提交的时间 提交版本修改的内容：就是我们commit -m “xxx”里的xxx 记住密码1git config --global credential.helper store 版本回退提交后回退到上一个版本 需要版本号 HEAD代表当前版本, HEAD^表示上一个版本, 以此类推 1234567# 查看版本号git log git reset --hard f37911a60ca123c86c712ff0539619902a7375e8(目标版本号)git reset --hard HEADgit reset --hard HEAD^git log 如果你又后悔了 1git reflog 键入版本号 1git reset --hard 版本号(你要回退的版本号) 又会回到你期望的版本！ GitHub 初始化提交 create a new repository on the command line 1234567echo "# NOTES" &gt;&gt; README.mdgit init# git add .git add README.mdgit commit -m "first commit"git remote add origin https://github.com/XXX/NOTES.gitgit push -u origin master push an existing repository from the command line 12git remote add origin https://github.com/XXX/NOTES.gitgit push -u origin master 自动上传脚本:123456789#! /bin/bashecho 'start autodeployment...:)'hexo cleanecho 'hexo clean over...'hexo generateecho 'hexo generate over...'hexo deployecho 'hexo deploy over...'echo 'run success!:)' 常用操作场景：本地新建了一个新的文件夹，想拉取远程的一个项目 1234567git initgit remote add origin https://github.com/XXX/NOTES.git# git pull &lt;remote&gt; &lt;branch&gt; 直接指定远程mastergit pull origin master# 先指定本地master到远程的master，然后再去pullgit branch --set-upstream-to=origin/master mastergit pull 场景：将本地的.git文件删除了，本地文件也更新了，新建git仓库和远程合并 123456789101112git initgit remote add origin https://github.com/XXX/NOTES.git# 会报错，failed to push some refs to ...git push origin master# 这个时候需要先拉取远程的项目git pull origin master# 然后会报错，fatal: refusing to merge unrelated historiesgit pull origin master --allow-unrelated-histories# 然后按照要求解决冲突就好，再commit一下git add .git commit -m "fix conflicts"git push origin master 场景：远程仓库里提交了一些不需要的文件，本地添加到.gitignore文件后不起作用 123git rm -r --cached .git add .git commit -m 'update .gitignore' 场景：远程仓库有两个分支master和dev，比较二者的版本差异 123456789101112# 查看远程dev有但是远程master没有的git log origin/dev ^origin/master# 查看远程master有但是远程dev没有的git log origin/dev..origin/master# 查看远程master有但是远程dev没有的git log origin/master ^origin/dev# 查看远程dev有但是远程master没有的git log origin/master..origin/dev# 单纯想比较两者之间的区别git log origin/dev...origin/master# 或者git log --left-right dev...master 上述命令可以加上--pretty=oneline让结果输出更简洁。 场景：处理完冲突后将文件查看分支的更改情况 12345git log --graph --pretty=oneline --abbrev-commit# 起别名git config --global alias.lg “log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit”# 直接使用git lg 场景：新建分支并上传，并给分支添加tag 123456789101112131415161718192021222324252627282930313233343536373839#!/bin/bashecho "请输入项目名称"read project_dirif [ ! -n "$project_dir" ]thenproject_dir="/"fiecho "cd ./$project_dir"cd ./$project_dirgit initecho "git init"git add .echo "git add ."git commit -m "R16.40"echo "git commit -m "R16.40""echo "请输入仓库地址"read repository_urlif [ ! -n "$repository_url" ]thenecho "仓库地址不正确"figit remote add origin $repository_urlecho "git remote add origin $repository_url"git checkout -b R16.40echo "git checkout -b R16.40"git push -u origin R16.40:R16.40echo "git push -u origin R16.40:R16.40"echo "请输入tag"read project_tagif [ ! -n "$project_tag" ]thenecho "项目标签不正确"figit tag $project_tagecho "git tag $project_tag"git push origin $project_tagecho "git push origin $project_tag"echo "代码上传成功"sleep 5s]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识-Map]]></title>
    <url>%2F2019%2F09%2F09%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-Map%2F</url>
    <content type="text"><![CDATA[JAVA基础知识-Map 相关知识。 遍历Map 方法一： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class SlowMap &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put("A", "01"); map.put("B", "02"); map.put("C", "03"); // 获取键的集合 Set&lt;String&gt; keySet = map.keySet(); // 利用set集合的迭代器 Iterator&lt;String&gt; iterator = keySet.iterator(); while(iterator.hasNext()) &#123; String key = iterator.next(); String value = map.get(key); System.out.println("MAP INFO KEY:" + key + " VALUE:" + value); &#125; &#125;&#125;``` &gt; 方法二：```javapublic class QuickMap &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put("A", "01"); map.put("B", "02"); map.put("C", "03"); // 通过entrySet()方法将map集合中的映射关系取出 Set&lt;Map.Entry&lt;String, String&gt;&gt; entry = map.entrySet(); // 利用关系集合entrySet的迭代器 Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = entry.iterator(); while(iterator.hasNext()) &#123; // 获取Map.Entry的关系对象me Map.Entry&lt;String, String&gt; me = iterator.next(); // 通过关系对象获取key String key = me.getKey(); // 通过关系对象获取value String value = me.getValue(); System.out.println("Key :" + key + " value :" + value); &#125; &#125;&#125; 方法三： 1234567891011121314151617public class CollectionMap &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put("A", "01"); map.put("B", "02"); map.put("C", "03"); // Collection集合存放Map的value值 Collection&lt;String&gt; collection = map.values(); // 遍历Collection Iterator&lt;String&gt; it = collection.iterator(); // 只能遍历value值，不能遍历key值 while(it.hasNext()) &#123; Object value = it.next(); System.out.println(value); &#125; &#125;&#125; 最佳实践： 1234567891011public class BestMap &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put("A", "01"); map.put("B", "02"); map.put("C", "03"); for (Map.Entry&lt;String, String&gt; entry : map.entrySet()) &#123; System.out.println("key= " + entry.getKey() + " and value= " + entry.getValue()); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识-HashMap]]></title>
    <url>%2F2019%2F09%2F05%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-HashMap%2F</url>
    <content type="text"><![CDATA[JAVA基础知识-HashMap 相关知识。 常用的通配符T, E, K, V, ? 本质上没有任何区别，但是为了维持可读性，常这样约定： ？表示不确定的JAVA类型 T (type) 表示一个具体的JAVA类型 K V (key value) 分别表示JAVA键值对 key - value E (element) 代表Element RBT(红黑树) 每个节点是红色或者黑色 根节点是黑色 每个叶节点是黑色的 如果一个节点是红色的，则它的两个儿子都是黑色的 对于每个节点，从该结点到其叶子节点构成的所有路径上的黑色结点个数相同 插入过程： 默认插入节点为红色 AVL(自平衡二叉查找树) 首先是一棵二叉查找树 某节点的左子树节点值仅包含小于该节点值 某节点的右子树节点值仅包含大于该节点值 左右子树每个也必须是二叉查找树 带有平衡条件，每个结点的左右子树的高度之差的绝对值（平衡因子）最多为1 RBT和AVL查找远远多于插入删除，选择AVL树 如果查找、插入、删除频率差不多，那么选择红黑树 JAVA 中红黑树的实现12345678910static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; 红黑树的插入过程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; // 初始染色为红 x.red = true; for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; // 插入的为根节点（父节点为空），则直接把颜色改为黑色 if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // 父节点为黑色节点或者插入节点的祖父节点为空，则直接返回 else if (!xp.red || (xpp = xp.parent) == null) return root; // I. 父节点和祖父节点都存在且父节点是祖父节点的左节点 if (xp == (xppl = xpp.left)) &#123; // 1. 祖父节点的右节点（叔叔节点）不是空且为红色 if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; // 将叔叔节点改为黑色 xppr.red = false; // 将父亲节点改为黑色 xp.red = false; // 将祖父节点改为红色 xpp.red = true; x = xpp; &#125; // 2. 祖父节点的右节点（叔叔节点）是空或者为黑色 else &#123; // 插入节点是父节点的右孩子，将父节点左旋 if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 插入节点是父节点的左孩子 if (xp != null) &#123; // 将父节点变成黑色节点 xp.red = false; if (xpp != null) &#123; // 祖父节点变成红色节点，然后将祖父节点右旋 xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; // II. 父节点和祖父节点都存在且父节点是祖父节点的右节点 else &#123; // 1. 祖父节点的左节点（叔叔节点）不为空且为红色 if (xppl != null &amp;&amp; xppl.red) &#123; // 将叔叔节点改为黑色 xppl.red = false; // 将父亲节点改为黑色 xp.red = false; // 将祖父节点改为红色 xpp.red = true; x = xpp; &#125; // 2. 祖父节点的左节点（叔叔节点）是空或者为黑色 else &#123; // 插入节点是父节点的左孩子，将父节点右旋 if (x == xp.left) &#123; root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 插入节点是父节点的右孩子 if (xp != null) &#123; // 将父节点变成黑色节点 xp.red = false; if (xpp != null) &#123; // 祖父节点变成红色节点，然后将祖父节点左旋 xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125;&#125; 左旋右旋方法： 123456789101112131415161718192021222324252627282930313233static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; r, pp, rl; if (p != null &amp;&amp; (r = p.right) != null) &#123; if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; &#125; return root;&#125;static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; l, pp, lr; if (p != null &amp;&amp; (l = p.left) != null) &#123; if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; &#125; return root;&#125; 红黑树的删除过程： 先进行二叉搜索树的删除 然后进行红黑树的调整 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// p是待删除节点，replacement用于后续的红黑树调整，指向的是p或者p的继承者。if (pl != null &amp;&amp; pr != null) &#123; TreeNode&lt;K,V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K,V&gt; sr = s.right; TreeNode&lt;K,V&gt; pp = p.parent; if (s == pr) &#123; // p was s's direct parent p.parent = s; s.right = p; &#125; else &#123; TreeNode&lt;K,V&gt; sp = s.parent; if ((p.parent = sp) != null) &#123; if (s == sp.left) sp.left = p; else sp.right = p; &#125; if ((s.right = pr) != null) pr.parent = s; &#125; p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p;&#125;else if (pl != null) replacement = pl;else if (pr != null) replacement = pr;else replacement = p;if (replacement != p) &#123; TreeNode&lt;K,V&gt; pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null;&#125;// 如果删除时的节点p是红色，则树平衡不会被破坏，无需调整。 TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement);if (replacement == p) &#123; // detach TreeNode&lt;K,V&gt; pp = p.parent; p.parent = null; if (pp != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; &#125;&#125;if (movable) moveRootToFront(tab, r); 删除后的调整 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112// root : 根节点 | x : 根节点的继承者static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; for (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123; // 如果x为空或x为根节点，直接返回 if (x == null || x == root) return root; // x为根节点，染成黑色，直接返回(删除的为根节点，则扶植继承者x为新的根节点) else if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // x为红色，则无需调整，直接返回根节点 else if (x.red) &#123; x.red = false; return root; &#125; // x为其父节点的左孩子 else if ((xpl = xp.left) == x) &#123; // 如果x有红色兄弟节点xpr, 则它的父节点xp一定是黑色节点 if ((xpr = xp.right) != null &amp;&amp; xpr.red) &#123; xpr.red = false; xp.red = true; // 将父节点左旋 root = rotateLeft(root, xp); // 重新将xp指向x的父节点，xpr指向xp新的右孩子 xpr = (xp = x.parent) == null ? null : xp.right; &#125; //如果xpr为空，则向上继续调整，将x的父节点xp作为新的x继续循环 if (xpr == null) x = xp; else &#123; // sl和sr分别为其近侄子和远侄子 TreeNode&lt;K,V&gt; sl = xpr.left, sr = xpr.right; // 若s1和s2都为黑色或者不存在，即xpr没有红色孩子，则将xpr染红 if ((sr == null || !sr.red) &amp;&amp; (sl == null || !sl.red)) &#123; xpr.red = true; // 继续向上循环 x = xp; &#125; else &#123; // 右孩子为空或者为黑色 if (sr == null || !sr.red) &#123; // 有左孩子则染黑 if (sl != null) sl.red = false; // 将xpr染红 xpr.red = true; // xpr右旋 root = rotateRight(root, xpr); // 右旋后xpr指向xp的新右孩子，即上面的s1 xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr != null) &#123; // xpr染成跟父节点一致的颜色，为后面父节点xp的左旋做准备 if ((sr = xpr.right) != null) xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) // xpr新的右孩子染黑，防止出现两个红色相连 sr.red = false; &#125; if (xp != null) &#123; // 将xp染黑，并对其左旋，这样就能保证被删除的X所在的路径又多了一个黑色节点，从而达到恢复平衡的目的 xp.red = false; root = rotateLeft(root, xp); &#125; // 到此调整已经完毕，进入下一次循环后将直接退出 x = root; &#125; &#125; &#125; // x为其父节点的右孩子... else &#123; // symmetric if (xpl != null &amp;&amp; xpl.red) &#123; xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) &amp;&amp; (sr == null || !sr.red)) &#123; xpl.red = true; x = xp; &#125; else &#123; if (sl == null || !sl.red) &#123; if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl != null) &#123; xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateRight(root, xp); &#125; x = root; &#125; &#125; &#125; &#125;&#125; 重写HashCode()方法和Equals()方法1234567891011121314151617// Prediction.javaimport java.util.Random;public class Prediction &#123; private static Random rand = new Random(47); private boolean shadow = rand.nextDouble() &gt; 0.5; public String toString() &#123; if (shadow) &#123; return "Six more weeks of Winner!"; &#125; else &#123; return "Early Spring"; &#125; &#125;&#125; 123456789101112// Groundhog.javapublic class Groundhog &#123; protected int number; public Groundhog(int n) &#123; number = n; &#125; public String toString() &#123; return "Groundhog #" + number; &#125;&#125; 123456789101112131415161718192021222324252627282930313233// SpringDetector.javaimport java.lang.reflect.Constructor;import java.util.HashMap;import java.util.Map;public class SpringDetector &#123; // 利用反射机制获取构造函数 public static &lt;T extends Groundhog&gt; void detectSpring(Class&lt;T&gt; type) throws Exception &#123; Constructor&lt;T&gt; ghog = type.getConstructor(int.class); Map&lt;Groundhog, Prediction&gt; map = new HashMap&lt;Groundhog, Prediction&gt;(); for (int i = 0; i &lt; 10; i++) &#123; map.put(ghog.newInstance(i), new Prediction()); &#125; System.out.println("MAP" + map); Groundhog groundhog = ghog.newInstance(3); System.out.println("Looking up prediction for" + groundhog); if (map.containsKey(groundhog)) &#123; System.out.println(map.get(groundhog)); &#125; else &#123; System.out.println("Key not found:" + groundhog); &#125; &#125; public static void main(String[] args) throws Exception &#123; detectSpring(Groundhog.class); &#125;&#125; Output: 123MAP&#123;Groundhog #1=Six more weeks of Winner!, Groundhog #4=Six more weeks of Winner!, Groundhog #5=Early Spring, Groundhog #3=Early Spring, Groundhog #8=Six more weeks of Winner!, Groundhog #7=Early Spring, Groundhog #0=Six more weeks of Winner!, Groundhog #2=Early Spring, Groundhog #9=Six more weeks of Winner!, Groundhog #6=Early Spring&#125;Looking up prediction forGroundhog #3Key not found:Groundhog #3 Groundhog默认调用的是基类Object的hashCode方法，默认使用的是对象的地址计算的散列码。同时只重写hashCode()方法是不够的，还需要重写equals()方法。 补充一句，hashCode()是基类Object的方法，所有Java对象都能产生散列码。 12345678910111213// Groundhog2.javapublic class Groundhog2 extends Groundhog &#123; public Groundhog2(int n) &#123; super(n); &#125; public int hashCode() &#123; return number; &#125; public boolean equals(Object object) &#123; return object instanceof Groundhog2 &amp;&amp; (number == ((Groundhog2)object).number); &#125;&#125; 123456// SpringDetector2.javapublic class SpringDetector2 &#123; public static void main(String[] args) throws Exception &#123; SpringDetector.detectSpring(Groundhog2.class); &#125;&#125; Output: 123MAP&#123;Groundhog #0=Six more weeks of Winner!, Groundhog #1=Six more weeks of Winner!, Groundhog #2=Early Spring, Groundhog #3=Early Spring, Groundhog #4=Six more weeks of Winner!, Groundhog #5=Early Spring, Groundhog #6=Early Spring, Groundhog #7=Early Spring, Groundhog #8=Six more weeks of Winner!, Groundhog #9=Six more weeks of Winner!&#125;Looking up prediction forGroundhog #3Early Spring]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[データとの絆]]></title>
    <url>%2F2019%2F08%2F15%2FTHE_DATA_ANALYSIS%2F</url>
    <content type="text"><![CDATA[もう一回もう一回行こうぜ 僕らの声 アイムアルーザー ずっと前から聞こえてた 准备工作1init_woody Matplotlib env init complete. Warnings off.1234import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as plt 12# 不省略行的查看数据pd.set_option('display.max_columns', 200) 数据源准备1234567891011121314151617181920212223242526272829303132333435363738# 导入train数据train_idv_td = pd.read_csv("../data/2/train/IDV_TD.csv", encoding='utf-8') # 个人定期存款账户信息（IDV_TD）train_idv_dpsa = pd.read_csv("../data/2/train/IDV_DPSA.csv", encoding='utf-8') # 个人活期存款账户信息（IDV_DPSA）train_loan = pd.read_csv("../data/2/train/LOAN.csv", encoding='utf-8') # 贷款账户信息（LOAN）train_bond = pd.read_csv("../data/2/train/BOND.csv", encoding='utf-8') # 国债账户信息（BOND）train_fund = pd.read_csv("../data/2/train/FUND.csv", encoding='utf-8') # 基金账户信息（FUND）train_prec_metal = pd.read_csv("../data/2/train/PREC_METAL.csv", encoding='utf-8') # 贵金属账户信息（PREC_METAL）train_aget_insr = pd.read_csv("../data/2/train/AGET_INSR.csv", encoding='utf-8') # 代理保险账户信息（AGET_INSR）train_thr_pty_cstd = pd.read_csv("../data/2/train/THR_PTY_CSTD.csv", encoding='utf-8') # 第三方存管账户信息（THR_PTY_CSTD）train_idv_cust_basic = pd.read_csv("../data/2/train/IDV_CUST_BASIC.csv", encoding='utf-8') # 个人客户基本信息（IDV_CUST_BASIC）train_tr_dc = pd.read_csv("../data/2/train/TR_DC.csv", encoding='utf-8') # 交易信息（TR_DC）train_base_excg = pd.read_csv("../data/2/train/BASE_EXCG.csv", encoding='utf-8') # 汇率表（BASE_EXCG）train_cust_result = pd.read_csv("../data/2/train/CUST_RESULT.csv", encoding='utf-8') # 客户标记（CUST_RESULT）# A榜# BASE_EXCG.csv 无A_idv_td = pd.read_csv("../data/2/A/IDV_TD.csv", encoding='utf-8')A_idv_dpsa = pd.read_csv("../data/2/A/IDV_DPSA.csv", encoding='utf-8')A_loan = pd.read_csv("../data/2/A/LOAN.csv", encoding='utf-8')A_bond = pd.read_csv("../data/2/A/BOND.csv", encoding='utf-8')A_fund = pd.read_csv("../data/2/A/FUND.csv", encoding='utf-8')A_prec_metal = pd.read_csv("../data/2/A/PREC_METAL.csv", encoding='utf-8')A_aget_insr = pd.read_csv("../data/2/A/AGET_INSR.csv", encoding='utf-8')A_thr_pty_cstd = pd.read_csv("../data/2/A/THR_PTY_CSTD.csv", encoding='utf-8')A_idv_cust_basic = pd.read_csv("../data/2/A/IDV_CUST_BASIC.csv", encoding='utf-8')A_tr_dc = pd.read_csv("../data/2/A/TR_DC.csv", encoding='utf-8')A_customid = pd.read_csv("../data/2/A/CUSTOMID.csv", encoding='utf-8')# B榜B_idv_td = pd.read_csv("../data/2/B/IDV_TD.csv", encoding='utf-8')B_idv_dpsa = pd.read_csv("../data/2/B/IDV_DPSA.csv", encoding='utf-8')B_loan = pd.read_csv("../data/2/B/LOAN.csv", encoding='utf-8')B_bond = pd.read_csv("../data/2/B/BOND.csv", encoding='utf-8')B_fund = pd.read_csv("../data/2/B/FUND.csv", encoding='utf-8')B_prec_metal = pd.read_csv("../data/2/B/PREC_METAL.csv", encoding='utf-8')B_aget_insr = pd.read_csv("../data/2/B/AGET_INSR.csv", encoding='utf-8')B_thr_pty_cstd = pd.read_csv("../data/2/B/THR_PTY_CSTD.csv", encoding='utf-8')B_idv_cust_basic = pd.read_csv("../data/2/B/IDV_CUST_BASIC.csv", encoding='utf-8')B_tr_dc = pd.read_csv("../data/2/B/TR_DC.csv", encoding='utf-8')B_customid = pd.read_csv("../data/2/B/CUSTOMID.csv", encoding='utf-8') /root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result)1234567# 把汇率表中的CCY_LETE_CD字段转化为CCY_CD字段train_base_excg = pd.read_csv("../data/2/train/BASE_EXCG.csv", encoding='utf-8') # 汇率表（BASE_EXCG）train_base_excg['CCY_CD'] = train_base_excg['CCY_LETE_CD']train_base_excg = train_base_excg.drop(['CCY_LETE_CD'], axis=1)train_base_excg['RMB_MID_PRIC'].apply(lambda x: float(x))# 把汇率表中的CCY_LETE_CD字段转化为CCY_CD字段train_tr_dc['TR_DAT'].apply(lambda x: int(x)) 公用函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 按汇率计算价值（传入2个参数返回计算结果）def compute_REAL_MONEY(train_IDV_DPSA_SAMPLE, A, B): NEED = train_IDV_DPSA_SAMPLE[A] RMB_MID_PRIC = train_IDV_DPSA_SAMPLE[B] return NEED * float(RMB_MID_PRIC)# one-hot 处理函数# 本地评价def evaluate(test_y, y_pred): from sklearn import metrics test_pct_1 = test_y.sum()/test_y.shape[0] pred_pct_1 = y_pred.sum()/y_pred.shape[0] roc_auc = metrics.roc_auc_score(test_y,y_pred) acc = metrics.accuracy_score(test_y,y_pred) Recall = metrics.recall_score(test_y,y_pred) F1 = metrics.f1_score(test_y,y_pred) Precision = metrics.precision_score(test_y,y_pred) Confusion = metrics.confusion_matrix(test_y,y_pred) print ('test_pct_1: %.4f' % test_pct_1) print ('pred_pct_1: %.4f' % pred_pct_1) print ('Precesion: %.4f' % Precision) print ('Recall: %.4f' % Recall) print ('F1-score: %.4f' % F1) print('confusion matrix:') print (metrics.confusion_matrix(test_y,y_pred)) return &#123;'test_pct_1':test_pct_1,'pred_pct_1':pred_pct_1,'roc_auc':roc_auc,'acc':acc,'recall':Recall,'F1':F1,'Precision':Precision,'Confusuion':Confusion&#125;def feature_rank(model,num): ##计算特征得分 feature_score = model.get_fscore() df_feature_score = pd.DataFrame(&#123;"feature_name":list(feature_score.keys()),"feature_score":list(feature_score.values())&#125;) df_feature_score_sort = df_feature_score.sort_values(ascending=0,by=['feature_score']) #result_name = "feature_score_1_A_%s_%s_%s_%6f.csv"%(time_str,feature_num,train_steps,F1_score) #result_name_full = datas_dir_e + "out/" + result_name #df_feature_score_sort.to_csv(result_name_full,index=None) return df_feature_score_sort.head(num)def stat_df(df): stats = [] for col in df.columns: stats.append((col, df[col].nunique(),df[col].isnull().sum()*100/df.shape[0], df[col].value_counts(normalize=True,dropna=False).values[0]*100,df[col].dtype)) stats_df = pd.DataFrame(stats, columns=['特征','唯一数数量','缺失值占比','最多数占比','类型']) stats_df.sort_values('缺失值占比',ascending=False) return stats_dfdef plot_feature_distribution(df1,df2,label1,label2,features,width=6,height=6): i=0 sns.set_style('whitegrid') plt.figure() fig,ax = plt.subplots(width,height,figsize=(20,12)) for feature in features: i+=1 plt.subplot(width,height,i) sns.kdeplot(df1[feature],bw=0.5,label=label1) sns.kdeplot(df2[feature],bw=0.5,label=label2) plt.xlabel(feature,fontsize=9) locs, labels = plt.xticks() plt.tick_params(axis='x',which='major',labelsize=6,pad=-6) plt.tick_params(axis='y',which='major',labelsize=6) plt.show() 特征工程个人客户基本信息表(IDV_CUST_BASIC) - 处理1234567# 删除没用的字段train_idv_cust_basic_SAMPLE = train_idv_cust_basic.drop(['DATA_DAT', 'PROV_CD', 'CUST_SEX_CD', 'RES_CD', 'RES_STA_CD', 'NATY_CD', 'NATN_CD', 'CULT_DGR_CD', 'DGR_CD', 'PLC_STS_CD', 'PRFN', 'ADMI_POS_CD', 'SPEC_TECH_PRFN_QUA_CD', 'TITLE_RANK_CD', 'WORK_TYP_CD', 'GC_BRTH', 'UNIT_PROP_CD', 'WORK_YEAR', 'OCP_CD'], axis=1)# 将个人客户基本信息表与结果整合train_idv_cust_basic_SAMPLE = pd.merge(train_idv_cust_basic_SAMPLE, train_cust_result, on='CUST_NO') 12345features = [x for x in train_idv_cust_basic_SAMPLE.columns if x not in ['CUST_NO','FLAG']]df0 = train_idv_cust_basic_SAMPLE[train_idv_cust_basic_SAMPLE.FLAG==0]df1 = train_idv_cust_basic_SAMPLE[train_idv_cust_basic_SAMPLE.FLAG==1]plot_feature_distribution(df0,df1,'0','1',features,2,2) /root/anaconda3/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:454: RuntimeWarning: invalid value encountered in greater X = X[np.logical_and(X&gt;clip[0], X&lt;clip[1])] # won&apos;t work for two columns. /root/anaconda3/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:454: RuntimeWarning: invalid value encountered in less X = X[np.logical_and(X&gt;clip[0], X&lt;clip[1])] # won&apos;t work for two columns. &lt;matplotlib.figure.Figure at 0x7f0167e93dd8&gt; 12345678# 声明函数def IDV_CUST_BASIC_PARSE(dataset_IDV_CUST_BASIC): # 删除个人用户信息表中无关元素 dataset_IDV_CUST_BASIC = dataset_IDV_CUST_BASIC.drop(['DATA_DAT', 'PROV_CD', 'CUST_SEX_CD', 'RES_CD', 'RES_STA_CD', 'NATY_CD', 'NATN_CD', 'CULT_DGR_CD', 'DGR_CD', 'PLC_STS_CD', 'PRFN', 'ADMI_POS_CD', 'SPEC_TECH_PRFN_QUA_CD', 'TITLE_RANK_CD', 'WORK_TYP_CD', 'GC_BRTH', 'UNIT_PROP_CD', 'WORK_YEAR', 'OCP_CD'], axis=1) return dataset_IDV_CUST_BASIC 个人定期存款账户信息（IDV_TD）- 处理12# 查看数据比例stat_df(train_idv_td) 12345678910111213141516171819202122232425262728# 编成函数def IDV_TD_PARSE(dataset_IDV_TD): train_idv_td_3 = dataset_IDV_TD[dataset_IDV_TD['DATA_DAT']==3734035200] # 简化IDV_TD表 train_idv_td_3 = train_idv_td_3.drop(['DATA_DAT', 'ARG_CRT_DAT', 'DATA_DAT', 'CLS_ACCT_DAT', 'MATU_DAT', 'LAC', 'ACCT_STS_CD','DP_DAY_CD', 'RDEP_IND_CD', 'RDEP_DP_DAY_CD', 'RAT_CTG','FXDI_SA_ACCM', 'MTH_ACT_DAYS_TOT'], axis=1) # 去重 train_idv_td_3.drop_duplicates(subset=None, keep='first', inplace=True) # 将train_idv_td和train_base_excg合并 train_IDV_TD_SAMPLE_3 = pd.merge(train_idv_td_3, train_base_excg, on = 'CCY_CD') # 执行汇率计算函数 train_IDV_TD_SAMPLE_3['CRBAL'] = train_IDV_TD_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('CRBAL', 'RMB_MID_PRIC')) train_IDV_TD_SAMPLE_3['REG_CAP'] = train_IDV_TD_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('REG_CAP', 'RMB_MID_PRIC')) train_IDV_TD_SAMPLE_3['FXDI_T_ACCM'] = train_IDV_TD_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('FXDI_T_ACCM', 'RMB_MID_PRIC')) train_IDV_TD_SAMPLE_3['TDOP_SHD_PAY_INTS'] = train_IDV_TD_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('TDOP_SHD_PAY_INTS', 'RMB_MID_PRIC')) train_IDV_TD_SAMPLE_3['MOTH_CR_ACCM'] = train_IDV_TD_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('MOTH_CR_ACCM', 'RMB_MID_PRIC')) train_IDV_TD_SAMPLE_3['IDV_TD_SUM'] = train_IDV_TD_SAMPLE_3['TDOP_SHD_PAY_INTS'] + train_IDV_TD_SAMPLE_3['REG_CAP'] # 再次精简表 train_IDV_TD_SAMPLE_3 = train_IDV_TD_SAMPLE_3.drop(['CCY_CD', 'RMB_MID_PRIC'], axis=1) # 数据去重 train_IDV_TD_SAMPLE_3.drop_duplicates(subset=None, keep='first', inplace=True) # 将个人账户里的数据合并即金额加起来 train_IDV_TD_SAMPLE_SUM_3 = train_IDV_TD_SAMPLE_3[['CUST_NO', 'CRBAL', 'REG_CAP', 'FXDI_T_ACCM', 'TDOP_SHD_PAY_INTS', 'MOTH_CR_ACCM', 'IDV_TD_SUM']].groupby('CUST_NO').sum().reset_index() # 区分字段 train_IDV_TD_SAMPLE_SUM_3 = train_IDV_TD_SAMPLE_SUM_3.rename(columns=&#123;'CRBAL':'IDV_TD_CRBAL', 'MOTH_CR_ACCM':'IDV_TD_MOTH_CR_ACCM'&#125;) # @ 去除过分关联的特征 train_IDV_TD_SAMPLE_SUM_3 = train_IDV_TD_SAMPLE_SUM_3.drop(['TDOP_SHD_PAY_INTS', 'REG_CAP'], axis=1) # 返回第三个时间段的数据 return train_IDV_TD_SAMPLE_SUM_3 个人活期存款账户信息（IDV_DPSA）- 处理1234567891011121314151617181920212223242526272829303132# 编成函数def IDV_DPSA_PARSE(dataset_IDV_DPSA): # 划分三张表 train_idv_dpsa_1 = dataset_IDV_DPSA[dataset_IDV_DPSA['DATA_DAT']==3728764800].reset_index() train_idv_dpsa_2 = dataset_IDV_DPSA[dataset_IDV_DPSA['DATA_DAT']==3731443200].reset_index() train_idv_dpsa_3 = dataset_IDV_DPSA[dataset_IDV_DPSA['DATA_DAT']==3734035200].reset_index() # 精简字段 train_idv_dpsa_3 = train_idv_dpsa_3.drop(['DATA_DAT', 'ARG_CRT_DAT', 'CLS_ACCT_DAT', 'MATU_DAT', 'LAC', 'ACCT_STS_CD', 'RAT_CTG', 'CUST_RANK_CD', 'DAY_TFO_SUM', 'MTH_ACT_DAYS_TOT'], axis=1) # 将IDV_DPSA表与汇率表整合 train_IDV_DPSA_SAMPLE_3 = pd.merge(train_idv_dpsa_3, train_base_excg, on = 'CCY_CD') # 去重 train_IDV_DPSA_SAMPLE_3.drop_duplicates(subset=None, keep='first', inplace=True) # 执行汇率计算函数 train_IDV_DPSA_SAMPLE_3['FRZ_TOT_AMT'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('FRZ_TOT_AMT', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['DAY_WD_ACT_AMT'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_WD_ACT_AMT', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['DAY_CSH_DP_SUM'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_CSH_DP_SUM', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['DAY_CSH_WD_SUM'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_CSH_WD_SUM', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['DAY_TFI_SUM'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_TFI_SUM', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['MOTH_CR_ACCM'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('MOTH_CR_ACCM', 'RMB_MID_PRIC')) train_IDV_DPSA_SAMPLE_3['BEG_MOTH_CRBAL'] = train_IDV_DPSA_SAMPLE_3.apply(compute_REAL_MONEY, axis = 1, args = ('BEG_MOTH_CRBAL', 'RMB_MID_PRIC')) # 再次精简表 train_IDV_DPSA_SAMPLE_3 = train_IDV_DPSA_SAMPLE_3.drop(['CCY_CD', 'RMB_MID_PRIC', 'index'], axis=1) # 避免重复字段 train_IDV_DPSA_SAMPLE_3 = train_IDV_DPSA_SAMPLE_3.rename(columns=&#123;'CRBAL':'IDV_DPSA_CRBAL', 'MOTH_CR_ACCM':'IDV_DPSA_MOTH_CR_ACCM'&#125;) # 按 CUST_NO 求和 train_IDV_DPSA_SAMPLE_3 = train_IDV_DPSA_SAMPLE_3[['CUST_NO', 'IDV_DPSA_CRBAL', 'ITST_BRNG_ACCM', 'FRZ_TOT_AMT', 'DAY_WD_ACT_AMT', 'DAY_CSH_DP_SUM', 'DAY_CSH_WD_SUM', 'DAY_TFI_SUM', 'IDV_DPSA_MOTH_CR_ACCM', 'BEG_MOTH_CRBAL']].groupby('CUST_NO').sum().reset_index() # @ 去除过分关联的特征 train_IDV_DPSA_SAMPLE_3 = train_IDV_DPSA_SAMPLE_3.drop(['BEG_MOTH_CRBAL', 'IDV_DPSA_MOTH_CR_ACCM'], axis=1) # 返回第三个时间段的数据 return train_IDV_DPSA_SAMPLE_3 交易信息（TR_DC） - 处理1234train_tr_dc_TR_TYPE = train_tr_dc.groupby(['CUST_NO','TR_TYPE'])['TR_TYPE'].count().unstack().reset_index()train_tr_dc_TR_TYPE['TR_TYPE_TIMES'] = train_tr_dc_TR_TYPE.drop('CUST_NO',axis=1).sum(axis=1)train_tr_dc_BOE = train_tr_dc_TR_TYPE[['CUST_NO','TR_TYPE_TIMES','EBMKO5RA']]train_tr_dc_BOE.columns = ['CUST_NO', 'TR_TYPE_TIMES','EBMKO5RA'] 1train_tr_dc_BOE.head() 12345678910111213141516171819202122232425262728293031323334# 声明函数def TR_DC_PARSE(dataset_TR_DC): # 去除抹帐(1),无意义列- RED_BLU_CD，这里全是0 dataset_TR_DC = dataset_TR_DC[dataset_TR_DC['CAN_IND']==0].drop(['RED_BLU_CD'],axis=1) # 将交易按照 正 / 负 分开 dataset_TR_DC['INCOME'] = dataset_TR_DC['TR_AMT'] &gt; 0 myseries = dataset_TR_DC.groupby(['CUST_NO', 'INCOME'])['TR_AMT'].sum() myseries = myseries.unstack() TR_DC_IN_OUT = pd.DataFrame(myseries).reset_index() TR_DC_IN_OUT['IN_SUM'] = TR_DC_IN_OUT[True] TR_DC_IN_OUT['OUT_SUM'] = TR_DC_IN_OUT[False] TR_DC_IN_OUT = TR_DC_IN_OUT.drop([True, False], axis=1) # 根据BOE交易代码划分 仅使用 EBMKO5RA train_tr_dc_TR_TYPE = dataset_TR_DC.groupby(['CUST_NO','TR_TYPE'])['TR_TYPE'].count().unstack().reset_index() train_tr_dc_TR_TYPE['TR_TYPE_TIMES'] = train_tr_dc_TR_TYPE.drop('CUST_NO',axis=1).sum(axis=1) train_tr_dc_BOE = train_tr_dc_TR_TYPE[['CUST_NO','TR_TYPE_TIMES','EBMKO5RA']] train_tr_dc_BOE.columns = ['CUST_NO', 'TR_TYPE_TIMES','EBMKO5RA'] # 根据SVRTO交易码划分 使用SVRTO061和SVRTO161 train_tr_dc_SVRTO061 = train_tr_dc_TR_TYPE[['CUST_NO','SVRTO061']] train_tr_dc_SVRTO161 = train_tr_dc_TR_TYPE[['CUST_NO','SVRTO161']] train_tr_dc_SVRTO061.columns = ['CUST_NO', 'SVRTO061'] train_tr_dc_SVRTO161.columns = ['CUST_NO', 'SVRTO161'] # 计算每笔交易金额的均值 TR_DC_IN_MEAN = dataset_TR_DC[dataset_TR_DC['TR_AMT']&gt;0][['CUST_NO', 'TR_AMT']].groupby('CUST_NO').mean().reset_index() TR_DC_IN_MEAN.columns = ['CUST_NO', 'TR_DC_IN_MEAN'] TR_DC_OUT_MEAN = dataset_TR_DC[dataset_TR_DC['TR_AMT']&lt;0][['CUST_NO', 'TR_AMT']].groupby('CUST_NO').mean().reset_index() TR_DC_OUT_MEAN.columns = ['CUST_NO', 'TR_DC_OUT_MEAN'] # 合并各个特征 36041 # train_TR_DC_SAMPLE = pd.merge(TR_DC_IN_OUT, train_tr_dc_BOE, on='CUST_NO', how='outer') train_TR_DC_SAMPLE = pd.merge(TR_DC_IN_OUT, TR_DC_IN_MEAN, on='CUST_NO', how='outer') train_TR_DC_SAMPLE = pd.merge(train_TR_DC_SAMPLE, TR_DC_OUT_MEAN, on='CUST_NO', how='outer') # train_TR_DC_SAMPLE = pd.merge(train_TR_DC_SAMPLE, train_tr_dc_SVRTO061, on='CUST_NO', how='outer') # train_TR_DC_SAMPLE = pd.merge(train_TR_DC_SAMPLE, train_tr_dc_SVRTO161, on='CUST_NO', how='outer') return train_TR_DC_SAMPLE 第三方存管账户信息（THR_PTY_CSTD） - 处理123456# 提前删掉无用字段train_thr_pty_cstd_temple = train_thr_pty_cstd.drop(['CCY_CD', 'CUST_CTG_CD', 'MTH_ACT_DAYS_TOT'], axis=1)# 划分三张表train_thr_pty_cstd_1 = train_thr_pty_cstd_temple[train_thr_pty_cstd_temple['DATA_DAT']==3728764800].reset_index()train_thr_pty_cstd_2 = train_thr_pty_cstd_temple[train_thr_pty_cstd_temple['DATA_DAT']==3731443200].reset_index()train_thr_pty_cstd_3 = train_thr_pty_cstd_temple[train_thr_pty_cstd_temple['DATA_DAT']==3734035200].reset_index() 12345678# 清理没用字段train_thr_pty_cstd_3 = train_thr_pty_cstd_3.drop(['index', 'DATA_DAT'], axis=1)# 对字段进行处理ARG_BAL = train_thr_pty_cstd_3.groupby(['CUST_NO'])['ARG_BAL'].sum().reset_index()AVL_BAL = train_thr_pty_cstd_3.groupby(['CUST_NO'])['AVL_BAL'].sum().reset_index()MTH_ARG_BAL_ACCM = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_ARG_BAL_ACCM'].sum().reset_index()MTH_FUD_TF_INWD_AMT = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_FUD_TF_INWD_AMT'].sum().reset_index()MTH_FUD_TF_OUT_AMT = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_FUD_TF_OUT_AMT'].sum().reset_index() 12345# 将字段合并train_thr_pty_cstd_SAMPLE = pd.merge(ARG_BAL, AVL_BAL, on='CUST_NO', how='outer')train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_ARG_BAL_ACCM, on='CUST_NO', how='outer')train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_FUD_TF_INWD_AMT, on='CUST_NO', how='outer')train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_FUD_TF_OUT_AMT, on='CUST_NO', how='outer') 12# @ 去除过分关联的特征train_thr_pty_cstd_SAMPLE = train_thr_pty_cstd_SAMPLE.drop(['ARG_BAL', 'MTH_ARG_BAL_ACCM'], axis=1) 12345678910111213141516171819202122232425# 声明函数def THR_PTY_CSTD_PARSE(dataset_THR_PTY_CSTD): # 提前删掉无用字段 dataset_THR_PTY_CSTD = dataset_THR_PTY_CSTD.drop(['CCY_CD', 'CUST_CTG_CD', 'MTH_ACT_DAYS_TOT'], axis=1) # 划分三张表 train_thr_pty_cstd_1 = dataset_THR_PTY_CSTD[dataset_THR_PTY_CSTD['DATA_DAT']==3728764800].reset_index() train_thr_pty_cstd_2 = dataset_THR_PTY_CSTD[dataset_THR_PTY_CSTD['DATA_DAT']==3731443200].reset_index() train_thr_pty_cstd_3 = dataset_THR_PTY_CSTD[dataset_THR_PTY_CSTD['DATA_DAT']==3734035200].reset_index() # 清理没用字段 train_thr_pty_cstd_3 = train_thr_pty_cstd_3.drop(['index', 'DATA_DAT'], axis=1) # 对字段进行处理 ARG_BAL = train_thr_pty_cstd_3.groupby(['CUST_NO'])['ARG_BAL'].sum().reset_index() AVL_BAL = train_thr_pty_cstd_3.groupby(['CUST_NO'])['AVL_BAL'].sum().reset_index() MTH_ARG_BAL_ACCM = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_ARG_BAL_ACCM'].sum().reset_index() MTH_FUD_TF_INWD_AMT = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_FUD_TF_INWD_AMT'].sum().reset_index() MTH_FUD_TF_OUT_AMT = train_thr_pty_cstd_3.groupby(['CUST_NO'])['MTH_FUD_TF_OUT_AMT'].sum().reset_index() # 将字段合并 train_thr_pty_cstd_SAMPLE = pd.merge(AVL_BAL, ARG_BAL, on='CUST_NO', how='outer') train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_ARG_BAL_ACCM, on='CUST_NO', how='outer') train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_FUD_TF_INWD_AMT, on='CUST_NO', how='outer') train_thr_pty_cstd_SAMPLE = pd.merge(train_thr_pty_cstd_SAMPLE, MTH_FUD_TF_OUT_AMT, on='CUST_NO', how='outer') # @ 去除过分关联的特征 train_thr_pty_cstd_SAMPLE = train_thr_pty_cstd_SAMPLE.drop(['ARG_BAL', 'MTH_ARG_BAL_ACCM'], axis=1) return train_thr_pty_cstd_SAMPLE 贷款账户信息（LOAN）- 处理123456789# 选择有用字段train_loan_temple = train_loan[['CUST_NO', 'DATA_DAT', 'ARG_TYP_CD', 'CCY_CD', 'ARG_LIF_CYC_STA_CD', 'LN_STS_CD', 'CHANL_CD', 'LN_TERM', 'RPAY_MOD_CD', 'LAC', 'NON_MATU_CAP', 'ACD_NML_INTS', 'INTS_TOT_AMT' , 'BUS_BREED_CD', 'NML_CAP_BAL', 'MTH_NML_CAP_ACCM']]train_loan_temple = pd.merge(train_loan_temple, train_base_excg, on = 'CCY_CD')# 划分三张表train_loan_1 = train_loan_temple[train_loan_temple['DATA_DAT']==3728764800]train_loan_2 = train_loan_temple[train_loan_temple['DATA_DAT']==3731443200]train_loan_3 = train_loan_temple[train_loan_temple['DATA_DAT']==3734035200]train_loan_3 = train_loan_3.drop(['DATA_DAT'], axis=1) 1train_loan_3.drop_duplicates(subset=None, keep='first', inplace=True) 123456# 执行汇率计算函数train_loan_3['ACD_NML_INTS'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('ACD_NML_INTS', 'RMB_MID_PRIC'))train_loan_3['NON_MATU_CAP'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('NON_MATU_CAP', 'RMB_MID_PRIC'))train_loan_3['NML_CAP_BAL'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('NML_CAP_BAL', 'RMB_MID_PRIC'))train_loan_3['INTS_TOT_AMT'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('INTS_TOT_AMT', 'RMB_MID_PRIC'))train_loan_3['MTH_NML_CAP_ACCM'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('MTH_NML_CAP_ACCM', 'RMB_MID_PRIC')) 123456789101112# 平均应计正常利息MEAN_ACD_NML_INTS = train_loan_3[['CUST_NO', 'ACD_NML_INTS']].groupby(['CUST_NO']).mean().reset_index()# 平均应计未到期本金MEAN_NON_MATU_CAP = train_loan_3[['CUST_NO', 'NON_MATU_CAP']].groupby(['CUST_NO']).mean().reset_index()# 平均月内正常本金基数MEAN_MTH_NML_CAP_ACCM = train_loan_3[['CUST_NO','MTH_NML_CAP_ACCM']].groupby(['CUST_NO']).mean().reset_index()# 平均贷款期限MEAN_LOAN_TERM = train_loan_3[['CUST_NO','LN_TERM']].groupby(['CUST_NO']).mean().reset_index()# 平均贷款金额MEAN_NML_CAP_BAL = train_loan_3[['CUST_NO', 'NML_CAP_BAL']].groupby(['CUST_NO']).mean().reset_index()# 平均利息总额MEAN_INTS_TOT_AMT = train_loan_3[['CUST_NO', 'INTS_TOT_AMT']].groupby(['CUST_NO']).mean().reset_index() 12train_loan_3_SAMPLE = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(MEAN_ACD_NML_INTS, MEAN_NON_MATU_CAP, on='CUST_NO', how='outer'), MEAN_MTH_NML_CAP_ACCM, on='CUST_NO', how='outer'), MEAN_LOAN_TERM, on='CUST_NO', how='outer'), MEAN_NML_CAP_BAL, on='CUST_NO', how='outer'), MEAN_INTS_TOT_AMT, on='CUST_NO', how='outer') 1234567891011121314151617181920212223242526272829303132def BY_MONTH_LOAN_METHOD(dataset_LOAN): # 选择有用字段 train_loan_temple = dataset_LOAN[['CUST_NO', 'DATA_DAT', 'ARG_TYP_CD', 'CCY_CD', 'ARG_LIF_CYC_STA_CD', 'LN_STS_CD', 'CHANL_CD', 'LN_TERM', 'RPAY_MOD_CD', 'LAC', 'NON_MATU_CAP', 'ACD_NML_INTS', 'INTS_TOT_AMT' , 'BUS_BREED_CD', 'NML_CAP_BAL', 'MTH_NML_CAP_ACCM']] train_loan_temple = pd.merge(train_loan_temple, train_base_excg, on = 'CCY_CD') # 划分三张表 train_loan_1 = train_loan_temple[train_loan_temple['DATA_DAT']==3728764800] train_loan_2 = train_loan_temple[train_loan_temple['DATA_DAT']==3731443200] train_loan_3 = train_loan_temple[train_loan_temple['DATA_DAT']==3734035200] train_loan_3 = train_loan_3.drop(['DATA_DAT'], axis=1) train_loan_3.drop_duplicates(subset=None, keep='first', inplace=True) # 执行汇率计算函数 train_loan_3['ACD_NML_INTS'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('ACD_NML_INTS', 'RMB_MID_PRIC')) train_loan_3['NON_MATU_CAP'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('NON_MATU_CAP', 'RMB_MID_PRIC')) train_loan_3['NML_CAP_BAL'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('NML_CAP_BAL', 'RMB_MID_PRIC')) train_loan_3['INTS_TOT_AMT'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('INTS_TOT_AMT', 'RMB_MID_PRIC')) train_loan_3['MTH_NML_CAP_ACCM'] = train_loan_3.apply(compute_REAL_MONEY, axis = 1, args = ('MTH_NML_CAP_ACCM', 'RMB_MID_PRIC')) # 平均应计正常利息 MEAN_ACD_NML_INTS = train_loan_3[['CUST_NO', 'ACD_NML_INTS']].groupby(['CUST_NO']).mean().reset_index() # 平均应计未到期本金 MEAN_NON_MATU_CAP = train_loan_3[['CUST_NO', 'NON_MATU_CAP']].groupby(['CUST_NO']).mean().reset_index() # 平均月内正常本金基数 MEAN_MTH_NML_CAP_ACCM = train_loan_3[['CUST_NO','MTH_NML_CAP_ACCM']].groupby(['CUST_NO']).mean().reset_index() # 平均贷款期限 MEAN_LOAN_TERM = train_loan_3[['CUST_NO','LN_TERM']].groupby(['CUST_NO']).mean().reset_index() # 平均贷款金额 MEAN_NML_CAP_BAL = train_loan_3[['CUST_NO', 'NML_CAP_BAL']].groupby(['CUST_NO']).mean().reset_index() # 平均利息总额 MEAN_INTS_TOT_AMT = train_loan_3[['CUST_NO', 'INTS_TOT_AMT']].groupby(['CUST_NO']).mean().reset_index() train_loan_3_SAMPLE = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(MEAN_ACD_NML_INTS, MEAN_NON_MATU_CAP, on='CUST_NO', how='outer'), MEAN_MTH_NML_CAP_ACCM, on='CUST_NO', how='outer'), MEAN_LOAN_TERM, on='CUST_NO', how='outer'), MEAN_NML_CAP_BAL, on='CUST_NO', how='outer'), MEAN_INTS_TOT_AMT, on='CUST_NO', how='outer') return train_loan_3_SAMPLE 基金账户信息（FUND）- 处理123456789101112131415161718192021222324252627282930313233343536373839def BY_MONTH_FUND_METHOD(dataset_FOUND): # 预处理 train_fund_temple = dataset_FOUND.drop(['CCY_CD', 'ARG_CRT_DAT', 'ARG_LIF_CYC_STA_CD', 'CHANL_CD', 'VLU_DAT', 'DATE_MATU', 'CLS_ACCT_DAT', 'LAST_ACT_CHNG_DAT', 'MTH_ACT_DAYS_TOT'], axis=1) # 按月将金额分组 BY_MONTH_FUND_TABLE = train_fund_temple.groupby(['CUST_NO', 'DATA_DAT'])[['FUD_UNIT_NET_VAL', 'FUD_PROD_TYP_CD', 'RSK_RANK_CD', 'SHR', 'MOTH_BAL_ACCM', 'FUND_BAL', 'FUND_BAL_MOTH_BAL_ACCM']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_FUND(key): train_fund_temple_STD = BY_MONTH_FUND_TABLE[key].std(axis=1).reset_index() train_fund_temple_STD.columns = ['CUST_NO', str('FUND_'+key+'_STD')] train_fund_temple_MEAN = BY_MONTH_FUND_TABLE[key].mean(axis=1).reset_index() train_fund_temple_MEAN.columns = ['CUST_NO',str('FUND_'+key+'_MEAN')] train_fund_temple_MAX = BY_MONTH_FUND_TABLE[key].max(axis=1).reset_index() train_fund_temple_MAX.columns = ['CUST_NO',str('FUND_'+key+'_MAX')] train_fund_temple_MIN = BY_MONTH_FUND_TABLE[key].min(axis=1).reset_index() train_fund_temple_MIN.columns = ['CUST_NO',str('FUND_'+key+'_MIN')] # 合并字段 train_fund_temple = pd.merge(train_fund_temple_STD, train_fund_temple_MEAN, on='CUST_NO', how='inner') train_fund_temple = pd.merge(train_fund_temple, train_fund_temple_MAX, on='CUST_NO', how='inner') train_fund_temple = pd.merge(train_fund_temple, train_fund_temple_MIN, on='CUST_NO', how='inner') return train_fund_temple train_fund_temple = BY_MONTH_FUND('RSK_RANK_CD') for key in ['FUD_UNIT_NET_VAL', 'FUD_PROD_TYP_CD', 'SHR', 'MOTH_BAL_ACCM', 'FUND_BAL', 'FUND_BAL_MOTH_BAL_ACCM']: train_fund_temple = pd.merge(train_fund_temple, BY_MONTH_FUND(key)) train_fund_temple.drop_duplicates(subset=None, keep='first', inplace=True) # 定申定赎 开通标识 SUM_FUND_RATN_APLY_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_APLY_OPN_IND']].groupby(['CUST_NO']).sum().reset_index() STD_FUND_RATN_APLY_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_APLY_OPN_IND']].groupby(['CUST_NO']).std().reset_index() MEAN_FUND_RATN_APLY_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_APLY_OPN_IND']].groupby(['CUST_NO']).mean().reset_index() MAX_FUND_RATN_APLY_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_APLY_OPN_IND']].groupby(['CUST_NO']).max().reset_index() MIN_FUND_RATN_APLY_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_APLY_OPN_IND']].groupby(['CUST_NO']).min().reset_index() SUM_FUND_RATN_REDM_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_REDM_OPN_IND']].groupby(['CUST_NO']).sum().reset_index() STD_FUND_RATN_REDM_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_REDM_OPN_IND']].groupby(['CUST_NO']).std().reset_index() MEAN_FUND_RATN_REDM_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_REDM_OPN_IND']].groupby(['CUST_NO']).mean().reset_index() MAX_FUND_RATN_REDM_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_REDM_OPN_IND']].groupby(['CUST_NO']).max().reset_index() MIN_FUND_RATN_REDM_OPN_IND = dataset_FOUND[['CUST_NO', 'RATN_REDM_OPN_IND']].groupby(['CUST_NO']).min().reset_index() train_fund_temple = pd.merge(pd.merge(train_fund_temple, STD_FUND_RATN_APLY_OPN_IND, on='CUST_NO', how='outer'), MIN_FUND_RATN_REDM_OPN_IND, on='CUST_NO', how='outer') return train_fund_temple 国债账户信息（BOND）- 处理1234567891011121314151617181920212223242526def BY_MONTH_BOND_METHOD(dataset_BOND): # 预处理 train_bond_temple = dataset_BOND.drop(['CCY_CD', 'ARG_CRT_DAT', 'MATU_DAT', 'MATU_DAT', 'PROD_CLS_CD', 'CERT_DAT', 'CLS_ACCT_DAT', 'ARG_LIF_CYC_STA_CD', 'NTNL_DEBT_INTS_TYP_CD', 'MTH_ACT_DAYS_TOT'], axis=1) # 按月将金额分组 BY_MONTH_BOND_TABLE = train_bond_temple.groupby(['CUST_NO', 'DATA_DAT'])[['BOND_TERM_CD', 'ARG_CUR_BAL', 'HOD_SHR', 'UNIT_NET_VAL', 'NVTA_MOTH_ACCM']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_BOND(key): train_bond_temple_STD = BY_MONTH_BOND_TABLE[key].std(axis=1).reset_index() train_bond_temple_STD.columns = ['CUST_NO', str('BOND_'+key+'_STD')] train_bond_temple_MEAN = BY_MONTH_BOND_TABLE[key].mean(axis=1).reset_index() train_bond_temple_MEAN.columns = ['CUST_NO',str('BOND_'+key+'_MEAN')] train_bond_temple_MAX = BY_MONTH_BOND_TABLE[key].max(axis=1).reset_index() train_bond_temple_MAX.columns = ['CUST_NO',str('BOND_'+key+'_MAX')] train_bond_temple_MIN = BY_MONTH_BOND_TABLE[key].min(axis=1).reset_index() train_bond_temple_MIN.columns = ['CUST_NO',str('BOND_'+key+'_MIN')] # 合并字段 train_bond_temple = pd.merge(train_bond_temple_STD, train_bond_temple_MEAN, on='CUST_NO', how='inner') train_bond_temple = pd.merge(train_bond_temple, train_bond_temple_MAX, on='CUST_NO', how='inner') train_bond_temple = pd.merge(train_bond_temple, train_bond_temple_MIN, on='CUST_NO', how='inner') return train_bond_temple train_bond_temple = BY_MONTH_BOND('BOND_TERM_CD') for key in ['ARG_CUR_BAL', 'HOD_SHR', 'UNIT_NET_VAL', 'NVTA_MOTH_ACCM']: train_bond_temple = pd.merge(train_bond_temple, BY_MONTH_BOND(key)) train_bond_temple.drop_duplicates(subset=None, keep='first', inplace=True) return train_bond_temple 贵金属账户信息（PREC_METAL）- 处理12345678910111213141516171819202122232425262728293031def BY_MONTH_PREC_METAL_METHOD(dataset_PREC_METAL): # 预处理 train_prec_metal_temple = dataset_PREC_METAL.drop(['PREC_METAL_BREED_CD', 'ARG_STS_CD', 'SIGD_DAT', 'SIGD_CHANL_CD', 'TEMN_DAT', 'TEMN_CHANL_CD', 'MTH_ACT_DAYS_TOT'], axis=1) # 将train_prec_metal和train_base_excg合并 train_prec_metal_temple = pd.merge(train_prec_metal_temple, train_base_excg, on = 'CCY_CD') # 执行汇率计算函数 train_prec_metal_temple['ARG_BAL'] = train_prec_metal_temple.apply(compute_REAL_MONEY, axis = 1, args = ('ARG_BAL', 'RMB_MID_PRIC')) # 按月将金额分组 BY_MONTH_PREC_METAL_TABLE = train_prec_metal_temple.groupby(['CUST_NO', 'DATA_DAT'])[['BUS_CTG_CD', 'CNT', 'ARG_BAL', 'MTH_ARG_ACCM']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_PREC_METAL(key): train_prec_metal_temple_STD = BY_MONTH_PREC_METAL_TABLE[key].std(axis=1).reset_index() train_prec_metal_temple_STD.columns = ['CUST_NO', str('PREC_METAL_'+key+'_STD')] train_prec_metal_temple_MEAN = BY_MONTH_PREC_METAL_TABLE[key].mean(axis=1).reset_index() train_prec_metal_temple_MEAN.columns = ['CUST_NO',str('PREC_METAL_'+key+'_MEAN')] train_prec_metal_temple_MAX = BY_MONTH_PREC_METAL_TABLE[key].max(axis=1).reset_index() train_prec_metal_temple_MAX.columns = ['CUST_NO',str('PREC_METAL_'+key+'_MAX')] train_prec_metal_temple_MIN = BY_MONTH_PREC_METAL_TABLE[key].min(axis=1).reset_index() train_prec_metal_temple_MIN.columns = ['CUST_NO',str('PREC_METAL_'+key+'_MIN')] # 合并字段 train_prec_metal_temple = pd.merge(train_prec_metal_temple_STD, train_prec_metal_temple_MEAN, on='CUST_NO', how='inner') train_prec_metal_temple = pd.merge(train_prec_metal_temple, train_prec_metal_temple_MAX, on='CUST_NO', how='inner') train_prec_metal_temple = pd.merge(train_prec_metal_temple, train_prec_metal_temple_MIN, on='CUST_NO', how='inner') return train_prec_metal_temple train_prec_metal_temple = BY_MONTH_PREC_METAL('BUS_CTG_CD') for key in ['CNT', 'ARG_BAL', 'MTH_ARG_ACCM']: train_prec_metal_temple = pd.merge(train_prec_metal_temple, BY_MONTH_PREC_METAL(key)) train_prec_metal_temple.drop_duplicates(subset=None, keep='first', inplace=True) return train_prec_metal_temple 代理保险账户信息（AGET_INSR）- 处理1234567891011121314151617181920212223242526def BY_MONTH_AGET_INSR_METHOD(dataset_AGET_INSR): # 预处理 train_aget_insr_temple = dataset_AGET_INSR[['DATA_DAT', 'CUST_NO', 'PREM', 'CVAG', 'INSE_CNT', 'MTH_PREM_ACCM', 'BEG_MTH_PREM_BAL']] # 按月将金额分组 BY_MONTH_AGET_INSR_TABLE = train_aget_insr_temple.groupby(['CUST_NO', 'DATA_DAT'])[['PREM', 'CVAG', 'INSE_CNT', 'MTH_PREM_ACCM', 'BEG_MTH_PREM_BAL']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_AGET_INSR(key): train_aget_insr_temple_STD = BY_MONTH_AGET_INSR_TABLE[key].std(axis=1).reset_index() train_aget_insr_temple_STD.columns = ['CUST_NO', str('AGET_INSR_'+key+'_STD')] train_aget_insr_temple_MEAN = BY_MONTH_AGET_INSR_TABLE[key].mean(axis=1).reset_index() train_aget_insr_temple_MEAN.columns = ['CUST_NO',str('AGET_INSR_'+key+'_MEAN')] train_aget_insr_temple_MAX = BY_MONTH_AGET_INSR_TABLE[key].max(axis=1).reset_index() train_aget_insr_temple_MAX.columns = ['CUST_NO',str('AGET_INSR_'+key+'_MAX')] train_aget_insr_temple_MIN = BY_MONTH_AGET_INSR_TABLE[key].min(axis=1).reset_index() train_aget_insr_temple_MIN.columns = ['CUST_NO',str('AGET_INSR_'+key+'_MIN')] # 合并字段 train_aget_insr_temple = pd.merge(train_aget_insr_temple_STD, train_aget_insr_temple_MEAN, on='CUST_NO', how='inner') train_aget_insr_temple = pd.merge(train_aget_insr_temple, train_aget_insr_temple_MAX, on='CUST_NO', how='inner') train_aget_insr_temple = pd.merge(train_aget_insr_temple, train_aget_insr_temple_MIN, on='CUST_NO', how='inner') return train_aget_insr_temple train_aget_insr_temple = BY_MONTH_AGET_INSR('PREM') for key in ['CVAG', 'INSE_CNT', 'MTH_PREM_ACCM', 'BEG_MTH_PREM_BAL']: train_aget_insr_temple = pd.merge(train_aget_insr_temple, BY_MONTH_AGET_INSR(key)) train_aget_insr_temple.drop_duplicates(subset=None, keep='first', inplace=True) return train_aget_insr_temple 表间融合TRAIN集123456789101112131415# train 集 38256dataset_train = pd.merge(IDV_CUST_BASIC_PARSE(CALC_AGE(train_idv_cust_basic)), IDV_TD_PARSE(train_idv_td), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, IDV_DPSA_PARSE(train_idv_dpsa), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, TR_DC_PARSE(train_tr_dc), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_IDV_TD_METHOD(train_idv_td), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_IDV_DPSA_METHOD(train_idv_dpsa), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_TR_DC_METHOD(train_tr_dc), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, THR_PTY_CSTD_PARSE(train_thr_pty_cstd), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_LOAN_METHOD(train_loan), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_FUND_METHOD(train_fund), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_BOND_METHOD(train_bond), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_PREC_METAL_METHOD(train_prec_metal), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, BY_MONTH_AGET_INSR_METHOD(train_aget_insr), on='CUST_NO', how='outer')dataset_train = pd.merge(dataset_train, AUM_NUMS(train_idv_td, train_bond, train_fund, train_prec_metal, train_aget_insr, train_idv_cust_basic), on='CUST_NO', how='outer')dataset_train = pd.merge(WEALTH(train_cust_result,train_idv_td,train_bond,train_fund,train_prec_metal,train_aget_insr,train_thr_pty_cstd), dataset_train, on='CUST_NO', how='outer') /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right) warnings.warn(msg, UserWarning) /root/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors)12# 将个人客户基本信息表与结果整合dataset_train = pd.merge(dataset_train, train_cust_result, on='CUST_NO') TRAIN 剪枝12# 删除无用字段dataset_train = dataset_train.drop(['CUST_NO'], axis=1) A集123456789101112131415# A 集 4782dataset_A = pd.merge(IDV_CUST_BASIC_PARSE(CALC_AGE(A_idv_cust_basic)), IDV_TD_PARSE(A_idv_td), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, IDV_DPSA_PARSE(A_idv_dpsa), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, TR_DC_PARSE(A_tr_dc), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_IDV_TD_METHOD(A_idv_td), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_IDV_DPSA_METHOD(A_idv_dpsa), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_TR_DC_METHOD(A_tr_dc), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, THR_PTY_CSTD_PARSE(A_thr_pty_cstd), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_LOAN_METHOD(A_loan), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_FUND_METHOD(A_fund), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_BOND_METHOD(A_bond), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_PREC_METAL_METHOD(A_prec_metal), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, BY_MONTH_AGET_INSR_METHOD(A_aget_insr), on='CUST_NO', how='outer')dataset_A = pd.merge(dataset_A, AUM_NUMS(A_idv_td, A_bond, A_fund, A_prec_metal, A_aget_insr, A_idv_cust_basic), on='CUST_NO', how='outer')dataset_A = pd.merge(WEALTH(A_customid,A_idv_td,A_bond,A_fund,A_prec_metal,A_aget_insr,A_thr_pty_cstd), dataset_A, on='CUST_NO', how='outer') /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right) warnings.warn(msg, UserWarning) /root/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors)A 剪枝12# 删除无用字段dataset_A = dataset_A.drop(['CUST_NO'], axis=1) B集123456789101112131415# A 集 4782dataset_B = pd.merge(IDV_CUST_BASIC_PARSE(CALC_AGE(B_idv_cust_basic)), IDV_TD_PARSE(B_idv_td), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, IDV_DPSA_PARSE(B_idv_dpsa), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, TR_DC_PARSE(B_tr_dc), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_IDV_TD_METHOD(B_idv_td), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_IDV_DPSA_METHOD(B_idv_dpsa), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_TR_DC_METHOD(B_tr_dc), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, THR_PTY_CSTD_PARSE(B_thr_pty_cstd), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_LOAN_METHOD(B_loan), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_FUND_METHOD(B_fund), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_BOND_METHOD(B_bond), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_PREC_METAL_METHOD(B_prec_metal), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, BY_MONTH_AGET_INSR_METHOD(B_aget_insr), on='CUST_NO', how='outer')dataset_B = pd.merge(dataset_B, AUM_NUMS(B_idv_td, B_bond, B_fund, B_prec_metal, B_aget_insr, B_idv_cust_basic), on='CUST_NO', how='outer')dataset_B = pd.merge(WEALTH(B_customid,B_idv_td,B_bond,B_fund,B_prec_metal,B_aget_insr,B_thr_pty_cstd), dataset_B, on='CUST_NO', how='outer') /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /root/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right) warnings.warn(msg, UserWarning) /root/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors)B集剪枝12# 删除无用字段dataset_B = dataset_B.drop(['CUST_NO'], axis=1) 欠采样1train_SAMPLE = pd.concat([dataset_train[dataset_train['FLAG']==0].sample(frac=0.20),dataset_train[dataset_train['FLAG']==1]],axis=0).sample(frac=1.0) 算法分析12345678910111213141516171819202122232425262728293031import numpy as np import pandas as pd import xgboost as xgb import time from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_splitfrom imblearn.over_sampling import SMOTEfrom sklearn.model_selection import GridSearchCV# 构建特征 #X = dataset_train.drop(['FLAG'],axis=1).fillna(0)#y = dataset_train['FLAG']X = train_SAMPLE.drop(['FLAG'],axis=1).fillna(0)y = train_SAMPLE['FLAG']# X = dataset_train_SAMPLE.drop(['FLAG'],axis=1)# y = dataset_train_SAMPLE['FLAG'] # 用sklearn.cross_validation进行训练数据集划分X, val_X, y, val_y = train_test_split( X, y, test_size=0.01, #test_size=0.125, random_state=2020, )# SMOTE过采样# smo = SMOTE(random_state=2019)# X_smo, y_smo = smo.fit_sample(X, y)# X_smo = pd.DataFrame(X_smo)# X_smo.columns=['XXX'] 123456789101112131415161718192021222324import numpy as np import pandas as pd import xgboost as xgb import time from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_splitfrom imblearn.over_sampling import SMOTEfrom sklearn.model_selection import GridSearchCV# 构建特征 X = dataset_train.drop(['FLAG'],axis=1).fillna(0)y = dataset_train['FLAG'] # 用sklearn.cross_validation进行训练数据集划分X, val_X, y, val_y = train_test_split( X, y, test_size=0.125, random_state=2019, )train_REAL = pd.concat([X,y],axis=1)train_REAL = pd.concat([train_REAL[train_REAL['FLAG']==0].sample(frac=0.20),train_REAL[train_REAL['FLAG']==1]],axis=0).sample(frac=1.0)X = train_REAL.drop(['FLAG'],axis=1)y = train_REAL['FLAG'] XGB算法12345678910111213141516171819202122232425262728293031323334353637383940414243# xgb矩阵赋值 xgb_val = xgb.DMatrix(val_X, label=val_y) # xgb_train = xgb.DMatrix(X_smo, label=y_smo)xgb_train = xgb.DMatrix(X, label=y) # xgboost模型#params = &#123; 'booster': 'gbtree', # 'objective': 'multi:softmax', # 多分类的问题、 # 'objective': 'multi:softprob', # 多分类概率 'objective': 'binary:logistic', 'eval_metric': 'logloss', # 'num_class': 9, # 类别数，与 multisoftmax 并用 'gamma': 0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。 'max_depth': 5, # 构建树的深度，越大越容易过拟合 'alpha': 0, # L1正则化系数 'lambda': 8, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 'subsample': 1, # 随机采样训练样本 'colsample_bytree': 0.6, # 生成树时进行的列采样 'min_child_weight': 3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 # ，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。 # 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent': 0, # 设置成1则没有运行信息输出，最好是设置为0. 'eta': 0.01 , #'eta': 0.1, # 如同学习率 - result 0.554455 'seed': 1000, 'nthread': 24, # cpu 线程数 'missing': 1, 'scale_pos_weight': (np.sum(y==0)/np.sum(y==1)) # 用来处理正负样本不均衡的问题,通常取：sum(negative cases) / sum(positive cases) # 'eval_metric': 'auc' &#125;plst = list(params.items()) num_rounds = 30000 # 迭代次数 watchlist = [(xgb_train, 'train'), (xgb_val, 'val')] # 交叉验证 result = xgb.cv(plst, xgb_train, num_boost_round=200, nfold=4, early_stopping_rounds=200, verbose_eval=True, folds=StratifiedKFold(n_splits=4).split(X, y)) # 训练模型并保存 # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练 model = xgb.train(plst, xgb_train, num_rounds, watchlist, early_stopping_rounds=400) 12# 模型保存model.save_model('./leezy_xgb.model') # 用于存储训练出的模型 12345678910111213# 读取模型# model = xgb.Booster(model_file='5903-xgb.model')# 本地验证xgb_test = xgb.DMatrix(val_X) preds = model.predict(xgb_test)# 导出结果 threshold = 0.70ans = []for pred in preds: result = 1 if pred &gt; threshold else 0 ans.append(result)pred_result= pd.Series(ans, dtype='int32')evaluate(val_y, pred_result) test_pct_1: 0.1581 pred_pct_1: 0.1836 Precesion: 0.5581 Recall: 0.6481 F1-score: 0.5998 confusion matrix: [[3638 388] [ 266 490]] {&apos;Confusuion&apos;: array([[3638, 388], [ 266, 490]]), &apos;F1&apos;: 0.5997552019583843, &apos;Precision&apos;: 0.5580865603644647, &apos;acc&apos;: 0.863237139272271, &apos;pred_pct_1&apos;: 0.1836051861145964, &apos;recall&apos;: 0.6481481481481481, &apos;roc_auc&apos;: 0.7758872881823701, &apos;test_pct_1&apos;: 0.15809284818067754}1234567891011# A集训练xgb_A = xgb.DMatrix(dataset_A)A_preds = model.predict(xgb_A) # 导出结果 threshold = 0.40ans = []for pred in A_preds: result = 1 if pred &gt; threshold else 0 ans.append(result)pred_A_result= pd.Series(ans, dtype='int32') 1234567891011# B集训练xgb_B = xgb.DMatrix(dataset_B)B_preds = model.predict(xgb_B) # 导出结果 threshold = 0.68ans = []for pred in B_preds: result = 1 if pred &gt; threshold else 0 ans.append(result)pred_B_result= pd.Series(ans, dtype='int32') 1234import operatorimportance = model.get_fscore()importance = sorted(importance.items(), key=operator.itemgetter(1))# print(importance) 1234import matplotlib.pyplot as pltplt.rcParams["figure.figsize"]=(30,40)xgb.plot_importance(model, max_num_features=100)plt.show() /root/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans (prop.get_family(), self.defaultFamily[fontext])) 12# 查看特征排名feature_rank(model, 30) 提交结果1pred_B_result.value_counts() 0 3901 1 881 dtype: int6412# 合并结果result = pd.concat([B_customid, pred_B_result],axis=1) 1result.to_csv('111.csv',header=0,index=0) 1init_woody The prv extension is already loaded. To reload it, use: %reload_ext prv Matplotlib env init complete. Warnings off.1predict 2 000.csv &apos;提交次数已用完，请明日再试！&apos;深化特征工程各类理财产品的持有数量123456train_idv_td = pd.read_csv("../data/2/train/IDV_TD.csv", encoding='utf-8') # 个人定期存款账户信息（IDV_TD）train_bond = pd.read_csv("../data/2/train/BOND.csv", encoding='utf-8') # 国债账户信息（BOND）train_fund = pd.read_csv("../data/2/train/FUND.csv", encoding='utf-8') # 基金账户信息（FUND）train_prec_metal = pd.read_csv("../data/2/train/PREC_METAL.csv", encoding='utf-8') # 贵金属账户信息（PREC_METAL）train_aget_insr = pd.read_csv("../data/2/train/AGET_INSR.csv", encoding='utf-8') # 代理保险账户信息（AGET_INSR）train_idv_cust_basic = pd.read_csv("../data/2/train/IDV_CUST_BASIC.csv", encoding='utf-8') # 个人客户基本信息（IDV_CUST_BASIC） /root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result)12345# 定期的个数train_idv_td_3 = train_idv_td[train_idv_td['DATA_DAT']==3734035200]train_idv_td_AUM = train_idv_td_3[['CUST_NO', 'ARG_CRT_DAT']]train_idv_td_num = train_idv_td_AUM.groupby('CUST_NO').count().reset_index()train_idv_td_num.rename(columns=&#123;'ARG_CRT_DAT':'IDV_TD_NUM'&#125;, inplace=True) 12345# 国债的个数train_bond_3 = train_bond[train_bond['DATA_DAT']==3734035200]train_bond_AUM = train_bond_3[['CUST_NO', 'ARG_CRT_DAT']]train_bond_num = train_bond_AUM.groupby('CUST_NO').count().reset_index()train_bond_num.rename(columns=&#123;'ARG_CRT_DAT':'BOND_NUM'&#125;, inplace=True) 12345# 基金的个数train_fund_3 = train_fund[train_fund['DATA_DAT']==3734035200]train_fund_AUM = train_fund[['CUST_NO', 'ARG_CRT_DAT']]train_fund_num = train_fund_AUM.groupby('CUST_NO').count().reset_index()train_fund_num.rename(columns=&#123;'ARG_CRT_DAT':'FUND_NUM'&#125;, inplace=True) 123456# 贵金属的个数train_prec_metal_3 = train_prec_metal[train_prec_metal['DATA_DAT']==3734035200]train_prec_metal_3.dropna(subset=['SIGD_DAT'], inplace=True)train_prec_metal_AUM = train_prec_metal_3[['CUST_NO', 'SIGD_DAT']]train_prec_metal_num = train_prec_metal_AUM.groupby('CUST_NO').count().reset_index()train_prec_metal_num.rename(columns=&#123;'SIGD_DAT':'PREC_METAL_NUM'&#125;, inplace=True) /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy This is separate from the ipykernel package so we can avoid doing imports until123456# 保险的个数train_aget_insr_3 = train_aget_insr[train_aget_insr['DATA_DAT']==3734035200]train_aget_insr_3.dropna(subset=['INSE_DAT'], inplace=True)train_aget_insr_AUM = train_aget_insr_3[['CUST_NO', 'INSE_DAT']]train_aget_insr_num = train_aget_insr_AUM.groupby('CUST_NO').count().reset_index()train_aget_insr_num.rename(columns=&#123;'INSE_DAT':'AGET_INSR_NUM'&#125;, inplace=True) /root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy This is separate from the ipykernel package so we can avoid doing imports until12# 客户IDCUST_NO_LSIT = train_idv_cust_basic['CUST_NO'].reset_index().drop('index', axis=1) 123# 合并为一个表CUST_NO_AUM_LSIT = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(CUST_NO_LSIT, train_idv_td_num, on='CUST_NO', how='outer'), train_bond_num, on='CUST_NO', how='outer'), train_fund_num, on='CUST_NO', how='outer'), train_prec_metal_num, on='CUST_NO', how='outer'), train_aget_insr_num, on='CUST_NO', how='outer').fillna(0) 1CUST_NO_AUM_LSIT['PRODUCTS_NUM'] = CUST_NO_AUM_LSIT[['IDV_TD_NUM','BOND_NUM','FUND_NUM','PREC_METAL_NUM','AGET_INSR_NUM']].sum(axis=1) 1CUST_NO_AUM_LSIT[CUST_NO_AUM_LSIT['CUST_NO'] == 'a100d07faf0bc3c60d7d65abd704142a'] 1CUST_NO_AUM_LSIT.head(100) 1MyAUMTest = pd.merge(CUST_NO_AUM_LSIT, train_cust_result, on='CUST_NO', how='outer') 1MyAUMTest.head(10) 1CUST_NO_AUM_LSIT.shape (38256, 6)12# 关联关系g = sns.heatmap(MyAUMTest[['FLAG', 'IDV_TD_NUM','BOND_NUM','FUND_NUM','PREC_METAL_NUM','AGET_INSR_NUM','PRODUCTS_NUM']].corr(), annot=True, cmap="coolwarm") /root/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans (prop.get_family(), self.defaultFamily[fontext])) 12345678910111213141516171819202122232425262728293031323334353637# 获取到所有投资种类的和def AUM_NUMS(train_idv_td, train_bond, train_fund, train_prec_metal, train_aget_insr, train_idv_cust_basic): # 定期的个数 train_idv_td_3 = train_idv_td[train_idv_td['DATA_DAT']==3734035200] train_idv_td_AUM = train_idv_td_3[['CUST_NO', 'ARG_CRT_DAT']] train_idv_td_num = train_idv_td_AUM.groupby('CUST_NO').count().reset_index() train_idv_td_num.rename(columns=&#123;'ARG_CRT_DAT':'IDV_TD_NUM'&#125;, inplace=True) # 国债的个数 train_bond_3 = train_bond[train_bond['DATA_DAT']==3734035200] train_bond_AUM = train_bond_3[['CUST_NO', 'ARG_CRT_DAT']] train_bond_num = train_bond_AUM.groupby('CUST_NO').count().reset_index() train_bond_num.rename(columns=&#123;'ARG_CRT_DAT':'BOND_NUM'&#125;, inplace=True) # 基金的个数 train_fund_3 = train_fund[train_fund['DATA_DAT']==3734035200] train_fund_AUM = train_fund[['CUST_NO', 'ARG_CRT_DAT']] train_fund_num = train_fund_AUM.groupby('CUST_NO').count().reset_index() train_fund_num.rename(columns=&#123;'ARG_CRT_DAT':'FUND_NUM'&#125;, inplace=True) # 贵金属的个数 train_prec_metal_3 = train_prec_metal[train_prec_metal['DATA_DAT']==3734035200] train_prec_metal_3.dropna(subset=['SIGD_DAT'], inplace=True) train_prec_metal_AUM = train_prec_metal_3[['CUST_NO', 'SIGD_DAT']] train_prec_metal_num = train_prec_metal_AUM.groupby('CUST_NO').count().reset_index() train_prec_metal_num.rename(columns=&#123;'SIGD_DAT':'PREC_METAL_NUM'&#125;, inplace=True) # 保险的个数 train_aget_insr_3 = train_aget_insr[train_aget_insr['DATA_DAT']==3734035200] train_aget_insr_3.dropna(subset=['INSE_DAT'], inplace=True) train_aget_insr_AUM = train_aget_insr_3[['CUST_NO', 'INSE_DAT']] train_aget_insr_num = train_aget_insr_AUM.groupby('CUST_NO').count().reset_index() train_aget_insr_num.rename(columns=&#123;'INSE_DAT':'AGET_INSR_NUM'&#125;, inplace=True) # 客户ID CUST_NO_LSIT = train_idv_cust_basic['CUST_NO'].reset_index().drop('index', axis=1) # 合并为一个表 CUST_NO_AUM_LSIT = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(CUST_NO_LSIT, train_idv_td_num, on='CUST_NO', how='outer'), train_bond_num, on='CUST_NO', how='outer'), train_fund_num, on='CUST_NO', how='outer'), train_prec_metal_num, on='CUST_NO', how='outer'), train_aget_insr_num, on='CUST_NO', how='outer').fillna(0) CUST_NO_AUM_LSIT['PRODUCTS_NUM'] = CUST_NO_AUM_LSIT[['IDV_TD_NUM','BOND_NUM','FUND_NUM','PREC_METAL_NUM','AGET_INSR_NUM']].sum(axis=1) return CUST_NO_AUM_LSIT 三个月变动定期账户12345678910111213141516171819202122232425262728293031323334353637def BY_MONTH_IDV_TD_METHOD(dataset_IDV_TD): # 预处理 # 简化IDV_TD表 train_idv_td_temple = dataset_IDV_TD.drop(['ARG_CRT_DAT', 'CLS_ACCT_DAT', 'MATU_DAT', 'LAC', 'ACCT_STS_CD','DP_DAY_CD', 'RDEP_IND_CD', 'RDEP_DP_DAY_CD', 'RAT_CTG','FXDI_SA_ACCM', 'MTH_ACT_DAYS_TOT'], axis=1) # 将train_idv_td和train_base_excg合并 train_idv_td_temple = pd.merge(train_idv_td_temple, train_base_excg, on = 'CCY_CD') # 执行汇率计算函数 train_idv_td_temple['CRBAL'] = train_idv_td_temple.apply(compute_REAL_MONEY, axis = 1, args = ('CRBAL', 'RMB_MID_PRIC')) train_idv_td_temple['REG_CAP'] = train_idv_td_temple.apply(compute_REAL_MONEY, axis = 1, args = ('REG_CAP', 'RMB_MID_PRIC')) train_idv_td_temple['FXDI_T_ACCM'] = train_idv_td_temple.apply(compute_REAL_MONEY, axis = 1, args = ('FXDI_T_ACCM', 'RMB_MID_PRIC')) train_idv_td_temple['TDOP_SHD_PAY_INTS'] = train_idv_td_temple.apply(compute_REAL_MONEY, axis = 1, args = ('TDOP_SHD_PAY_INTS', 'RMB_MID_PRIC')) train_idv_td_temple['MOTH_CR_ACCM'] = train_idv_td_temple.apply(compute_REAL_MONEY, axis = 1, args = ('MOTH_CR_ACCM', 'RMB_MID_PRIC')) train_idv_td_temple = train_idv_td_temple.drop(['CCY_CD', 'RMB_MID_PRIC'], axis=1) # 按月将金额分类 BY_MONTH_TD = train_idv_td_temple.groupby(['CUST_NO', 'DATA_DAT'])[['CRBAL', 'REG_CAP', 'FXDI_T_ACCM', 'TDOP_SHD_PAY_INTS', 'MOTH_CR_ACCM']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_IDV_TD(key): train_idv_td_temple_STD = BY_MONTH_TD[key].std(axis=1).reset_index() train_idv_td_temple_STD.columns = ['CUST_NO', str('IDV_TD_'+key+'_STD')] train_idv_td_temple_MEAN = BY_MONTH_TD[key].mean(axis=1).reset_index() train_idv_td_temple_MEAN.columns = ['CUST_NO',str('IDV_TD_'+key+'_MEAN')] train_idv_td_temple_MAX = BY_MONTH_TD[key].max(axis=1).reset_index() train_idv_td_temple_MAX.columns = ['CUST_NO',str('IDV_TD_'+key+'_MAX')] train_idv_td_temple_MIN = BY_MONTH_TD[key].min(axis=1).reset_index() train_idv_td_temple_MIN.columns = ['CUST_NO',str('IDV_TD_'+key+'_MIN')] # 合并字段 train_idv_td_temple = pd.merge(train_idv_td_temple_STD, train_idv_td_temple_MEAN, on='CUST_NO', how='inner') train_idv_td_temple = pd.merge(train_idv_td_temple, train_idv_td_temple_MAX, on='CUST_NO', how='inner') train_idv_td_temple = pd.merge(train_idv_td_temple, train_idv_td_temple_MIN, on='CUST_NO', how='inner') train_idv_td_temple = train_idv_td_temple.fillna(0) return train_idv_td_temple train_idv_td_temple = BY_MONTH_IDV_TD('CRBAL') for key in ['REG_CAP', 'FXDI_T_ACCM','TDOP_SHD_PAY_INTS', 'MOTH_CR_ACCM']: train_idv_td_temple = pd.merge(train_idv_td_temple, BY_MONTH_IDV_TD(key)) train_idv_td_temple.drop_duplicates(subset=None, keep='first', inplace=True) return train_idv_td_temple 活期账户12345678910111213141516171819202122232425262728293031323334353637383940414243def BY_MONTH_IDV_DPSA_METHOD(dataset_IDV_DPSA): # 预处理 # 将IDV_DPSA表与汇率表整合 train_idv_dpsa_temple = pd.merge(dataset_IDV_DPSA, train_base_excg, on = 'CCY_CD', how='inner') train_idv_dpsa_temple.drop_duplicates(subset=None, keep='first', inplace=True) # 执行汇率计算函数 train_idv_dpsa_temple['CRBAL'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis=1, args = ('CRBAL', 'RMB_MID_PRIC')) train_idv_dpsa_temple['ITST_BRNG_ACCM'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis=1, args = ('ITST_BRNG_ACCM', 'RMB_MID_PRIC')) train_idv_dpsa_temple['FRZ_TOT_AMT'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('FRZ_TOT_AMT', 'RMB_MID_PRIC')) train_idv_dpsa_temple['DAY_WD_ACT_AMT'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_WD_ACT_AMT', 'RMB_MID_PRIC')) train_idv_dpsa_temple['DAY_CSH_DP_SUM'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_CSH_DP_SUM', 'RMB_MID_PRIC')) train_idv_dpsa_temple['DAY_CSH_WD_SUM'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_CSH_WD_SUM', 'RMB_MID_PRIC')) train_idv_dpsa_temple['DAY_TFI_SUM'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('DAY_TFI_SUM', 'RMB_MID_PRIC')) train_idv_dpsa_temple['MOTH_CR_ACCM'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('MOTH_CR_ACCM', 'RMB_MID_PRIC')) train_idv_dpsa_temple['BEG_MOTH_CRBAL'] = train_idv_dpsa_temple.apply(compute_REAL_MONEY, axis = 1, args = ('BEG_MOTH_CRBAL', 'RMB_MID_PRIC')) train_idv_dpsa_temple = train_idv_dpsa_temple.drop(['CCY_CD', 'RMB_MID_PRIC'], axis=1) # 按月将金额分组 BY_MONTH_DPSA = train_idv_dpsa_temple.groupby(['CUST_NO', 'DATA_DAT'])[['CRBAL', 'ITST_BRNG_ACCM', 'FRZ_TOT_AMT', 'DAY_WD_ACT_AMT', 'DAY_CSH_DP_SUM', 'DAY_CSH_WD_SUM', 'DAY_TFI_SUM', 'MOTH_CR_ACCM', 'BEG_MOTH_CRBAL']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_IDV_DPSA(key): train_idv_dpsa_temple_STD = BY_MONTH_DPSA[key].std(axis=1).reset_index() train_idv_dpsa_temple_STD.columns = ['CUST_NO', str('IDV_DPSA_'+key+'_STD')] train_idv_dpsa_temple_MEAN = BY_MONTH_DPSA[key].mean(axis=1).reset_index() train_idv_dpsa_temple_MEAN.columns = ['CUST_NO',str('IDV_DPSA_'+key+'_MEAN')] train_idv_dpsa_temple_MAX = BY_MONTH_DPSA[key].max(axis=1).reset_index() train_idv_dpsa_temple_MAX.columns = ['CUST_NO',str('IDV_DPSA_'+key+'_MAX')] train_idv_dpsa_temple_MIN = BY_MONTH_DPSA[key].min(axis=1).reset_index() train_idv_dpsa_temple_MIN.columns = ['CUST_NO',str('IDV_DPSA_'+key+'_MIN')] # 合并字段 train_idv_dpsa_temple = pd.merge(train_idv_dpsa_temple_STD, train_idv_dpsa_temple_MEAN, on='CUST_NO', how='inner') train_idv_dpsa_temple = pd.merge(train_idv_dpsa_temple, train_idv_dpsa_temple_MAX, on='CUST_NO', how='inner') train_idv_dpsa_temple = pd.merge(train_idv_dpsa_temple, train_idv_dpsa_temple_MIN, on='CUST_NO', how='inner') train_idv_dpsa_temple = train_idv_dpsa_temple.fillna(0) return train_idv_dpsa_temple train_idv_dpsa_temple = BY_MONTH_IDV_DPSA('CRBAL') for key in ['ITST_BRNG_ACCM','FRZ_TOT_AMT', 'DAY_WD_ACT_AMT', 'DAY_CSH_DP_SUM', 'DAY_CSH_WD_SUM','DAY_TFI_SUM', 'MOTH_CR_ACCM', 'BEG_MOTH_CRBAL']: train_idv_dpsa_temple = pd.merge(train_idv_dpsa_temple, BY_MONTH_IDV_DPSA(key)) train_idv_dpsa_temple.drop_duplicates(subset=None, keep='first', inplace=True) return train_idv_dpsa_temple 交易信息123456789101112131415161718192021222324252627282930313233343536def BY_MONTH_TR_DC_METHOD(dataset_TR_DC): # 预处理 # 去除抹帐(1),无意义列，这里全是0 dataset_TR_DC = dataset_TR_DC[dataset_TR_DC['CAN_IND']==0].drop(['RED_BLU_CD', 'CRD_TYP1', 'TR_TYPE', 'TR_CHANL_CD', 'CAN_IND', 'CARD_USETYPE', 'CARD_ELECASH', 'CARD_MATERIAL'],axis=1) dataset_TR_DC.drop_duplicates(subset=None, keep='first', inplace=True) # 交易表时间段打上标签 # 2月份 [3726432000, 3728937600), 3月份 [3728937600, 3731616000), 4月份 [3731616000, 3734035200] dataset_TR_DC['TR_DAT'] = pd.cut(dataset_TR_DC['TR_DAT'], [3726432000, 3728937600, 3731616000, 3734035200], labels=['2月份', '3月份', '4月份']) # 将交易按照 正 / 负 分开 train_tr_dc['INCOME'] = train_tr_dc['TR_AMT'] &gt; 0 myseries = train_tr_dc.groupby(['CUST_NO', 'INCOME'])['TR_AMT'].sum() myseries = myseries.unstack().fillna(0) TR_DC_IN_OUT = pd.DataFrame(myseries).reset_index() TR_DC_IN_OUT['IN_SUM'] = TR_DC_IN_OUT[True] TR_DC_IN_OUT['OUT_SUM'] = TR_DC_IN_OUT[False] TR_DC_IN_OUT = TR_DC_IN_OUT.drop([True, False], axis=1) BY_MONTH_DC = dataset_TR_DC.groupby(['CUST_NO', 'TR_DAT'])[['TR_AMT']].sum().unstack() # 声明函数 计算三个月的标准差、均值、最大值、最小值 def BY_MONTH_TR_DC(key): train_tr_dc_temple_STD = BY_MONTH_DC[key].std(axis=1).reset_index() train_tr_dc_temple_STD.columns = ['CUST_NO', str('TR_DC_'+key+'_STD')] train_tr_dc_temple_MEAN = BY_MONTH_DC[key].mean(axis=1).reset_index() train_tr_dc_temple_MEAN.columns = ['CUST_NO',str('TR_DC_'+key+'_MEAN')] train_tr_dc_temple_MAX = BY_MONTH_DC[key].max(axis=1).reset_index() train_tr_dc_temple_MAX.columns = ['CUST_NO',str('TR_DC_'+key+'_MAX')] train_tr_dc_temple_MIN = BY_MONTH_DC[key].min(axis=1).reset_index() train_tr_dc_temple_MIN.columns = ['CUST_NO',str('TR_DC_'+key+'_MIN')] # 合并字段 train_tr_dc_temple = pd.merge(train_tr_dc_temple_STD, train_tr_dc_temple_MEAN, on='CUST_NO', how='inner') train_tr_dc_temple = pd.merge(train_tr_dc_temple, train_tr_dc_temple_MAX, on='CUST_NO', how='inner') train_tr_dc_temple = pd.merge(train_tr_dc_temple, train_tr_dc_temple_MIN, on='CUST_NO', how='inner') train_tr_dc_temple = train_tr_dc_temple.fillna(0) return train_tr_dc_temple train_tr_dc_temple = BY_MONTH_TR_DC('TR_AMT') return train_tr_dc_temple 1train_tr_dc['TR_DAT'] = pd.cut(train_tr_dc['TR_DAT'], [3726432000, 3728937600, 3731616000, 3734035200], labels=['FEB', 'MAR', 'APR']) 1BY_MONTH_TR_DC_METHOD = train_tr_dc[['TR_DAT']] 1sns.countplot(x='TR_DAT', data=BY_MONTH_TR_DC_METHOD) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0167e23a58&gt; 年龄123456789101112# 返回单个 或 series 的ms对应日期, 再被 2015 减得到的年数def adj_date(series, datatype='series'): import time from datetime import datetime initial_year = 2015 adj_msj_obj = datetime.strptime("2042-08-31 16:00:00.123", "%Y-%m-%d %H:%M:%S.%f") adj_ms = int(time.mktime(adj_msj_obj.timetuple())*1000 + adj_msj_obj.microsecond/1000.0)/1000 current_ms = train_idv_cust_basic['DATA_DAT'][0] # 3736713600 if(datatype=='single'): return initial_year - int(time.strftime("%Y%m", time.localtime(current_ms-adj_ms))) else: return series.map(lambda x: initial_year - int(time.strftime("%Y", time.localtime(x - adj_ms))) if pd.notna(x) else x) 1234567# 年龄计算函数def CALC_AGE(dataset_idv_cust_basic): dataset_idv_cust_basic['GC_BRTH'] = dataset_idv_cust_basic['GC_BRTH'].fillna(dataset_idv_cust_basic['GC_BRTH'].mean()) dataset_idv_cust_basic['AGE'] = adj_date(dataset_idv_cust_basic['GC_BRTH']) dataset_idv_cust_basic['AGE'] = np.where(dataset_idv_cust_basic['AGE'] &gt; 100, 31, dataset_idv_cust_basic['AGE']) dataset_idv_cust_basic['AGE'] = np.where(dataset_idv_cust_basic['AGE'] &lt; 16, 31, dataset_idv_cust_basic['AGE']) return dataset_idv_cust_basic 1AGE = CALC_AGE(train_idv_cust_basic)['AGE'] 1AGE.hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f01681b5cc0&gt; /root/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans (prop.get_family(), self.defaultFamily[fontext])) 所有资产123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# td定期、bond国债、fund基金、prec_metal贵金属、aget_insr保险def WEALTH(target,td,bond,fund,prec_metal,agent_insurance,thr_pty_cstd): ''' # 销户日期必须 &gt; 2月底，合约建立日期必须 &lt; 4月底 #gp = df_tr_train.groupby(['CUST_NO','TR_CD']).agg(&#123;'TR_AMT':['sum','mean','max','min','count','std']&#125;) if len(suffix)==1: td = td[td['DATA_DAT']==time_point[1]] bond = bond[bond['DATA_DAT']==time_point[1]] fund = fund[fund['DATA_DAT']==time_point[1]] prec_metal = prec_metal[prec_metal['DATA_DAT']==time_point[1]] agent_insurance = agent_insurance[agent_insurance['DATA_DAT']==time_point[1]] thr_pty_cstd = thr_pty_cstd[thr_pty_cstd['DATA_DAT']==time_point[1]] elif len(suffix)==2: td = td[td['DATA_DAT']&gt;time_point[1]] bond = bond[bond['DATA_DAT']&gt;time_point[1]] fund = fund[fund['DATA_DAT']&gt;time_point[1]] prec_metal = prec_metal[prec_metal['DATA_DAT']&gt;time_point[1]] agent_insurance = agent_insurance[agent_insurance['DATA_DAT']&gt;time_point[1]] thr_pty_cstd = thr_pty_cstd[thr_pty_cstd['DATA_DAT']&gt;time_point[1]] ''' # 选取各表合并特征 td = td[['CUST_NO','DATA_DAT','CRBAL','MOTH_CR_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() bond = bond[['CUST_NO','DATA_DAT','ARG_CUR_BAL','NVTA_MOTH_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() fund = fund[['CUST_NO','DATA_DAT','FUND_BAL','FUND_BAL_MOTH_BAL_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() prec_metal = prec_metal[['CUST_NO','DATA_DAT','ARG_BAL','MTH_ARG_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() agent_insurance = agent_insurance[['CUST_NO','DATA_DAT','BEG_MTH_PREM_BAL','MTH_PREM_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() thr_pty_cstd = thr_pty_cstd[['CUST_NO','DATA_DAT','AVL_BAL','MTH_ARG_BAL_ACCM']].groupby(['CUST_NO','DATA_DAT']).sum().unstack().reset_index() # 理财各月的金额、持有产品的数量、 wealth = pd.merge(target[['CUST_NO']],td,on=['CUST_NO'],how='outer') wealth = pd.merge(wealth,bond,on=['CUST_NO'],how='outer') wealth = pd.merge(wealth,fund,on=['CUST_NO'],how='outer') wealth = pd.merge(wealth,prec_metal,on=['CUST_NO'],how='outer') wealth = pd.merge(wealth,thr_pty_cstd,on=['CUST_NO'],how='outer') wealth = pd.merge(wealth,agent_insurance,on=['CUST_NO'],how='outer').fillna(0) # 理财各月的金额、持有产品的数量、 #wealth['WEALTH_BAL'] = wealth['CRBAL'] + wealth['ARG_CUR_BAL'] + wealth['FUND_BAL'] + wealth['ARG_BAL'] + wealth['BEG_MTH_PREM_BAL'] #+ wealth['AVL_BAL'] #wealth['WEALTH_ACCM'] = wealth['MOTH_CR_ACCM'] + wealth['NVTA_MOTH_ACCM'] + wealth['FUND_BAL_MOTH_BAL_ACCM'] + wealth['MTH_ARG_ACCM'] + wealth['MTH_PREM_ACCM'] #+ wealth['MTH_ARG_BAL_ACCM'] #wealth = wealth.groupby(['CUST_NO'])['WEALTH_BAL','WEALTH_ACCM'].sum().reset_index() # 持有的理财产品种类（不同期限/合约建立日期、相同品种的算不同产品） # 首份理财合约建立时间、最近一份理财合约建立时间 # 3个月内买入理财产品次数 # 3个月存款积数之和（活期，活期+定期+三方存管） X = wealth.copy() X['WEALTH_BAL_1'] = X[('CRBAL', 3728764800)]+X[('ARG_CUR_BAL', 3728764800)]+X[('FUND_BAL', 3728764800)]+X[('ARG_BAL', 3728764800)]+X[('AVL_BAL', 3728764800)]+X[('BEG_MTH_PREM_BAL', 3728764800)] X['WEALTH_BAL_2'] = X[('CRBAL', 3731443200)]+X[('ARG_CUR_BAL', 3731443200)]+X[('FUND_BAL', 3731443200)]+X[('ARG_BAL', 3731443200)]+X[('AVL_BAL', 3731443200)]+X[('BEG_MTH_PREM_BAL', 3731443200)] X['WEALTH_BAL_3'] = X[('CRBAL', 3734035200)]+X[('ARG_CUR_BAL', 3734035200)]+X[('FUND_BAL', 3734035200)]+X[('ARG_BAL', 3734035200)]+X[('AVL_BAL', 3734035200)]+X[('BEG_MTH_PREM_BAL', 3734035200)] X['WEALTH_ACCM_1'] = X[('MOTH_CR_ACCM', 3728764800)]+X[('NVTA_MOTH_ACCM', 3728764800)]+X[('FUND_BAL_MOTH_BAL_ACCM', 3728764800)]+X[('MTH_ARG_ACCM', 3728764800)]+X[('MTH_ARG_BAL_ACCM', 3728764800)]+X[('MTH_PREM_ACCM', 3728764800)] X['WEALTH_ACCM_2'] = X[('MOTH_CR_ACCM', 3731443200)]+X[('NVTA_MOTH_ACCM', 3731443200)]+X[('FUND_BAL_MOTH_BAL_ACCM', 3731443200)]+X[('MTH_ARG_ACCM', 3731443200)]+X[('MTH_ARG_BAL_ACCM', 3731443200)]+X[('MTH_PREM_ACCM', 3731443200)] X['WEALTH_ACCM_3'] = X[('MOTH_CR_ACCM', 3734035200)]+X[('NVTA_MOTH_ACCM', 3734035200)]+X[('FUND_BAL_MOTH_BAL_ACCM', 3734035200)]+X[('MTH_ARG_ACCM', 3734035200)]+X[('MTH_ARG_BAL_ACCM', 3734035200)]+X[('MTH_PREM_ACCM', 3734035200)] X2 = X[['CUST_NO','WEALTH_BAL_1','WEALTH_BAL_2','WEALTH_BAL_3','WEALTH_ACCM_1','WEALTH_ACCM_2','WEALTH_ACCM_3']].set_index('CUST_NO') X2['WEALTH_BAL_MAX'] = X2[['WEALTH_BAL_1','WEALTH_BAL_2','WEALTH_BAL_3']].max(axis=1) X2['WEALTH_BAL_MEAN'] = X2[['WEALTH_BAL_1','WEALTH_BAL_2','WEALTH_BAL_3']].mean(axis=1) X2['WEALTH_BAL_STD'] = X2[['WEALTH_BAL_1','WEALTH_BAL_2','WEALTH_BAL_3']].std(axis=1).fillna(0.0) X2['WEALTH_BAL_CV'] = X2['WEALTH_BAL_STD']/X2['WEALTH_BAL_MEAN'] X2['WEALTH_ACCM_MAX'] = X2[['WEALTH_ACCM_1','WEALTH_ACCM_2','WEALTH_ACCM_3']].max(axis=1) X2['WEALTH_ACCM_MEAN'] = X2[['WEALTH_ACCM_1','WEALTH_ACCM_2','WEALTH_ACCM_3']].mean(axis=1) X2['WEALTH_ACCM_STD'] = X2[['WEALTH_ACCM_1','WEALTH_ACCM_2','WEALTH_ACCM_3']].std(axis=1).fillna(0.0) X2['WEALTH_ACCM_CV'] = X2['WEALTH_ACCM_STD']/X2['WEALTH_ACCM_MEAN'] X3 = X2.fillna(0.0).drop(['WEALTH_BAL_1','WEALTH_BAL_2','WEALTH_BAL_3','WEALTH_ACCM_1','WEALTH_ACCM_2','WEALTH_ACCM_3','WEALTH_BAL_STD','WEALTH_ACCM_STD'],axis=1).reset_index() return X3[['CUST_NO','WEALTH_ACCM_MEAN','WEALTH_ACCM_CV']] 结果挖掘12train_cust_result_1 = train_cust_result[train_cust_result['FLAG']==1]train_cust_result_0 = train_cust_result[train_cust_result['FLAG']==0] 1from lightgbm import LGBMRegressor 1! pip install lightgbm-2.2.3-py2.py3-none-manylinux1_x86_64.whl Processing ./lightgbm-2.2.3-py2.py3-none-manylinux1_x86_64.whl Requirement already satisfied: scikit-learn in /root/anaconda3/lib/python3.6/site-packages (from lightgbm==2.2.3) Requirement already satisfied: scipy in /root/anaconda3/lib/python3.6/site-packages (from lightgbm==2.2.3) Requirement already satisfied: numpy in /root/anaconda3/lib/python3.6/site-packages (from lightgbm==2.2.3) Installing collected packages: lightgbm Successfully installed lightgbm-2.2.31! cp ./xgb_rfm-Copy1.ipynb ./xgb_rfm-Copy2.ipynb 1! pip install imbalanced-learn Requirement already satisfied: imbalanced-learn in /root/anaconda3/lib/python3.6/site-packages Requirement already satisfied: scipy&gt;=0.17 in /root/anaconda3/lib/python3.6/site-packages (from imbalanced-learn) Requirement already satisfied: joblib&gt;=0.11 in /root/anaconda3/lib/python3.6/site-packages (from imbalanced-learn) Requirement already satisfied: scikit-learn&gt;=0.21 in /root/anaconda3/lib/python3.6/site-packages (from imbalanced-learn) Requirement already satisfied: numpy&gt;=1.11 in /root/anaconda3/lib/python3.6/site-packages (from imbalanced-learn)]]></content>
      <tags>
        <tag>Python</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装MySQL 8.0]]></title>
    <url>%2F2019%2F02%2F27%2FCentOS%E5%AE%89%E8%A3%85MySQL8%2F</url>
    <content type="text"><![CDATA[CentOS MySQL的安装和配置记录。 安装MySQL 打开官网 https://dev.mysql.com/downloads/repo/yum/ 下载(mysql80-community-release-el7-2.noarch.rpm)rpm文件 通过 WinSCP将文件上传至 /usr/ 目录下 本地安装 1yum install mysql80-community-release-el7-2.noarch.rpm 查看MySQL 的所有版本 1yum repolist all | grep mysql 默认会下载最新的版本，可以通过 12sudo yum-config-manager --disable mysql80-communitysudo yum-config-manager --enable mysql57-community 来调整默认版本， 也可以通过修改 /etc/yum.repos.d/mysql-community.repo 中enabled的值(1 或 0)来进行修改。6. 安装MySQL 1sudo yum install mysql-community-server 启动MySQL 1sudo service mysqld start 用临时密码登录 123456 sudo grep 'temporary password' /var/log/mysqld.log mysql -uroot -p # 修改密码 ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';# 查看MySQL版本 SHOW VARIABLES WHERE Variable_name = 'version'; 允许远程连接MySQL数据库 123456# 首先确保3306端口开放user mysql# 创建用户CREATE USER 'myuser'@'%' IDENTIFIED BY 'password'; (myuser 和 password 替换)GRANT ALL ON *.* TO 'myuser'@'%' WITH GRANT OPTION; (myuser 替换)FLUSH PRIVILEGES;]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro 安全框架学习笔记]]></title>
    <url>%2F2019%2F02%2F26%2FShiro%E5%AE%89%E5%85%A8%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[记录我学习安全框架Shiro的一些笔记。 Shiro VS Spring SecurityApache Shiro简单灵活，可脱离Spring，粒度较粗；可以自己拓展，适合通过资源进行权限控制；Spring Security复杂笨重，不可脱离Spring，力度更细；适合做数据权限控制； Shiro的认证过程 创建Security Manager 主体提交认证 Security Manager认证 Authenticator认证 Realm认证 代码如下：java代码 1234567891011121314151617181920212223242526272829303132333435363738394041package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.realm.SimpleAccountRealm;import org.apache.shiro.subject.Subject;import org.junit.Before;import org.junit.Test;public class AuthenticationTest &#123; SimpleAccountRealm simpleAccountRealm = new SimpleAccountRealm(); @Before public void addUser() &#123; simpleAccountRealm.addAccount("LEEZY", "123456"); &#125; @Test public void testAuthentication() &#123; // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); // 将SimpleAccountRealm设置到环境变量中来 defaultSecurityManager.setRealm(simpleAccountRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("LEEZY", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.isAuthenticated(); subject.logout(); System.out.println("isAuthenticated: " + subject.isAuthenticated()); &#125;&#125; pom文件 1234567891011121314151617181920212223242526272829&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leezy.shiro&lt;/groupId&gt; &lt;artifactId&gt;shirotest&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;shirotest&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;/.&lt;/project&gt; Shiro授权 创建 Security Manager 主体授权 Security Manager 授权 Authorizer认证 Realm 获取角色权限数据 123456789101112131415161718192021222324252627282930313233343536373839404142package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.realm.SimpleAccountRealm;import org.apache.shiro.subject.Subject;import org.junit.Before;import org.junit.Test;public class AuthenticationTest &#123; SimpleAccountRealm simpleAccountRealm = new SimpleAccountRealm(); @Before public void addUser() &#123; simpleAccountRealm.addAccount("LEEZY", "123456", "admin", "user"); &#125; @Test public void testAuthentication() &#123; // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); // 将SimpleAccountRealm设置到环境变量中来 defaultSecurityManager.setRealm(simpleAccountRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("LEEZY", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.isAuthenticated(); // 检查一个角色 // subject.checkRole("admin"); // 检查是否具备参数里所有的角色，通过遍历参数来进行授权 subject.checkRoles("admin", "user"); &#125;&#125; InitRealm的使用1234567891011121314151617181920212223242526272829303132333435package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.realm.text.IniRealm;import org.apache.shiro.subject.Subject;import org.junit.Test;public class InitRealmTest &#123; @Test public void testAuthentication() &#123; IniRealm iniRealm = new IniRealm("classpath:user.ini"); // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(iniRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("LEEZY", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.checkRole("admin"); subject.checkPermission("user:delete"); subject.checkPermission("user:update"); &#125;&#125; user.ini 1234[users]LEEZY=123456,admin[roles]admin=user:delete,user:update JDBCRealm的使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.realm.jdbc.JdbcRealm;import org.apache.shiro.subject.Subject;import org.junit.Test;import com.alibaba.druid.pool.DruidDataSource;public class JdbcRealmTest &#123; DruidDataSource dataSource = new DruidDataSource(); &#123; dataSource.setUrl("jdbc:mysql://192.168.56.101:3306/test"); dataSource.setUsername("root"); dataSource.setPassword("Zdh!123456"); &#125; @Test public void testAuthentication() &#123; JdbcRealm jdbcRealm = new JdbcRealm(); jdbcRealm.setDataSource(dataSource); // 开启后才可以进行权限认证(默认关闭) jdbcRealm.setPermissionsLookupEnabled(true); String select_sql = "select password from test_user where username = ?"; jdbcRealm.setAuthenticationQuery(select_sql); String role_sql = "select role_name from test_user_role where user_name = ?"; jdbcRealm.setUserRolesQuery(role_sql); // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(jdbcRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("SAKURA", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.checkRole("admin"); subject.checkPermission("user:select"); &#125;&#125; pom.xml 123456789101112&lt;!-- MySQL驱动包 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 数据源 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.14&lt;/version&gt;&lt;/dependency&gt; 查看JdbcRealm源码可以发现，它有默认的SQL查询语句，只要建立相应的数据表就可以。 自定义Realm自定义Realm继承AuthorizingRealm 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.leezy.shiro.realm;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import org.apache.shiro.authc.AuthenticationException;import org.apache.shiro.authc.AuthenticationInfo;import org.apache.shiro.authc.AuthenticationToken;import org.apache.shiro.authc.SimpleAuthenticationInfo;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.subject.PrincipalCollection;public class CustomRealm extends AuthorizingRealm&#123; Map&lt;String, String&gt; userMap = new HashMap&lt;String, String&gt;(16); &#123; userMap.put("LEEZY", "123456"); super.setName("CustomRealm"); &#125; // 授权 protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; String username = (String) principals.getPrimaryPrincipal(); // 从数据库或者缓存中获取角色数据 Set&lt;String&gt; roles = getRolesByUserName(username); Set&lt;String&gt; permissions = getPermissionsByUserName(username); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.setRoles(roles); info.setStringPermissions(permissions); return info; &#125; // 认证 protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; //1 从主体传过来的认证信息中获得用户名 String username = (String) token.getPrincipal(); //2.通过用户名到数据库中获取凭证 String password = getPasswordByUserName(username); if(password == null) &#123; return null; &#125; SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo("LEEZY", password, "CustomRealm"); return authenticationInfo; &#125; // 模拟数据库中权限查询 private Set&lt;String&gt; getPermissionsByUserName(String username) &#123; Set&lt;String&gt; sets = new HashSet&lt;String&gt;(); sets.add("user:delete"); sets.add("user:add"); return sets; &#125; // 模拟数据库中角色查询 public Set&lt;String&gt; getRolesByUserName(String username) &#123; Set&lt;String&gt; sets = new HashSet&lt;String&gt;(); sets.add("admin"); sets.add("user"); return sets; &#125; // 模拟数据库的查询凭证 private String getPasswordByUserName(String username) &#123; return userMap.get(username); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.subject.Subject;import org.junit.Test;import com.leezy.shiro.realm.CustomRealm;public class CustomRealmTest &#123; @Test public void testAuthentication() &#123; CustomRealm customRealm = new CustomRealm(); // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(customRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("LEEZY", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.checkRole("admin"); subject.checkPermission("user:delete"); subject.checkPermission("user:add"); &#125;&#125; Shiro加密Shiro加密方式： HashedCredentialsMatcher 自定义Realm中使用散列 盐的使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.leezy.shiro.realm;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import org.apache.shiro.authc.AuthenticationException;import org.apache.shiro.authc.AuthenticationInfo;import org.apache.shiro.authc.AuthenticationToken;import org.apache.shiro.authc.SimpleAuthenticationInfo;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.crypto.hash.Md5Hash;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.subject.PrincipalCollection;import org.apache.shiro.util.ByteSource;public class CustomRealm extends AuthorizingRealm&#123; Map&lt;String, String&gt; userMap = new HashMap&lt;String, String&gt;(16); &#123; // 使用加密之后的密文 userMap.put("LEEZY", "e43a7da8514a1c9d566164b3ea731a4a"); super.setName("CustomRealm"); &#125; // 授权 protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; String username = (String) principals.getPrimaryPrincipal(); // 从数据库或者缓存中获取角色数据 Set&lt;String&gt; roles = getRolesByUserName(username); Set&lt;String&gt; permissions = getPermissionsByUserName(username); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.setRoles(roles); info.setStringPermissions(permissions); return info; &#125; // 认证 protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; //1 从主体传过来的认证信息中获得用户名 String username = (String) token.getPrincipal(); //2.通过用户名到数据库中获取凭证 String password = getPasswordByUserName(username); if(password == null) &#123; return null; &#125; SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo("LEEZY", password, "CustomRealm"); // 将盐设置进去 authenticationInfo.setCredentialsSalt(ByteSource.Util.bytes("SAKURA")); return authenticationInfo; &#125; // 模拟数据库中权限查询 private Set&lt;String&gt; getPermissionsByUserName(String username) &#123; Set&lt;String&gt; sets = new HashSet&lt;String&gt;(); sets.add("user:delete"); sets.add("user:add"); return sets; &#125; // 模拟数据库中角色查询 public Set&lt;String&gt; getRolesByUserName(String username) &#123; Set&lt;String&gt; sets = new HashSet&lt;String&gt;(); sets.add("admin"); sets.add("user"); return sets; &#125; // 模拟数据库的查询凭证 private String getPasswordByUserName(String username) &#123; return userMap.get(username); &#125; public static void main(String[] args) &#123; // MD5 + 加盐 Md5Hash md5Hash = new Md5Hash("123456", "SAKURA"); System.out.println(md5Hash.toString()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.leezy.shiro.shirotest;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.authc.credential.HashedCredentialsMatcher;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.subject.Subject;import org.junit.Test;import com.leezy.shiro.realm.CustomRealm;public class CustomRealmTest &#123; @Test public void testAuthentication() &#123; CustomRealm customRealm = new CustomRealm(); // 1. 构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(customRealm); HashedCredentialsMatcher matcher = new HashedCredentialsMatcher(); // 设置加密方式 matcher.setHashAlgorithmName("md5"); // 设置加密次数 matcher.setHashIterations(1); customRealm.setCredentialsMatcher(matcher); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("LEEZY", "123456"); subject.login(token); System.out.println("isAuthenticated: " + subject.isAuthenticated()); subject.checkRole("admin"); subject.checkPermission("user:delete"); subject.checkPermission("user:add"); &#125;&#125; Shiro集成Spring// TODO]]></content>
      <tags>
        <tag>SHIRO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令总结]]></title>
    <url>%2F2019%2F02%2F03%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[常用的CentOS及Ubuntu命令总结，方便查看与记忆。 Linux常用命令(1)查看DNS记录的命令（A记录、MX记录） 123yum install bind-utilsdig +noall +answer www.leezy.top (2) 查看端口常用命令： 1netstat -lnp|grep 80 (3) 查看版本 12ll /etc/*centos*cat /etc/centos-release (4) 查看内核经常在执行 yum udpate 命令后CentOS会出现多个启动项，所以我们经常要删除： 12345678# 查看当前系统正在使用的内核版本uname -a# 查看当前系统的全部Kernelrpm -q kernel# 删除多余的内核启动项yum remove kernel-3.10.0-957.el7.x86_64# 重新启动reboot (5) 修改主机名 123hostnamectl set-hostname NAME# 重启机器reboot CentOS Minimal版本配置换源：http://mirrors.163.com/.help/centos.html 安装软件123yum install net-toolsyum install wgetyum install vim 配置JAVA环境变量12345678910111213141516171819202122vim /etc/profile# 在文件末尾添加环境配置export JAVA_HOME=/usr/jdk-11.0.2export CLASSPATH=.:$&#123;JAVA_HOME&#125;/jre/lib/rt.jar:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin# 使新的环境生效source /etc/profile# 当Java版本更新时，仅仅修改JAVA_HOME的路径是不够的，需要删除链接which java/usr/jdk_xxx/bin/javawhich javac /usr/jdk_xxx_/bin/javac# 删除链接rm -rf /usr/jdk_xxx/bin/javarm -rf /usr/jdk_xxx_/bin/javac# 再执行使得新环境生效source /etc/profile GitLab CE版安装新建 /etc/yum.repos.d/gitlab-ce.repo，内容为： 12345[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1 再执行： 1234567891011sudo yum install curl openssh-server openssh-clients postfix croniesudo service postfix startsystemctl enable postfix.service(yum install lokkit)sudo lokkit -s http -s sshsudo yum makecachesudo yum install gitlab-ce#启动GitLabsudo gitlab-ctl reconfigure 常用的网络配置修改DNS(貌似会重启失效) 1vim /etc/resolv.conf 修改IP地址 1vim /etc/sysconfig/network-scripts/ifcfg-enp0s3 清理内存1234echo 3 &gt; /proc/sys/vm/drop_caches# 查看内存使用情况free -h Docker安装及配置Docker官方文档访问这里 阿里云安装文档这里Docker安装 12# 仅公网环境，使用官方安装脚本curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 由于直接使用DockerHub下载的速度很慢，所以可以使用阿里云的容器镜像加速服务,阿里云配置地址访问网址这里： 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["加速器地址"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 这样就可以快速的使用Docker pull 等命令了。清理Dcoker 容器： 1docker system prune 开放防火墙端口12345678910# 查看所有开放的端口firewall-cmd --list-ports# --permanent 永久生效firewall-cmd --zone=public --add-port=9092/tcp --permanent# 重新载入firewall-cmd --reload# 查看firewall-cmd --zone=public --query-port=80/tcp# 删除firewall-cmd --zone=public --remove-port=80/tcp --permanent Putty连接Ubuntu虚拟机 首先确保自己有root权限 12# 设置root密码sudo passwd 首先配置Host-Only网卡，然后点击 登入虚拟机使用ifconfig -a命令, 发现会有三个网卡(lo-本地、enp0s3-NET、enp0s8-Host-Only), 但是Host-Only没有IP地址 输入sudo dhclient enp0s8, 即可获取IP地址 Ubuntu 18.0新版后, /etc/network/interfaces的文件配置已经被弃用, vim /etc/netplan/50-cloud-init.yaml 配置如下： 12345678910111213141516# This file is generated from information provided by# the datasource. Changes to it will not persist across an instance.# To disable cloud-init's network configuration capabilities, write a file# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:# network: &#123;config: disabled&#125;network: ethernets: enp0s3: dhcp4: true enp0s8: dhcp4: false addresses: [192.168.56.101/24] nameservers: &#123;&#125; optional: true version: 2~ 修改完以后直接执行 sudo netplan apply 重启ssh服务 1service ssh restart Linux运维常用命令 Linux文件管理df 检查文件系统的磁盘空间占用情况，删除的文件但是还有程序在使用的时候会进行显示。这个命令会比du命令大。du 对文件和目录磁盘使用的空间查看，不包含已经删除的文件，命令执行结果小于或等于df。常用的命令如下 1234# 以GB MB KB的格式查看文件df -h# 查看文件节点数df -i Linux文件压缩与解压 1234# 文件压缩 tar -zcvf &lt;压缩包名称&gt; &lt;要压缩的文件路径或者文件名&gt;tar -zcvf my.tar.gz my.txt# 文件解压tar -xzvf my.tar.gz xargs 将命令输出的结果作为一个参数传递给另一个命令 找出/目录下以.conf结尾的文件，并进行文件分类。 1find / -name *.conf -type f -print | xargs file 命令或者脚本后台运行数据库的导入导出操作，耗时较久。1nohup mysqldump -uroot -pXXX --all-databases &gt; ./alldatabases.sql &amp; 命令后台执行的结果会在命令执行的当前目录下留下一个nohup.out文件，查看这个文件就可以知道命令有没有执行报错等信息。 找出当前系统资源使用量较高的进程 1234# 内存使用量前3ps -ef | sort -rnk 4 | head -3# CPU使用量前3ps -ef | sort -rnk 3 | head -3 VIM 使用技巧 123456789# 显示当前行数:set nu# 跳到指定行 - 80:80gg:80G# 第一行:gg# 最后一行:G wc 命令 1234# 统计文件的 行数，单词数，字节数wc &lt;file&gt;# 统计行数wc -l &lt;file&gt; sh -c它可以让 bash 将一个字串作为完整的命令来执行。 1sh -c netstat命令 12# 查看指定端口占用情况netstat -anp | grep -i "8080" curl用法 123456789101112131415161718-X 指定请求方法-x 指定HTTP请求的代理-H 指定请求标头-d 发送POST请求提交的数据，使用-d参数后，会自动将请求转为POST，HTTP请求会自动加上标头Content-Type:application/x-www-form-urlencoded，可省略-X POST-v 显示http通信的整个过程-u 设置服务器认证的用户名和密码-i 显示Response头信息，并打印源码-I 显示Response头信息，不打印源码-s 不输出错误和进度信息-S 指定只输出错误信息-L 自动跳转，curl默认不跟随跳转-k 跳过SSL检测-o 文件名 保存-O 将URL的最后部分当作文件名保存 grep 命令 -i 忽略大小写-n 显示结果所在行号-v 反向查询-e 多个选项匹配-w 匹配整个单词，如果字符串中包含整个单词，并不匹配-E 对应正则表达式，也可以实现或的操作-a 让二进制文件等价于文本文件 1234567# 多个 -e 实现or的操作grep -e true -e false test.txt# &lt;/dev/tcp/ip/7550是判断ip在7550端口的连通性，将结果输出到/dev/null这个黑洞，然后错误信息也输出到这里# 在Linux中0表示标准输入，1表示标准输出，2表示标准错误sh -c "&lt;/dev/tcp/$local_ip/7550" &gt;/dev/null 2&gt;&amp;1grep -a '^uyun.baseurl=' curl 命令 1234# 判定指定url的连通性curl -v # 当发生错误的时候返回错误信息curl -sS sed 命令 12# 将test.txt的false改为truesed -i `s/false/true/g` test.txt Linux查看CPU、内存 123456# 查看总内存cat /proc/meminfo | gerp MemTotal# 查看CPU核数grep 'physical id' /proc/cpuinfo | sort | uniq | wc -l# 查看CPU的型号 - uniq -c 统计出现的次数cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c awk 命令 12# 根据分隔符查找内容awk -F '=' '&#123;print $1&#125;']]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《重新定义SpringCloud实战》读书笔记]]></title>
    <url>%2F2018%2F12%2F18%2FSpringCloud%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[SpringCloud提供了快速构建分布式中常见模式的工具，包括配置管理、服务发现、断路器、智能路由、微代理、控制总线等。SpringCloud中间件是基于SpringBoot的实现，提供了对微服务完整的一套解决方案。 应用架构的发展历程：单体应用架构 –&gt; 分布式架构 –&gt;面向服务的SOA架构 –&gt; 微服务架构SOA架构个人理解是多个应用之间通过企业数据总线ESB通信的架构，其应用程序通过网络协议提供服务，消费服务，不同业务提供不同的服务。(阿里的服务治理框架Dubbo)微服务架构：一个大型的应用拆分为多个相互独立的微服务，每个服务之间松耦合，通过REST API或者HTTP进行通信。 SpringCloud包包含以下组件：服务治理组件 Eureka / Consul + 客户端负载均衡组件 Ribbon + 声明式服务调用组件 Feign + API网关治理组件 Zuul / GateWay(高并发) + 熔断机制 HyStrix + 分布式配置中心组件 Spring Cloud Config / 携程 Apollo + 消息总线组件 Bus + 消息驱动组件 Stream + 分布式服务跟踪组件 Sleuth + 全链路监控 SkyWalking. Tips: 代码基于Spring Cloud Finchley 版本 服务治理：Spring Cloud Eureka负责微服务架构中的服务治理功能，即各个微服务实例的自动化注册与发现。SpringCloud Eureka 是由 Netflix Eureka实现的，即包含了服务端组件也包含了客户端组件。Eureka服务端也被称为服务注册中心，各个微服务启动时会向Eureka Server 注册自己的信息。代码如下：在https://start.spring.io/ 中新建一个Eureka Server的Demo，或者直接在Maven项目中的pom.xml文件中添加如下Dependence: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;eureka-server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;eureka-server&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Greenwich.RC1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; EurekaServerApplication.java 1234567891011121314151617package com.leezy.eureka_server;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication//该注解启动一个服务注册中心提供给其他应用会话@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; System.out.println("Hello Eureka Server!"); SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; Eureka Server 中的 application.yml 和 application-standalone.yml 12345678910111213141516# application.ymlspring: profiles: active: standalone jackson: serialization: FAIL_ON_EMPTY_BEANS: falseeureka: server: use-read-only-response-cache: false response-cache-auto-expiration-in-seconds: 10management: endpoints: web: exposure: include: '*' 123456789101112131415# application-standalone.ymlserver: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false #是否将自己注册到Eureka Server, 默认为True fetchRegistry: false #是否需要从Eureka Server获取注册信息, 默认为Ture serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ # 查询服务和注册服务的地址，多个用","隔开 server: waitTimeInMsWhenSyncEmpty: 0 enableSelfPreservation: false 打开 http://localhost:8761/ 看到Eureka的控制面板。Eureka服务注册端， Eureka Client将微服务注册到Eureka Server上。EurekaClientApplication.java 1234567891011121314151617package com.leezy.eureka_client;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication// 该注解适配性比较好，可以用于多种服务发现组件(Zookeeper、Consul)@EnableDiscoveryClientpublic class EurekaClientApplication &#123; public static void main(String[] args) &#123; System.out.println("Hello Eureka Client!"); SpringApplication.run(EurekaClientApplication.class, args); &#125;&#125; Maven依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; Eureka Client的配置文件： application.yml 和 application-demo.yml 1234#application.ymlspring: profiles: active: demo 1234567891011121314#application-demo.ymlserver: port: 8081Spring: application: name: demo-leezy #声明服务提供者的应用名称eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ #设置与Eureka Server交互的地址 instance: prefer-ip-address: true 刷新Eureka控制台就可以看到注册到Server上的服务了。 Eureka的设计理念： 服务实例如何注册到服务中心：（1）调用Eureka Server的REST API 的 register方法（2）Java语言使用者可以调用NetFlix的Eureka Client封装的API（3）Spring Cloud使用者在pom.xml文件中引用 spring-cloud-starter-netflix-eureka-client，基于Spring Boot的自动配置即可。 服务实例从服务中心剔除（1）服务实例正常关闭时，通过钩子方法或者生命周期回调方法调用Eureka Server 的REST API的de-register方法。（2）Eureka要求Client定时续约(30s)，如果90s没有续约操作则Eureka Server主动剔除该操作。 服务实例信息的一致性问题服务注册与发现中心应该也是一个集群，如何保证一致性（1）AP优于CP (Zookeeper-CP, Eureka-AP)（2）Peer to Peer架构(1. 主从复制 2. 对等复制)（3）Zone及Region设计（4）SELF PRESERVATION设计 WebService客户端 FeignFeign是一个声明式的Web Service客户端，用于服务与服务之间的调用，支持SpringMVC注解，整合了Ribbon以及Hystrix。对应的POM依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 应用入口程序SpringCloudFeignApplication.java 123456789101112131415package com.leezy.hello_feign;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableFeignClientspublic class HelloFeignApplication &#123; public static void main( String[] args ) &#123; SpringApplication.run(HelloFeignApplication.class, args); &#125;&#125; 接口类：HelloFeignService.java，作用是应用指定的URL最终转化为Github API允许的URL。eg：https://api.github.com/search/repositories?q=spring-cloud(Github RestAPI的文档：https://developer.github.com/v3/search/) 1234567891011121314package com.leezy.hello_feign.service;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import com.leezy.hello_feign.config.HelloFeignServiceConfig;@FeignClient(name = "github-client", url = "https://api.github.com", configuration = HelloFeignServiceConfig.class)public interface HelloFeignService &#123; @RequestMapping(value = "/search/repositories", method = RequestMethod.GET) String searchRepo(@RequestParam(name = "q") String quertStr);&#125; 控制类：HelloFeignController.java，作用：调用服务提供者的接口 123456789101112131415161718192021package com.leezy.hello_feign.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import com.leezy.hello_feign.service.HelloFeignService;@RestControllerpublic class HelloFeignController &#123; @Autowired private HelloFeignService helloFeignService; @GetMapping(value = "/search/github") public String searchGithubRepoByStr(@RequestParam("str") String queryStr) &#123; return helloFeignService.searchRepo(queryStr); &#125;&#125; 配置类：HelloFeignServiceConfig.java，@Bean注解配置日志的bean 123456789101112131415package com.leezy.hello_feign.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import feign.Logger;@Configurationpublic class HelloFeignServiceConfig &#123; @Bean Logger.Level feignLoggerLevel() &#123; //level有四个级别 return Logger.Level.FULL; &#125;&#125; 配置文件: application.yml 123456789server: port: 8010spring: application: name: Hello Feignlogging: level: com.leezy.hello_feign.service.HelloFeignService: DEBUG #在这里配置日志的输出级别 启动应用后，访问网址：http://localhost:8010/search/github?str=spring-cloudFeign支持的属性文件配置方式有两种：application.yml(application.properties) 以及 Java方式的配置类，但是配置文件的优先级会高于Java类的优先级。Feign默认的是JDK原生的URL Connection，并没有使用连接池，可以用Http Client和 okHttp进行替换对项目进行调优。 Feign 的 POST 和 GET 的多参数传递Feign拦截器，将Json转化为Map。实现Feign的RequestInterceptor中的apply方法来进行统一拦截转换处理Feign中的GET方法多参数传递问题。集成Swapper，编写服务消费者用于调用Feign进行Get或Post多参数传递。 负载均衡组件 RibbonFeign中集成了Ribbon，但是Ribbon可以单独使用，它是一种进程内负载均衡器（客户端负载均衡），它赋予了应用支配Http和Tcp的能力。负载均衡策略：最常用的是RoundRobinRule 轮询策略代码样例：pom.xml 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动类：RibbonLoadbalancerApplication.java注入一个RestTemplate的Bean，并且使用@LoadBalances注解才能使其具备负载均衡的能力。 1234567891011121314151617181920212223package cn.springcloud.book;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplication@EnableDiscoveryClientpublic class RibbonLoadbalancerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RibbonLoadbalancerApplication.class, args); &#125; @Bean @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; TestController.javaRibbon客户端需要创建一个API来调用Eureka源服务自定义的API 12345678910111213141516171819import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;@RestControllerpublic class TestController &#123; @Autowired private RestTemplate restTemplate; @GetMapping("/add") public String add(Integer a, Integer b) &#123; String result = restTemplate .getForObject("http://CLIENT-A/add?a=" + a + "&amp;b=" + b, String.class); System.out.println(result); return result; &#125;&#125; 通过查找继承关系，发现接口ILoadBalancer的实现抽象类AbstractLoadBalancer的实现类BaseLoadBalancer中的chooseServer方法是真正实现负载均衡的地方。 123456789101112131415161718192021/* * Get the alive server dedicated to key * * @return the dedicated server */public Server chooseServer(Object key) &#123; if (counter == null) &#123; counter = createCounter(); &#125; counter.increment(); if (rule == null) &#123; return null; &#125; else &#123; try &#123; return rule.choose(key); &#125; catch (Exception e) &#123; logger.warn("LoadBalancer [&#123;&#125;]: Error choosing server for key &#123;&#125;", name, key, e); return null; &#125; &#125;&#125; 熔断机制 Spring Cloud HystrixHystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable.Hystrix的设计目标是： 通过客户端对延迟和故障进行保护和控制 在一个复杂的分布式系统中停止级联故障 快速失败和迅速恢复 在合理的情况下回退和优雅地降级 开启实时监控、告警和操作控制pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.leezy&lt;/groupId&gt; &lt;artifactId&gt;hystrix&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;hystrix&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Greenwich.RC1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; ClientApplication.java 123456789101112131415package com.leezy.hystrix;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.hystrix.EnableHystrix;@SpringBootApplication@EnableHystrix@EnableDiscoveryClientpublic class ClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ClientApplication.class, args); &#125;&#125; TestController.java 12345678910111213141516171819package com.leezy.hystrix.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import com.leezy.hystrix.service.IUserService;@RestControllerpublic class TestController &#123; @Autowired private IUserService userService; @GetMapping("/getUser") public String getUser(@RequestParam String username) throws Exception&#123; return userService.getUser(username); &#125;&#125; IUserService.java 12345package com.leezy.hystrix.service;public interface IUserService &#123; public String getUser(String username) throws Exception;&#125; UserService.java 12345678910111213141516171819202122232425package com.leezy.hystrix.service.impl;import org.springframework.stereotype.Component;import com.leezy.hystrix.service.IUserService;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;@Componentpublic class UserService implements IUserService&#123; // 降级处理 @Override @HystrixCommand(fallbackMethod="defaultUser") public String getUser(String username) throws Exception &#123; if (username.equals("spring")) &#123; return "This is real user."; &#125; else &#123; throw new Exception(); &#125; &#125; public String defaultUser(String username) &#123; return "The User does not exist in the system...Test!"; &#125;&#125; bootstrap.yml 1234567891011server: port: 8888spring: application: name: hystrix-client-serviceeureka: client: serviceUrl: defaultZone: http://$&#123;eureka.host:127.0.0.1&#125;:$&#123;eureka.port:8761&#125;/eureka/ instance: prefer-ip-address: true 打开浏览器访问：http://localhost:8888/getUser?username=spring 和 http://localhost:8888/getUser?username=testERROR Hystrix DashboardHystrix Dashboard仪表盘是根据系统一段时间内发生的请求情况来展示的可视化面板。Hystrix的指标需要端口进行支撑，所以需要增加actuator依赖。pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; HystrixDashboardApplication.java 12345678@SpringBootApplication@EnableDiscoveryClient@EnableHystrixDashboardpublic class HystrixDashboardApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(HystrixDashboardApplication.class, args); &#125;&#125; 上面是单个实例的Hystrix Dashboard,整个系统和集群的情况下并不是特别有用。Turbine就是聚合所有相关Hystrix.stream 流的方案。 网关治理组件 ZuulZuul is the front door for all requests from devices and web sites to the backend of the Netflix streaming application. Zuul是对内部的微服务提供可配置的，对外URL到服务的映射关系，基于JVM的后端路由器。pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt; 启动类ZuulServerApplication.java 12345678@SpringBootApplication@EnableDiscoveryClient@EnableZuulProxypublic class ZuulServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZuulServerApplication.class, args); &#125;&#125; bootstrap.yml 12345678910111213141516spring: application: name: zuul-serverserver: port: 5555eureka: client: serviceUrl: defaultZone: http://$&#123;eureka.host:127.0.0.1&#125;:$&#123;eureka.port:8888&#125;/eureka/ instance: prefer-ip-address: truezuul: routes: client-a: path: /client/** serviceId: client-a 最后五行的代码表示，Zuul组件的端口为portA，则将/client开头的URL映射搭配client-a这个服务中去，即实际访问portB。/* 匹配任意数量的路径和字符/ 匹配任意数量的字符/? 匹配单个字符 Spring Cloud Zuul Filter链(1) Filter的类型(2) Filter的执行顺序(3) Filter的执行条件(4) Filter的执行效果Zuul有四种不同生命周期的Filter,分别是：pre Filter 按照规则路由到下级服务之前执行。比如鉴权、限流等route Filter 路由动作的执行者（Apache HttpClient或Netflix Ribbon构建和发送原始Http请求的地方）post Fliter 在源服务返回结果或者异常信息发生后执行的，对返回信息做一些处理error Filter 在整个生命周期内如果发生异常，则会进入error Filter Spring Cloud Zuul 权限集成OAuth2.0 + JWT(JSON Web Token) 动态路由 Dynamic Routing有两种解决方案：（1） Spring Cloud Config + Bus、动态刷新配置文件。（2） 重写Zuul的配置读取方式]]></content>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot学习]]></title>
    <url>%2F2018%2F12%2F11%2FSpringBoot%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Spring Boot makes it easy to create stand-alone, production-grade Spring-based Applications that you can run. 1. 常见注解@RequestMapping (@GetMapping &amp; @PostMapping)The @RequestMapping annotation provides “routing” information. It tells Spring that any HTTP request with the / path should be mapped to the home method.@GetMapping是一个组合注解，是@RequestMapping(method = RequestMethod.GET)的缩写。@PostMapping是一个组合注解，是@RequestMapping(method = RequestMethod.POST)的缩写。 @RestControllerThe @RestController annotation tells Spring to render the resulting string directly back to the caller.等价于 @Controller + @ResponseBody,为了使Http请求返回数据格式为json格式。 @EnableAutoConfigurationSince spring-boot-starter-web added Tomcat and Spring MVC, the auto-configuration assumes that you are developing a web application and sets up Spring accordingly. @ConditionalOnProperty可以用来控制配置是否生效。 @SpringBootApplicationThe @SpringBootApplication annotation is often placed on your main class, and it implicitly defines a base “search package” for certain items. same as @Configuration,@EnableAutoConfiguration,@ComponentScan. 12345678910111213package com.example.myapplication;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @ConfigurationDeclare the configuration Classes. @EnableAutoConfigurationIf you find that specific auto-configuration classes that you do not want are being applied, you can use the exclude attribute of @EnableAutoConfiguration to disable them. 12345678import org.springframework.boot.autoconfigure.*;import org.springframework.boot.autoconfigure.jdbc.*;import org.springframework.context.annotation.*;@Configuration@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)public class MyConfiguration &#123;&#125; @EnableConfigurationProperties 和 @ConfigurationProperties这两个注解的作用可以参见 Spring Boot 官方文档 12345678910111213@Component@ConfigurationPropertiespublic class UriConfigurationProperties &#123; private String httpbin = "http://httpbin.org:80"; public String getHttpbin() &#123; return httpbin; &#125; public void setHttpbin(String httpbin) &#123; this.httpbin = httpbin; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536package org.spring.cloud.gateway;import org.spring.cloud.gateway.properties.UriConfigurationProperties;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.cloud.gateway.route.RouteLocator;import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;import org.springframework.context.annotation.Bean;@SpringBootApplication//@EnableConfigurationProperties(org.spring.cloud.gateway.properties.UriConfigurationProperties.class)public class GateWayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GateWayApplication.class, args); &#125; @Bean public RouteLocator myRoutes(RouteLocatorBuilder builder, UriConfigurationProperties uriConfiguration) &#123; String httpUri = uriConfiguration.getHttpbin(); return builder.routes() .route(p -&gt; p .path("/get") .filters(f -&gt; f.addRequestHeader("Hello", "World")) .uri(httpUri)) .route(p -&gt; p .host("*.hystrix.com") .filters(f -&gt; f .hystrix(config -&gt; config .setName("mycmd") .setFallbackUri("forward:/fallback"))) .uri(httpUri)) .build(); &#125;&#125; 上述两个文件位于两个包下，如果在第一个文件中使用了@Component，则在第二个文件中就不能再使用@EnableConfigurationProperties注解，否则将会报错，提示有两个Bean。如果第一个文件没有使用@Component则第二个文件可以将注释去掉。 单元测试常用注解 @RunWith(SpringJUnit4ClassRunner.class)引入Spring对JUnit4的支持。 @SpringApplicationConfiguration(classes = HelloApplication.class)指定SpringBoot的启动类 @WebAppConfiguration开启Web应用配置，用于模拟ServletContext @Before &amp; @Test &amp; @After@Before：JUnit中定义在测试用例@Test内容执行前预加载的内容，同理判断。 2. Starter POMs1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 命名规则都是spring-boot-starter-，代表一个特别的应用功能模块。 spring-boot-starter-web全栈Web开发模块，包含嵌入式的Tomcat、SpringMVC spring-boot-starter-test包含Junit、Hamcrest、Mockito spring-boot-starter-actuator为SpringBoot构建的应用提供一系列用于监控的端点。访问：http://127.0.0.1:8080/actuator可以看到输出了如下JSON文件： 123456789101112131415161718192021222324&#123; "_links": &#123; "self": &#123; "href": "http://127.0.0.1:8080/actuator", "templated": false &#125;, "health": &#123; "href": "http://127.0.0.1:8080/actuator/health", "templated": false &#125;, "health-component": &#123; "href": "http://127.0.0.1:8080/actuator/health/&#123;component&#125;", "templated": true &#125;, "health-component-instance": &#123; "href": "http://127.0.0.1:8080/actuator/health/&#123;component&#125;/&#123;instance&#125;", "templated": true &#125;, "info": &#123; "href": "http://127.0.0.1:8080/actuator/info", "templated": false &#125; &#125;&#125; spring-boot-devtools(1) Property Defaults(2) Automatic Restart(3) LiveReload(4) Global Settings(5) Remote Applications 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 配置文件SpringBoot的默认配置文件位置是src/main/resources/application.properties. 123&lt;!-- application.properties --&gt;book.name=JAVA编程思想book.author=Bruce Eckel 应用中可以使用 @Vlaue注解来加载这些自定义的参数 1234567@Componentpublic class Book &#123; @Value("$&#123;book.name&#125;") private String name; @value("$&#123;book.author&#125;") private String author;&#125; Java代码中引用有以下两种方式： PlaceHolder方式 ${…} SpEL表达式 #{…} 通过application-{profile}多环境配置文件：application-dev.properties:开发环境application-test.properties:测试环境application-prod.properties:生产环境在application.properties中的spring.profiles.active属性来设置。 12// 调用测试环境spring.profiles.active = test; 在采用java -jar xxx.jar 形式运行项目时，”–”就是对application.properties中的属性值进行赋值的标识。配置文件的优先级顺序如下：命令行输入 &gt; 包外 &gt; 包内所以可以在利用这一点来对配置进行快速准确的更改。 spring.factories文件Spring Factories实现原理(package org.springframework.core.io.support包下的SpringFactoriesLoader.class文件;)Spring 容器初始化时会加载该文件声明的类，我们可以通过@SpringBootApplication-&gt;@EnableAutoConfiguration-&gt;AutoConfigurationImportSelector-&gt;getCandidateConfigurations-&gt;SpringFactoriesLoaderspring-core包里定义了SpringFactoriesLoader类，这个类实现了检索META-INF/spring.factories文件，并获取指定接口的配置的功能。在这个类中定义了两个对外的方法： loadFactories: 根据接口类获取其实现类的实例，这个方法返回的是对象列表。loadFactoryNames: 根据接口获取其接口类的名称，这个方法返回的是类名的列表。 bootstrap.yml和appllication.yml的区别bootstrap.yml加载（父SpringApplicationContext）顺序在application.yml之前，用于应用程序上下文的引导阶段。用于指定spring.application.name和spring.cloud.config.server.git.uri以及一些加密和解密信息。eg：Spring Cloud Config 配置中心再使用时，通常将访问远程文件配置信息写在bootstrap.yml文件中。]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebService学习]]></title>
    <url>%2F2018%2F11%2F26%2FWebService%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[WebService的两种实现方式，Soap WebService、Restful WebService。 RPC refers to Remote Procedure Call.Use of RPC is recommended when there is heavy use of the client/server model.RPC allows for the processing of multiple threads that share a given address.RPC employed on a platform that uses EJB.Web Service used in non-Java platforms when an app wants access.Web Service also is used for synchronization of asynchronous communication.[1] WebService是一种技术，有两种实现方式：JAX-WS(Java API for XML-Based Service面向消息)、JAX-RS(Java API for Restful WebService面向资源)ps. RESTful 请求常用的方法有以下四种： GET: 用于查询对象 POST: 用于创建对象 PUT: 用于修改对象 DELETE: 用于删除对象 1. CXF 基于 SOAP 的 WebService12345678&lt;!-- IHelloWorld.java --&gt;package top.leezy.www;import javax.jws.WebService;@WebServicepublic interface IHelloWorld &#123; String sayHello(String name);&#125; 12345678&lt;!-- HelloWorldImpl.java --&gt;package top.leezy.www;public class HelloWorldImpl implements IHelloWorld &#123; public String sayHello(String name) &#123; return "Hello " + name; &#125;&#125; 1234567891011121314151617&lt;!-- applicationContext.xml --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jaxws="http://cxf.apache.org/jaxws" xmlns:cxf="http://cxf.apache.org/core" xmlns:wsa="http://cxf.apache.org/ws/addressing" xsi:schemaLocation="http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.1.xsd http://cxf.apache.org/core http://cxf.apache.org/schemas/core.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://cxf.apache.org/jaxws http://cxf.apache.org/schemas/jaxws.xsd"&gt; &lt;import resource="classpath:META-INF/cxf/cxf.xml" /&gt; &lt;import resource="classpath:META-INF/cxf/cxf-servlet.xml" /&gt; &lt;jaxws:endpoint id="HelloWorld" implementor="top.leezy.www.HelloWorldImpl" address="/sayHello" /&gt; &lt;/beans&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!-- web.xml --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://xmlns.jcp.org/xml/ns/javaee" xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd" id="WebApp_ID" version="3.1"&gt; &lt;display-name&gt;CXFwebservice&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:top/**/applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;CxfServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;CxfServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/webservice/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; 打开浏览器输入：http://127.0.0.1:8081/SoapWebService/webservice 2. CXF 基于 RestFul 的 WebService12345678910111213141516&lt;!-- Config.java --&gt;package top.leezy.www;import java.util.LinkedList;import java.util.List;public class Config &#123; public static List&lt;User&gt; users; static &#123; users = new LinkedList&lt;User&gt;(); User user = new User(); user.setId("123456"); user.setName("SAKURA"); users.add(user); &#125;&#125; 12345678910111213141516171819202122232425262728&lt;!-- User.java --&gt;package top.leezy.www;import javax.xml.bind.annotation.XmlRootElement;@XmlRootElement(name = "User")public class User &#123; private String name; private String id; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!-- UserService.java --&gt;package top.leezy.www;import javax.ws.rs.Consumes;import javax.ws.rs.DELETE;import javax.ws.rs.GET;import javax.ws.rs.POST;import javax.ws.rs.PUT;import javax.ws.rs.Path;import javax.ws.rs.Produces;import javax.ws.rs.PathParam;import javax.ws.rs.QueryParam;import javax.ws.rs.core.MediaType;import javax.ws.rs.core.Response;import javax.ws.rs.core.Response.Status;@Path("/UserService")// 可以注释在方法上或者类上（以最小单位为准）, 指定返回给客户端的类型@Produces(&#123;"application/json", "application/xml"&#125;)public class UserService &#123; @GET @Path("/getUser/&#123;id&#125;") @Produces(MediaType.APPLICATION_XML) public User getUser(@PathParam("id") String id) &#123; if (id != null &amp;&amp; id.length() &gt; 0) &#123; for (User user : Config.users) &#123; if(id.equals(user.getId())) return user; &#125; User add_user = new User(); add_user.setId(id); return add_user; &#125; else &#123; return new User(); &#125; &#125; @POST @Path("/regUser") // 一般用于 @PUT、@POST, 用于接受客户端发送过来的MIME类型 @Consumes(&#123;"application/json", "application/xml"&#125;) public Response regUser(User user) &#123; if (Config.users.contains(user)) &#123; return Response.status(Status.BAD_REQUEST).build(); &#125; else &#123; Config.users.add(user); return Response.ok("id = " + user.getId() + ", name = " + user.getName()).build(); &#125; &#125; @DELETE @Path("/delUser") @Consumes(&#123;"application/json", "application/xml"&#125;) public Response delPerson(@QueryParam("id") String id) &#123; User user = new User(); user.setId(id); if (Config.users.contains(user)) &#123; return Response.status(Status.BAD_REQUEST).build(); &#125; else &#123; Config.users.remove(user); return Response.ok(user).build(); &#125; &#125; @PUT @Path("/updateUser") @Consumes(&#123;"application/json", "application/xml"&#125;) public Response updateUser(User user) &#123; if (Config.users.contains(user)) &#123; return Response.status(Status.BAD_REQUEST).build(); &#125;else &#123; for (User old_user : Config.users) &#123; if (old_user.equals(user)) &#123; old_user.setName(user.getName()); &#125; &#125; return Response.ok(user).build(); &#125; &#125;&#125; 123456789101112131415161718192021&lt;!-- beans.xml --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jaxrs="http://cxf.apache.org/jaxrs" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://cxf.apache.org/jaxrs http://cxf.apache.org/schemas/jaxrs.xsd"&gt; &lt;import resource="classpath:META-INF/cxf/cxf.xml" /&gt; &lt;import resource="classpath:META-INF/cxf/cxf-servlet.xml" /&gt; &lt;bean id="userService" class="top.leezy.www.UserService" /&gt; &lt;jaxrs:server id="rs_server" address="/restfulService"&gt; &lt;jaxrs:serviceBeans&gt; &lt;ref bean="userService" /&gt; &lt;/jaxrs:serviceBeans&gt; &lt;/jaxrs:server&gt;&lt;/beans&gt; 12345678910111213141516171819202122232425262728293031&lt;!-- web.xml --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" id="WebApp_ID" version="3.0"&gt; &lt;display-name&gt;CxfRestWebservice&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:top/**/beans.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;cxfservlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;cxfservlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/webservice/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 打开浏览器输入：http://127.0.0.1:8081/RestFulWebService/webservice/使用POSTMan进行测试：RestFul WebService 的JAR包较 Soap WebService 要区别以下几个：-javax.ws.rs-api-2.1.1.jar-cxf-rt-frontend-jaxrs-3.2.7.jar 参考文献：[1]http://www.differencebetween.net/technology/protocols-formats/difference-between-rpc-and-web-service/#ixzz5YLemRQhd[2]https://www.cnblogs.com/zhuyiqizhi/p/6213502.html]]></content>
      <tags>
        <tag>JavaWEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装Tomcat]]></title>
    <url>%2F2018%2F11%2F20%2FCentOS%E5%AE%89%E8%A3%85Tomcat%2F</url>
    <content type="text"><![CDATA[系统：CentOS Linux release 7.5.1804Tomcat版本： apache-tomcat-7.0.92.tar.gz 1. 下载Tomcat下载链接：https://www-us.apache.org/dist/tomcat/tomcat-7/v7.0.92/bin/apache-tomcat-7.0.92.tar.gz 2. 解压缩文件1tar -zxvf apache-tomcat-7.0.92.tar.gz 3. 打开8080端口并重启防火墙12firewall-cmd --zone=public --add-port=8080/tcp --permanentfirewall-cmd --reload 4. 配置tomcat-users.xml12345&lt;!-- 在最后添加用户角色用户名密码 参数意义可以在自带的文档上查看--&gt;&lt;role rolename="admin-gui"/&gt;&lt;role rolename="manager-gui"/&gt;&lt;role rolename="manager-jmx"/&gt;&lt;user username="admin" password="admin" roles="admin-gui,manager-gui,manager-jmx" /&gt; 5. 访问Tomcat Manager访问地址： http://192.168.56.101:8080/ 并登陆 admin admin]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB安装]]></title>
    <url>%2F2018%2F11%2F20%2FMongoDB%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装解压缩版本的MongoDB(mongodb-win32-x86_64-2008plus-ssl-4.0.4)的简单命令。 1. 配置环境变量12# 加入环境变量C:\Program Files\mongodb-win32-x86_64-2008plus-ssl-4.0.4\bin 2. 创建 data 文件夹 及 log 文件夹创建 mongodb-data 和 mongodb-log 文件夹 3. 用管理员权限执行安装命令12cd C:\Program Files\mongodb-win32-x86_64-2008plus-ssl-4.0.4\binmongod --dbpath "C:\Program Files\mongodb-win32-x86_64-2008plus-ssl-4.0.4\mongodb-data" --logpath "C:\Program Files\mongodb-win32-x86_64-2008plus-ssl-4.0.4\mongodb-log\MongoDB.log" --install --serviceName "mongo" --logappend --directoryperdb 4. 测试安装结果访问 http://127.0.0.1:27017/]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow实验记录]]></title>
    <url>%2F2018%2F03%2F06%2FTensorFlow%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[环境说明：系统：Ubuntu Server 16.04.1 LTS 64位Python版本：Python 3.5.2TensorFlow版本：1.4 用户切换（最好启用root否则会有Bug）1234# 输入Linux root用户密码sudo passwd root# 默认用户切换到root用户su root 安装python环境12345# 不用默认的2.7 安装python3.5sudo apt-get install python3.5# 系统环境切换成python3.5sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150 安装TensorFlow12345678# 采用pip安装，首先安装# 查看系统内置的pip版本pip3 -V# 更新并安装sudo apt-get install python3-pip python3-devsudo pip install --upgrade pip# 安装CPU版本的TensorFlowpip3 install tensorflow 注意要保证pip是最新版本否则会出现以下错误： 测试TensorFlow123456python&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello)) 查看TensorFlow版本 备用知识点(Note)123451. vim -r filename 恢复未正常保存生成的.swap文件2. (Note)convolutional neural network 卷积神经网络rectified linear unit 线性修正单元 ReLUinput layer --&gt; hidden layer(more than one) --&gt; output layer 环境准备12345678910111213pip3 install -U scikit-learnpip3 install scipy# 下载 MNIST 数据集（http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_download.html）wget http://tensorflow-1253902462.cosgz.myqcloud.com/mnist_cnn/t10k-images-idx3-ubyte.gzwget http://tensorflow-1253902462.cosgz.myqcloud.com/mnist_cnn/t10k-labels-idx1-ubyte.gzwget http://tensorflow-1253902462.cosgz.myqcloud.com/mnist_cnn/train-images-idx3-ubyte.gzwget http://tensorflow-1253902462.cosgz.myqcloud.com/mnist_cnn/train-labels-idx1-ubyte.gzpip3 install \ -i https://pypi.tuna.tsinghua.edu.cn/simple/ \ https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/tensorflow-1.7.0-cp35-cp35m-linux_x86_64.whl Windows环境下.ipynb文件相关（python笔记）1234567# 安装python环境不必多说pip install ipythonpip install &quot;ipython[notebook]&quot;# cd 到有.ipynb文件的目录下执行下列命令就会自动打开浏览器点击文件即可打开文件ipython notebook]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++标准模板库STL学习笔记]]></title>
    <url>%2F2018%2F02%2F27%2FC%2B%2B%E6%A0%87%E5%87%86%E6%A8%A1%E6%9D%BF%E5%BA%93STL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[面试笔试算法必备知识点！文章内容总结自《算法笔记》，时刻巩固以免忘记(lll￢ω￢) 1.vectory用法：Vectory：变长数组 定义方法： 1234567891011vectory&lt;typename&gt; name;// eg:vectory&lt;int&gt; nums;// 如果typename类型也是一个STL容器，定义方式如下：vectory&lt;vectory&lt;int&gt; &gt; name; // &gt;&gt;之间有空格避免歧义 --》两维都可变的二维数组// vectory数组的定义方式如下：vectory&lt;typename&gt; Arrayname[arraySize] // 一维可变的二维数组// eg:vectory&lt;int&gt; nums[10] 访问方法： 123456789// 通过下标访问：nums[0], nums[1];// 通过迭代器访问：vectory&lt;typename&gt;::iterator it;//eg:vectory&lt;int&gt;::iterator it; // 得到迭代器it，通过*it进行访问vectory// nums[i] 等价于 *(it + i) 常用函数： 1234567891. push_back(x) 在vectory末尾添加一个元素x O(1) nums.push_back(i);2. pop_back() 在vectory末尾删除一个元素 O(1) nums.pop_back();3. size() 求得vectory元素个数 O(1) nums.size()4. begin() 求得vectory数组nums首地址与it指向地址一样 O(1) nums.beagin()5. end() 求得vectory数组nums尾元素的下一个地址 O(1) nums.end()6. clear() 清空vectory所有元素 O(n) nums.clear()7. insert(it, x) 向vectory任意迭代器it处插入一个元素x O(N) nums.insert(nums.begin() + 2， 3)8. erase(it) 删除迭代器it处元素 O(n) nums.erase(nums.begin() + 2)9. erase(first, last) 删除[first, last)内的所有元素 O(n) nums.erase(num.begin() + 1, num.end())删除除第一个元素外的其他元素]]></content>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM框架总结]]></title>
    <url>%2F2017%2F08%2F09%2FSSM%E6%A1%86%E6%9E%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[SSM框架的一些基本知识总结。 1.SSM定义SSM = Spring + SpringMVC + MyBatisSpring:Spring是一个轻量级的控制反转（IoC）和面向切面（AOP）的容器框架。 SpringMVC命名及分层：edu.xju.common.util 公共部分edu.xju.controller 控制层edu.xju.dao 数据层edu.xju.entity 实体层edu.xju.service 服务层 MyBatis:比Hibernate要灵活多用于需求不断变更的项目。 2.Web项目执行顺序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!-- web.xml 程序执行入口 --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" id="WebApp_ID" version="3.0"&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;!-- 如果使用如下配置，Spring Web MVC框架将加载“classpath:spring-mvc.xml”来进行初始化上下文而不是“/WEB-INF/[servlet名字]-servlet.xml”。 --&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;!-- 传给上下文实例（由contextClass指定）的字符串，用来指定上下文的位置。这个字符串可以被分成多个字符串（使用逗号作为分隔符） 来支持多个上下文（在多上下文的情况下，如果同一个bean被定义两次，后面一个优先）。 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- load-on-startup：表示启动容器时初始化该Servlet； --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;!-- url-pattern：表示哪些请求交给Spring Web MVC处理， “/” 是用来定义默认servlet映射的。也可以如“*.html”表示拦截所有以html为扩展名的请求。 --&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;*.json&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;!-- 从main.jsp开始 --&gt; &lt;welcome-file&gt;main.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt;&lt;/web-app&gt; DispatcherServlet拦截所有请求，在这里是拦截.do和.json结尾的请求。该DispatcherServlet默认使用WebApplicationContext作为上下文，Spring默认配置文件为“/WEB-INF/[servlet名字]-servlet.xml”。ContextLoaderListener初始化的上下文和DispatcherServlet初始化的上下文关系ContextLoaderListener初始化的上下文加载的Bean是对整个应用程序共享的；DispatcherServlet初始化的上下文加载的Bean是只对Spring Web MVC有效的Bean；即只加载Web相关的组件。DispatcherServlet的继承关系 DispatcherServlet initialization parameters![DispatcherServlet initialization parameters](/assets/blogImg/DispatcherServlet initialization parameters.png) HttpServletBean继承HttpServlet，因此在Web容器启动时将调用它的init方法，该初始化方法的主要作用将Servlet初始化参数（init-param）设置到该组件上（如contextAttribute、contextClass、namespace、contextConfigLocation），通过BeanWrapper简化设值过程，方便后续使用；提供给子类初始化扩展点，initServletBean()，该方法由FrameworkServlet覆盖。 FrameworkServlet继承HttpServletBean，通过initServletBean()进行Web上下文初始化，该方法主要覆盖一下两件事情： 初始化web上下文； 提供给子类初始化扩展点； DispatcherServlet继承FrameworkServlet，并实现了onRefresh()方法提供一些前端控制器相关的配置； 12345678910111213141516171819202122232425262728&lt;!-- spring-mvc.xml --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd"&gt; &lt;!-- 扫描这个控制包内部的Controller --&gt; &lt;context:component-scan base-package="edu.xju.controller"/&gt; &lt;mvc:annotation-driven /&gt; &lt;bean id="jspViewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="viewClass" value="org.springframework.web.servlet.view.JstlView"/&gt; &lt;property name="prefix" value="/WEB-INF/jsp/"/&gt; &lt;property name="suffix" value=".jsp"/&gt; &lt;/bean&gt; &lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter"&gt; &lt;property name="messageConverters"&gt; &lt;list&gt; &lt;bean class="org.springframework.http.converter.json.MappingJackson2HttpMessageConverter"&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// spring-mybatis.xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;context:component-scan base-package="edu"/&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/mydb?charsetEncoding=utf8"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="root"/&gt; &lt;/bean&gt; &lt;!--sessionFactory代理--&gt; &lt;bean id="sessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!--注解扫描--&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="edu.xju.dao"/&gt; &lt;property name="sqlSessionFactoryBeanName" value="sessionFactory"/&gt; &lt;/bean&gt; &lt;!--spring 事务管理--&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!--定义事务声明 --&gt; &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="get*" read-only="true" /&gt; &lt;tx:method name="add*" propagation="REQUIRED" /&gt; &lt;tx:method name="update*" propagation="REQUIRED" /&gt; &lt;tx:method name="delete*" propagation="REQUIRED" /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id="serviceAop" expression="execution(* edu.xju.service.*Service.*(..))" /&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="serviceAop" /&gt; &lt;/aop:config&gt;&lt;/beans&gt; 1234567891011121314151617181920212223// main.jsp&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;基本的 ssm 框架&lt;/title&gt;&lt;style type="text/css"&gt; a&#123; font-size: 20px; display: block; margin-top: 30px; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/user/addUser.do"&gt;添加用户&lt;/a&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/user/list.do"&gt;查找所有用户&lt;/a&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/user/getUserA.json?id=1"&gt;获取某个用户JSON格式1&lt;/a&gt;&lt;a href="$&#123;pageContext.request.contextPath &#125;/user/getUserB.json?id=2"&gt;获取某个用户JSON格式2&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// UserController.javapackage edu.xju.controller;import java.util.List;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import edu.xju.common.util.JsonUtil;import edu.xju.entity.User;import edu.xju.service.UserService;@Controller@RequestMapping("/user")public class UserController &#123; @Autowired private UserService userService; /** * 这个是返回页面 * @param model * @return */ @RequestMapping("/list.do") public String getUsers(Model model,HttpSession session)&#123; List&lt;User&gt; users = userService.getAll(); model.addAttribute("list",users); return "list"; &#125; /** * 获取json 的第一种方式 * 返回json 格式 * @param model * @param session * @param id * @return */ @RequestMapping("/getUserA.json") @ResponseBody public Object getUserByIdA(Model model,HttpSession session,Integer id )&#123; User user = userService.findUserById(id); return user; &#125; /** * 获取第json 的第二种方式 * @param model * @param response * @param session * @param id */ @RequestMapping("/getUserB.json") public void getUserByIdB(Model model,HttpServletResponse response,HttpSession session,Integer id )&#123; User user = userService.findUserById(id); JsonUtil.printByJSON(response, user); &#125; @RequestMapping("/addUser.do") public String insertUser(Model model,HttpSession session,Integer id,String name,Integer age )&#123; name = "测试姓名"; age = 99; User user = new User(name, age); userService.addUser(user); return "insertOK"; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738// User.javapackage edu.xju.entity;public class User &#123; private Integer id; private String name; private int age; public User() &#123; &#125; public User( String name, int age) &#123; this.name = name; this.age = age; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 12345678910111213// UserService.javapackage edu.xju.service;import java.util.List;import edu.xju.entity.User;public interface UserService &#123; public List&lt;User&gt; getAll(); public User findUserById(Integer id); public void addUser(User user); &#125; 1234567891011121314151617181920212223242526272829// UserServiceImpl.javapackage edu.xju.service;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import edu.xju.dao.UserMapper;import edu.xju.entity.User;@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper; public List&lt;User&gt; getAll()&#123; return userMapper.getAll(); &#125; @Override public User findUserById(Integer id) &#123; return userMapper.getUserById(id); &#125; @Override public void addUser(User user) &#123; userMapper.insertUser(user); &#125;&#125; MyBatis 12345678910111213141516// UserMapper.javapackage edu.xju.dao;import java.util.List;import org.apache.ibatis.annotations.Param;import org.springframework.stereotype.Repository;import edu.xju.entity.User;@Repositorypublic interface UserMapper &#123; public List&lt;User&gt; getAll(); public User getUserById(@Param("id")Integer id); public void insertUser(User user);&#125; 123456789101112131415161718192021// UserMapper.xml&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="edu.xju.dao.UserMapper" &gt; &lt;!-- 声明属性与查询字段之间的的对应关系 --&gt; &lt;resultMap id="BaseResultMap" type="edu.xju.entity.User" &gt; &lt;id column="id" property="id" jdbcType="INTEGER" /&gt; &lt;result column="name" property="name" jdbcType="VARCHAR" /&gt; &lt;result column="age" property="age" jdbcType="INTEGER" /&gt; &lt;/resultMap&gt; &lt;select id="getAll" resultMap="BaseResultMap"&gt; select id,name,age from user &lt;/select&gt; &lt;select id="getUserById" parameterType="int" resultMap="BaseResultMap"&gt; select id,name,age from user where id = #&#123;id&#125; &lt;/select&gt; &lt;insert id="insertUser" parameterType="edu.xju.entity.User"&gt; insert into user(name,age) values(#&#123;name&#125;,#&#123;age&#125;) &lt;/insert&gt;&lt;/mapper&gt; 3.序列图 4.常用注解解析： @Autowired 和 @Resource的使用场景和区别 @Autowired 是byType自动注入，是Spring的注解。@Resource默认是byName注入，默认使用成员属性的变量名注入，是Java自带的注解。 @Scope(“prototype”) 和 @Scope(“singleton”) 的区别 singleton 表示Spring容器中的单例，通过spring 容器获取该bean时总是返回唯一的实例。prototype 表示每次获取该bean时都会new 一个新的对象实例。 123456@Service@Scope("singleton")public class SingletonTestService &#123; private static int a = 0; private int b = 0;&#125; 如代码所示，如果设置为singleton，当a++和b++时，a和b都会增加。如果设置为prototype的话，则只有a会增加，b不会增加。 static 静态变量 静态变量整个内存中只有一个副本，为所有对象所共享，当且仅当类加载时才会被初始化；而非静态变量是对象所拥有的，在创建对象时被初始化，存在多个副本，每个对象之间拥有的副本互不影响。]]></content>
      <tags>
        <tag>JavaWEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库设计技巧]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[设计数据库的一些坑的总结。 1.Int与Integer的区别1234567891011121314151617181920/** * 学生 * 在使用Hibernate设计数据表声明数据类型时，Java代码中ID一般都设置成Integer类型，因为它可以默认为null，而 * 在使用性别时，一般都是使用Int类型 */@Entity@Table(name = "tb_student")@Cache(usage=CacheConcurrencyStrategy.READ_WRITE)public class Student &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; // 姓名 private String name; // 年龄 private int age; // 性别 private int gender;&#125; 2.有依赖关联表与无依赖关联表一张表里只能有一个主键，或者有一种情况是one primary key included two fileds.有依赖关联表：通过在用户信息表里加入用户ID实现两张表的关联。 具体实现：逻辑模型物理模型 无依赖关联表：通过引入第三张表来进行用户表与用户信息表的关联。 3.Hibernate中一对一、一对多以及多对多的注解实现12345678910111213141516171819202122232425262728293031323334353637// 1:1 &lt;==&gt; Person:IdCard// Person.java@Entity@Table(name = "tb_person")public class Person &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; // 姓名 private String name; // 年龄 private int age; // 身份证 @OneToOne(mappedBy="person") //放弃维护主控权 private IdCard card;&#125;//IdCard.java@Entity@Table(name = "tb_idcard")public class IdCard &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; // 身份证号 private String sno; // 所属人 @OneToOne @JoinColumn(name="person_id", unique=true) private Person person;&#125; 123456789101112131415161718192021222324252627282930313233343536373839// 1:n &lt;==&gt; Department:Employee// Department.java@Entity@Table(name = "tb_dept")@Cache(usage=CacheConcurrencyStrategy.READ_WRITE)public class Department &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; // 名称 @Column(length = 50) private String name; // 默认实体类所有字段都映射, 可使用@Transient声明不做映射 private int sn; // 拥有多个员工 @OneToMany(mappedBy = "dept") //mappedBy：表示放弃维护主控权 private Set&lt;Employee&gt; emps = new HashSet&lt;Employee&gt;();&#125;// Employee.java@Entity@Table(name="tb_emp")@Cache(usage=CacheConcurrencyStrategy.READ_WRITE)public class Employee &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; // 姓名 private String name; // 部门 @ManyToOne @JoinColumn(name="dept_id") private Department dept;&#125; 123456789101112131415161718192021222324252627282930313233// n:n &lt;==&gt; Course:Student// Course.java@Entity@Table(name = "tb_course")public class Course &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; // 名称 private String name; // 学生 @ManyToMany @JoinTable(name="tb_student_course", joinColumns=@JoinColumn(name="course_id"), inverseJoinColumns=@JoinColumn(name="student_id")) private Set&lt;Student&gt; students = new HashSet&lt;Student&gt;();&#125;// Student.java@Entity@Table(name="tb_student")public class Student &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; // 姓名 private String name; // 课程 @ManyToMany(mappedBy="students") private Set&lt;Course&gt; courses = new HashSet&lt;Course&gt;();&#125;]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>Hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-Java基础知识]]></title>
    <url>%2F2017%2F07%2F28%2FJAVA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[扎实基础！ 笔试让我明白了自己的一些问题。 123456byte a = 100;byte b = 10;byte c = a + b;// 报错 默认最低是Int类型byte c = (byte)a + b;int c = a + b; 1234567891011121314151617181920212223package com.lzy.test;public class Test1 &#123; public static void main(String[] args) &#123; int a = 0; int b = a++; System.out.println(a);//1 int c = ++a; System.out.println(a);//2 System.out.println(a++);//2 System.out.println(++a);//4 System.out.println(b);//0 System.out.println(c);//2 &#125;&#125;public static void main(String[] args) &#123; int a = 5; // a的变化：6 7 8 9 10 // b的变化： 5 7 7 8 10 int b = a++ + ++a + a++ + a++ + ++a; System.out.println(b); //37 &#125; 123456789101112131415161718public class Test2 &#123; public static void main(String[] args) &#123; int a = 0; //&amp;对每一个都判断； if (a==0 &amp; a++==1) &#123; System.out.println("&amp;");//不输出 &#125; System.out.println(a);//1 //&amp;&amp;只要前面是false就输出false，而不继续判断后面了 if (a==0 &amp;&amp; a++==1) &#123; System.out.println("&amp;&amp;");//不输出 &#125; if (a==1 &amp;&amp; a++==1) &#123; System.out.println("&amp;&amp;");//输出 &#125; &#125;&#125; 12345678910111213141516171819202122// 随机数 public static void main(String[] args) &#123; // 随机数产生的范围是[0, 1) double num = Math.random(); // 这样子[0, 0.5) [0.5, 1)才能真正平衡 if (num &gt;= 0.5) &#123; System.out.println("成都"); &#125; else &#123; System.out.println("赵雷"); &#125; // 输出指定范围的数字 // [26, 38] // 方法一： (int)(Math.random()*(max - min) + min); int a = 25; int b = 38; int scal = (int)(Math.random()*(b - a + 1)) + a; System.out.println(scal); // 方法二： int scall = (int)(Math.random() * 100 % 12) + a; System.out.println(scall); &#125; 基础知识点一：== 与 equal的区别： == 是一个运算符。 Equals则是string对象的方法，可以.（点）出来。 因为对象变量的存储的是对象在内存中的路径，即内存地址。所以用“==”比较时，即使对象的值相等，但是他们的内存地址不同(引用数据类型)，所以==的结果为false。故“==”用于比较两个变量的值是否相等，而不是变量引用的对象是否相等。 equal用于比较两个对象是否相同。 “==”比较的是值【变量(栈)内存中存放的对象的(堆)内存地址】equal用于比较两个对象的值是否相同【不是比地址】 【特别注意】Object类中的equals方法和“==”是一样的，没有区别，而String类，Integer类等等一些类，是重写了equals方法，才使得equals和“==不同”，所以，当自己创建类时，自动继承了Object的equals方法，要想实现不同的等于比较，必须重写equals方法。”==”比”equal”运行速度快,因为”==”只是比较引用. 二：public、protected、private、default（friendly）的区别: public：可以被其他类访问private：只能被自己访问和修改protected:类内部、子类、同一个包中的类之间可以访问default：作用域是包，可以不写。被认为是friendly final关键字：修饰类：表示该类不能被继承，final类的所有成员方法都会被隐式的指定为final方法修饰方法：变是该方法不能被子类修改，类的private方法会被隐式的指定为final方法修饰变量：基本数据变量不可以被更改 引用类型变量不可以再更改指向另一个对象 Java线程安全与非线程安全：多线程安全优点体现在多个线程操作同一个对象，非线程安全 != 不安全线程安全是通过线程同步控制来实现的，也就是synchronized关键字。ArrayList是非线程安全 Vector是线程安全；HashMap是非线程安全 HashTable是线程安全；StringBuilder是非线程安全 StringBuffer是线程安全；http://blog.csdn.net/YiZhiCXY/article/details/51335385 String、StringBuffer、StringBuilder的区别：String： 字符串常量StringBuffer: 字符串变量（线程安全） 多线程下有优势StringBuilder: 字符串变量（非线程安全） 单线程下有优势 Array、ArrayList、Vectory、LinkedList的区别：(推荐看源码)ArrayList是为可变数组实现的，当更多的元素添加到ArrayList的时候，它的大小会动态增大。它的元素可以通过get/set方法直接访问，因为ArrayList本质上是一个数组。 1234567891011121314/** * Resizable-array implementation of the &#123;@code List&#125; interface. Implements * all optional list operations, and permits all elements, including * &#123;@code null&#125;. In addition to implementing the &#123;@code List&#125; interface, * this class provides methods to manipulate the size of the array that is * used internally to store the list. (This class is roughly equivalent to * &#123;@code Vector&#125;, except that it is unsynchronized.) * &lt;p&gt;The &#123;@code size&#125;, &#123;@code isEmpty&#125;, &#123;@code get&#125;, &#123;@code set&#125;, * &#123;@code iterator&#125;, and &#123;@code listIterator&#125; operations run in constant * time. The &#123;@code add&#125; operation runs in &lt;i&gt;amortized constant time&lt;/i&gt;, * that is, adding n elements requires O(n) time. All of the other operations * run in linear time (roughly speaking). The constant factor is low compared * to that for the &#123;@code LinkedList&#125; implementation. */ LinkedList是为双向链表实现的,非线程安全(not ) 123456789101112131415161718/** * Doubly-linked list implementation of the &#123;@code List&#125; and &#123;@code Deque&#125; * interfaces. Implements all optional list operations, and permits all * elements (including &#123;@code null&#125;). * * &lt;p&gt;All of the operations perform as could be expected for a doubly-linked * list. Operations that index into the list will traverse the list from * the beginning or the end, whichever is closer to the specified index. * * &lt;p&gt;&lt;strong&gt;Note that this implementation is not synchronized.&lt;/strong&gt; * If multiple threads access a linked list concurrently, and at least * one of the threads modifies the list structurally, it &lt;i&gt;must&lt;/i&gt; be * synchronized externally. (A structural modification is any operation * that adds or deletes one or more elements; merely setting the value of * an element is not a structural modification.) This is typically * accomplished by synchronizing on some object that naturally * encapsulates the list. */ Vector与ArrayList相似，但是它是同步的。 12345678910111213141516/** * The &#123;@code Vector&#125; class implements a growable array of * objects. Like an array, it contains components that can be * accessed using an integer index. However, the size of a * &#123;@code Vector&#125; can grow or shrink as needed to accommodate * adding and removing items after the &#123;@code Vector&#125; has been created. * * &lt;p&gt;Each vector tries to optimize storage management by maintaining a * &#123;@code capacity&#125; and a &#123;@code capacityIncrement&#125;. The * &#123;@code capacity&#125; is always at least as large as the vector * size; it is usually larger because as components are added to the * vector, the vector's storage increases in chunks the size of * &#123;@code capacityIncrement&#125;. An application can increase the * capacity of a vector before inserting a large number of * components; this reduces the amount of incremental reallocation. */ 如果你的程序是线程安全的，ArrayList是一个比较好的选择。当更多的元素被添加的时候，Vector和ArrayList需要更多的空间。Vector每次扩容会增加一倍的空间，而ArrayList增加50%。 注意：ArrayList默认的初始空间大小相当的小，通过构造函数去初始化一个更大的空间是一个好习惯，可以避免扩容开销。部分引自于：https://www.cnblogs.com/chenpi/p/5505375.html Map、List、Set、Array的区别：https://www.cnblogs.com/chuanheliu/p/6363948.html Iterator 的hasNext方法和next方法:hasNext():如果仍有元素可以迭代，则返回 true。（换句话说，如果 next 返回了元素而不是抛出异常，则返回 true）。next():返回迭代的下一个元素。 12345678910111213141516/** * Returns &#123;@code true&#125; if the iteration has more elements. * (In other words, returns &#123;@code true&#125; if &#123;@link #next&#125; would * return an element rather than throwing an exception.) * * @return &#123;@code true&#125; if the iteration has more elements */boolean hasNext();/** * Returns the next element in the iteration. * * @return the next element in the iteration * @throws NoSuchElementException if the iteration has no more elements */E next(); JAVA多线程和并发基础面试问答:http://www.cnblogs.com/dolphin0520/p/3932934.html Oracle数据库中TRUNCATE 与 DELETE 的区别： ROLLBACK可以撤销DELETE操作但撤销不了TRUNCATE操作 TRUNCATE TABLE比DELETE的速度快； TRUNCATE TABLE是删除表的所有行，而DELETE是删除表的一行或者多行 TRUNCATE TABLE在遇到任何一行违反约束(外键约束)时仍然删除表的所有行，但表的结构及其列、约束、索引等保持不变，DELETE则直接返回报错。 对于被外键约束的表，不能使用TRUNCATE TABLE，而应该使用不带WHERE语句的DELETE语句。 如果想保留标识计数值，要用DELETE，因为TRUNCATE TABLE会对新行标志符列搜用的计数值重置为该列的种子。 序列化的作用：为了保存在内存中的各种对象的状态（序列化），并且可以把保存的对象状态再读出来（反序列化）。 JAVA 中的 super和this this的用法 指向当前对象本身。 形参与成员名字重名，用this来区分123456789101112public class User &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; // 形参与成员名字重名，用this来区分 this.name = name; &#125;&#125; super用法 指向当前对象的父类 子类中的成员变量或方法与父类同名12345678910111213141516171819public class TestTest extends Country &#123; String name; void value() &#123; name = "Shanghai"; super.value(); System.out.println(super.name); &#125; public static void main(String[] args) &#123; TestTest tt = new TestTest(); tt.value(); &#125;&#125;class Country &#123; String name; void value() &#123; name = "China"; &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Npm模块学习]]></title>
    <url>%2F2017%2F07%2F27%2FNpm%E6%A8%A1%E5%9D%97%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[// TODO]]></content>
      <tags>
        <tag>Npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔]]></title>
    <url>%2F2017%2F07%2F27%2F%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[今年7月中旬，我来到了成都，见识到了成都的美。 成都 让我掉下眼泪的 不止昨夜的酒 让我依依不舍的 不止你的温柔 余路还要走多久 你攥着我的手 让我感到为难的 是挣扎的自由 分别总是在九月 回忆是思念的愁 深秋嫩绿的垂柳 亲吻着我额头 在那座阴雨的小城里 我从未忘记你 成都 带不走的 只有你 和我在成都的街头走一走 直到所有的灯都熄灭了也不停留 你会挽着我的衣袖 我会把手揣进裤兜 走到玉林路的尽头 坐在(走过)小酒馆的门口 分别总是在九月 回忆是思念的愁 深秋嫩绿的垂柳 亲吻着我额头 在那座阴雨的小城里 我从未忘记你 成都 带不走的 只有你 和我在成都的街头走一走 直到所有的灯都熄灭了也不停留 你会挽着我的衣袖 我会把手揣进裤兜 走到玉林路的尽头 坐在(走过)小酒馆的门口 和我在成都的街头走一走 直到所有的灯都熄灭了也不停留 和我在成都的街头走一走 直到所有的灯都熄灭了也不停留 你会挽着我的衣袖 我会把手揣进裤兜 走到玉林路的尽头 坐在(走过)小酒馆的门口 和我在成都的街头走一走 直到所有的灯都熄灭了也不停留]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
